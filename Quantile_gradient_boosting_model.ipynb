{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CITjzuvtVPCq"
   },
   "source": [
    "# Pinball loss-guided LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcI6gtibwlXn"
   },
   "source": [
    "### 1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_ZDgAX2tVryI"
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import numpy as np\n",
    "from numpy import array, hstack\n",
    "from numpy import asarray\n",
    "\n",
    "import pickle # load and save data\n",
    "\n",
    "# Data preparation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV # time series validation windows\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LightGBM - Package\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaUDYg27d1Ay"
   },
   "source": [
    "### 2. Load & prepare data\n",
    "#### Consumptin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "94pMXbg0d0pv"
   },
   "outputs": [],
   "source": [
    "df_consumption = pd.read_csv(\"{}/consumption_prepared.csv\".format(data_source_folder), sep=\",\", decimal = ',')\n",
    "df_consumption['date'] = pd.to_datetime(df_consumption['date']) # change date to date-time var\n",
    "df_consumption.set_index('date', inplace = True) # create index\n",
    "df_consumption['consumption'] = df_consumption['consumption'].astype(float) # change consumption values to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5-ncgHHebwc"
   },
   "source": [
    "#### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5QROIUkld0kW"
   },
   "outputs": [],
   "source": [
    "# Load weather Data\n",
    "df_weather = pd.read_csv(\"{}/daily_weather_avg.csv\".format(data_source_folder), sep=\",\", decimal = ',')\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add date-time information to weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6jRmJo1rel3f"
   },
   "outputs": [],
   "source": [
    "# add date information\n",
    "def compute_weekend(row):\n",
    "    if row['weekday'] > 4:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0exnzJmd0ht",
    "outputId": "b28f90a0-1296-431a-b14e-3d7745f12e8d"
   },
   "outputs": [],
   "source": [
    "# subset to 2018\n",
    "df_weather[['temp', 'wind', 'cloud']] = df_weather[['temp', 'wind', 'cloud']].shift(+1) # each day gets informaiton on weather the day before\n",
    "df_weather_shift = df_weather.iloc[1:, :]\n",
    "\n",
    "df_weather_rep = DataFrame()\n",
    "for i in range(3):\n",
    "    temp = pd.Series(df_weather_shift.iloc[:,1+i])\n",
    "    rep = temp.repeat(96)\n",
    "    df_weather_rep[i] = rep\n",
    "df_weather_rep.columns = ['temp', 'wind', 'cloud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_rep['date'] = pd.date_range(start = '2018-01-01 00:00:00', periods = 43680, freq = '15T')\n",
    "df_weather_rep.set_index('date', inplace=True)\n",
    "\n",
    "df_weather_rep.loc[:,'month'] = df_weather_rep.index.month\n",
    "df_weather_rep.loc[:,'day'] = df_weather_rep.index.day\n",
    "df_weather_rep.loc[:,'hour'] = df_weather_rep.index.hour\n",
    "df_weather_rep['weekday']= df_weather_rep.index.weekday\n",
    "df_weather_rep['is_weekend'] = df_weather_rep.apply (lambda row:compute_weekend(row), axis = 1)\n",
    "\n",
    "# Transform categorical date-time variables\n",
    "df_weather_rep['month'] = pd.Series(df_weather_rep['month'], dtype=\"category\")\n",
    "df_weather_rep['day'] = pd.Series(df_weather_rep['day'], dtype=\"category\")\n",
    "df_weather_rep['hour'] = pd.Series(df_weather_rep['hour'], dtype=\"category\")\n",
    "df_weather_rep['weekday'] = pd.Series(df_weather_rep['weekday'], dtype=\"category\")\n",
    "df_weather_rep['is_weekend'] = pd.Series(df_weather_rep['is_weekend'], dtype=\"category\")\n",
    "\n",
    "df_weather_rep.drop(df_weather_rep.columns[[6]], axis=1, inplace=True) # 6 position weekday column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGBRSBGIpw98"
   },
   "source": [
    "### 3. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "U0_Sr5bjgzbX"
   },
   "outputs": [],
   "source": [
    "# train data: year 2018/ test data: year 2019 90 days\n",
    "\n",
    "# split into standard days (atm 20%)\n",
    "train_cons, test_cons = df_consumption.iloc[:35040,:], df_consumption.iloc[35040:, :]\n",
    "train_weath, test_weath = df_weather_rep.iloc[:35040,:], df_weather_rep.iloc[35040:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NQuoSATwoTR"
   },
   "source": [
    "### 4. Data transformation\n",
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fui4_ATqUIdL",
    "outputId": "e44b98c9-38d5-4c8c-ea70-81b89380f713"
   },
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, data1, n_in=72, n_out=48, dropnan=True):\n",
    "    \n",
    "    df_weather_time = DataFrame(data1)\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('consumption(t-{})'.format(i)) for j in range(n_vars)]\n",
    "    \n",
    "    weather_cols = list(train_weath)\n",
    "    for i in range(df_weather_time.shape[1]):\n",
    "        cols.append(df_weather_time.iloc[:,i])\n",
    "        names += [weather_cols[i]]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('consumption(t)') for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('consumption(t+{})'.format(i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# Define forecasting approach\n",
    "n_in = 192\n",
    "n_out = 96\n",
    "n_vars = 6\n",
    "\n",
    "# load dataset\n",
    "cons = train_cons.values\n",
    "date_time = train_weath.iloc[:, 3:7]\n",
    "weather = train_weath.iloc[:, 0:3]\n",
    "\n",
    "encoder1 = LabelEncoder()\n",
    "#date_time = encoder1.fit_transform(date_time)\n",
    "date_time_scaled = df_weather_rep.iloc[:, 3:7].apply(encoder1.fit_transform).values\n",
    "date_time_scaled_train = date_time_scaled[:35040]\n",
    "date_time_scaled_test =date_time_scaled[35040:]\n",
    "\n",
    "# integer encode direction\n",
    "date_time = date_time.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "scaler1 = MinMaxScaler()\n",
    "scaled_consumption = scaler1.fit_transform(cons)\n",
    "scaler2 = MinMaxScaler()\n",
    "scaled_weather = scaler2.fit_transform(weather)\n",
    "weather_all = hstack((scaled_weather, date_time_scaled_train)) # combine transformed data\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed_train = series_to_supervised(scaled_consumption, weather_all, n_in, n_out)\n",
    "\n",
    "#print(list(reframed_train.head(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-1w0pWSleZX",
    "outputId": "441329cc-c303-4174-b571-420fee9e3760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27803, 199) (27803, 96) (6950, 199) (6950, 96)\n"
     ]
    }
   ],
   "source": [
    "# Split transformed train set into train and validation set\n",
    "\n",
    "#Input numbers\n",
    "n_out = 96\n",
    "cons_hist = 192\n",
    "other_vars = 7\n",
    "n_in = cons_hist+other_vars\n",
    "\n",
    "# split into train and test sets\n",
    "values_train = reframed_train.values\n",
    "train, val = values_train[:-6950, :], values_train[-6950:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-n_out], train[:, -n_out:]\n",
    "val_X, val_y = val[:, :-n_out], val[:, -n_out:]\n",
    "\n",
    "print(train_X.shape, train_y.shape, val_X.shape, val_y.shape)\n",
    "\n",
    "# Combined train set\n",
    "full_train_X, full_train_y = values_train[:, :-n_out], values_train[:, -n_out:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD066ScOn3W0"
   },
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "\n",
    "# Define forecasting approach\n",
    "n_in = 192\n",
    "n_out = 96\n",
    "n_vars = 6\n",
    "\n",
    "# load dataset\n",
    "cons_test = test_cons\n",
    "weather_test_s = test_weath.iloc[:, 0:3].values\n",
    "\n",
    "# normalize features\n",
    "scaled_consumption = scaler1.transform(cons_test)\n",
    "scaled_weather = scaler2.transform(weather_test_s)\n",
    "scaled = hstack((scaled_weather, date_time_scaled_test))\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed_test = series_to_supervised(scaled_consumption, scaled, n_in, n_out)\n",
    "\n",
    "#print(reframed_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8353, 199) (8353, 96)\n"
     ]
    }
   ],
   "source": [
    "#Input numbers\n",
    "n_out = 96\n",
    "cons_hist = 192\n",
    "other_vars = 7\n",
    "n_in = cons_hist+other_vars\n",
    "\n",
    "# split into train and test sets\n",
    "values_test = reframed_test.values\n",
    "\n",
    "# split into input and outputs\n",
    "test_X, test_y = values_test[:, :-n_out], values_test[:, -n_out:]\n",
    "\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKojxLixV6ui"
   },
   "source": [
    "### 5. Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo4dwT18nnt3"
   },
   "source": [
    "#### Create loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kGha9TiwnmKQ"
   },
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, q):\n",
    "    return np.max([q*(y_true - y_pred), (1-q)*(y_pred-y_true)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define goal of prediction approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EESIpUfXzdNZ"
   },
   "outputs": [],
   "source": [
    "# Define the quantiles to predict as decimal\n",
    "quantiles = [.2, .4, .6, .8]\n",
    "n_steps_ahead = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNWlg3M0nsgT"
   },
   "source": [
    "### Hyperparameter tuning - working\n",
    "-> for each forecast horizon and quantile individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xWGuxsMMdWae"
   },
   "outputs": [],
   "source": [
    "# Preparation\n",
    "validation_window_size = n_steps_ahead\n",
    "tsp = TimeSeriesSplit(n_splits = validation_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "collapsed": true,
    "id": "Noq8CaJto7ZP",
    "outputId": "a4b7252a-901a-4b0b-98a5-d38737800033"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f88b563ac5dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         param_grid = variable_params)\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhour\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best Accuracy:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    820\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m                     categorical_feature=categorical_feature, callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[0;32m    823\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    686\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_metrics_callable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m                               callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   2227\u001b[0m                 )\n\u001b[0;32m   2228\u001b[0m             \u001b[1;31m# construct booster object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2229\u001b[1;33m             \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2230\u001b[0m             \u001b[1;31m# copy the parameters from train_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1470\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1473\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1268\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1271\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init_from_list_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init_from_np2d\u001b[1;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[0mref_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m             ctypes.byref(self.handle)))\n\u001b[0m\u001b[0;32m   1321\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_of_features_lagged = 6\n",
    "quantile = 0.2\n",
    "\n",
    "model_params = list()\n",
    "for hour in range(n_steps_ahead):\n",
    "\n",
    "    hyperparams = {'objective': 'quantile', 'metric': 'quantile', 'boosting_type': 'gbdt', 'learning_rate': 0.1}\n",
    "    variable_params = {'max_depth': [3, 5], 'num_leaves': [5, 10, 15],'n_estimators': [50, 100], 'subsample':[0.4, 0.6], 'learning_rate': [0.01, 0.1]}\n",
    "    xgb1 = LGBMRegressor(**hyperparams, alpha=quantile)\n",
    "    model = GridSearchCV(\n",
    "        xgb1,\n",
    "        cv = tsp,\n",
    "        param_grid = variable_params)\n",
    "\n",
    "    model.fit(train_X, train_y[:,hour])\n",
    "    print(\"Best Accuracy:{}\".format(model.best_score_))\n",
    "    print(model.best_params_)\n",
    "    model_params.append(model.best_score_)\n",
    "    \n",
    "with open(\"C:/Users/alexl/Desktop/daily/pickle_lstm_best_model_params.text\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(model_params, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZyawzb7qfZ2"
   },
   "source": [
    "### 5. Final model fitting and predicting\n",
    "- fit model to combined train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of model\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    'metric': 'quantile',\n",
    "    'max_depth': 6,\n",
    "    'num_leaves': 15,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 500,\n",
    "    'subsample': 0.7,\n",
    "    'boosting_type': 'gbdt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sr8-Vk9OxK0I"
   },
   "outputs": [],
   "source": [
    "def lgbm_final_fit(trainx, trainy, testx, testy, hour, quant):\n",
    "    predictions = list()\n",
    "    actuals = list()\n",
    "\n",
    "    final_lgbm = LGBMRegressor(**params, alpha=quant)\n",
    "    final_lgbm.fit(trainx, trainy[:, hour])\n",
    "\n",
    "    #history = [x for x in train]\n",
    "    for i in range(len(testx)):\n",
    "        yhat = final_lgbm.predict(asarray([testx[i,:]]))[0]\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(testy[i, hour])\n",
    "        #print('>actual=%.1f, predicted=%.1f' % (test_y[i, h], yhat))\n",
    "    pinball_loss = quantile_loss(testy[:, h], predictions, q = quant).mean()\n",
    "\n",
    "    return pinball_loss, actuals, predictions, final_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoycU5_r_VeY",
    "outputId": "6b561513-8ed9-436b-ed40-4eda96b4c827",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinball loss quantile 0.2 hour 1: 0.002868176254252173\n",
      "Pinball loss quantile 0.2 hour 2: 0.004169655970982844\n",
      "Pinball loss quantile 0.2 hour 3: 0.005199382252769026\n",
      "Pinball loss quantile 0.2 hour 4: 0.006045757124357763\n",
      "Pinball loss quantile 0.2 hour 5: 0.006778686913174301\n",
      "Pinball loss quantile 0.2 hour 6: 0.007430593914816876\n",
      "Pinball loss quantile 0.2 hour 7: 0.008102450656837215\n",
      "Pinball loss quantile 0.2 hour 8: 0.00859208750100966\n",
      "Pinball loss quantile 0.2 hour 9: 0.008871713604966235\n",
      "Pinball loss quantile 0.2 hour 10: 0.009380564575847484\n",
      "Pinball loss quantile 0.2 hour 11: 0.009689188237186326\n",
      "Pinball loss quantile 0.2 hour 12: 0.010152084479556869\n",
      "Pinball loss quantile 0.2 hour 13: 0.01041021120826952\n",
      "Pinball loss quantile 0.2 hour 14: 0.010637382502599652\n",
      "Pinball loss quantile 0.2 hour 15: 0.01126395831682201\n",
      "Pinball loss quantile 0.2 hour 16: 0.011329701641670815\n",
      "Pinball loss quantile 0.2 hour 17: 0.011518173947068661\n",
      "Pinball loss quantile 0.2 hour 18: 0.012047752225328709\n",
      "Pinball loss quantile 0.2 hour 19: 0.0120951360236022\n",
      "Pinball loss quantile 0.2 hour 20: 0.012298153047802554\n",
      "Pinball loss quantile 0.2 hour 21: 0.012266963470510607\n",
      "Pinball loss quantile 0.2 hour 22: 0.012735434576453904\n",
      "Pinball loss quantile 0.2 hour 23: 0.012801398650358625\n",
      "Pinball loss quantile 0.2 hour 24: 0.013076313208117231\n",
      "Pinball loss quantile 0.2 hour 25: 0.013196648199218465\n",
      "Pinball loss quantile 0.2 hour 26: 0.013075623384240618\n",
      "Pinball loss quantile 0.2 hour 27: 0.013963863500353833\n",
      "Pinball loss quantile 0.2 hour 28: 0.01423418955434869\n",
      "Pinball loss quantile 0.2 hour 29: 0.014317535971852159\n",
      "Pinball loss quantile 0.2 hour 30: 0.015478547910589611\n",
      "Pinball loss quantile 0.2 hour 31: 0.015264825625389288\n",
      "Pinball loss quantile 0.2 hour 32: 0.015522393611067255\n",
      "Pinball loss quantile 0.2 hour 33: 0.01621371325344558\n",
      "Pinball loss quantile 0.2 hour 34: 0.01662136600345855\n",
      "Pinball loss quantile 0.2 hour 35: 0.016410635453836475\n",
      "Pinball loss quantile 0.2 hour 36: 0.016899085870406432\n",
      "Pinball loss quantile 0.2 hour 37: 0.017017504708909722\n",
      "Pinball loss quantile 0.2 hour 38: 0.017437483656377636\n",
      "Pinball loss quantile 0.2 hour 39: 0.017576540512277593\n",
      "Pinball loss quantile 0.2 hour 40: 0.017779763037897918\n",
      "Pinball loss quantile 0.2 hour 41: 0.018365078644265013\n",
      "Pinball loss quantile 0.2 hour 42: 0.018177171860076106\n",
      "Pinball loss quantile 0.2 hour 43: 0.018355878394761486\n",
      "Pinball loss quantile 0.2 hour 44: 0.018651748839667793\n",
      "Pinball loss quantile 0.2 hour 45: 0.01897864932259074\n",
      "Pinball loss quantile 0.2 hour 46: 0.019726973572874142\n",
      "Pinball loss quantile 0.2 hour 47: 0.01940020682330739\n",
      "Pinball loss quantile 0.2 hour 48: 0.01997073735286614\n",
      "Pinball loss quantile 0.2 hour 49: 0.019856937159090815\n",
      "Pinball loss quantile 0.2 hour 50: 0.02011881287568478\n",
      "Pinball loss quantile 0.2 hour 51: 0.020257388153821443\n",
      "Pinball loss quantile 0.2 hour 52: 0.02163888453943607\n",
      "Pinball loss quantile 0.2 hour 53: 0.021070858078061215\n",
      "Pinball loss quantile 0.2 hour 54: 0.02081130665144416\n",
      "Pinball loss quantile 0.2 hour 55: 0.021736338703761137\n",
      "Pinball loss quantile 0.2 hour 56: 0.022152216779567007\n",
      "Pinball loss quantile 0.2 hour 57: 0.021740816498591106\n",
      "Pinball loss quantile 0.2 hour 58: 0.022021536095227712\n",
      "Pinball loss quantile 0.2 hour 59: 0.022339860408013328\n",
      "Pinball loss quantile 0.2 hour 60: 0.022736211245978237\n",
      "Pinball loss quantile 0.2 hour 61: 0.0228248087449435\n",
      "Pinball loss quantile 0.2 hour 62: 0.022861731032515306\n",
      "Pinball loss quantile 0.2 hour 63: 0.022019215726624707\n",
      "Pinball loss quantile 0.2 hour 64: 0.023688384487050216\n",
      "Pinball loss quantile 0.2 hour 65: 0.023196210975142184\n",
      "Pinball loss quantile 0.2 hour 66: 0.02351066584930272\n",
      "Pinball loss quantile 0.2 hour 67: 0.023891656005283177\n",
      "Pinball loss quantile 0.2 hour 68: 0.023609944437778838\n",
      "Pinball loss quantile 0.2 hour 69: 0.024423933685840633\n",
      "Pinball loss quantile 0.2 hour 70: 0.023704826075098176\n",
      "Pinball loss quantile 0.2 hour 71: 0.0243213453045865\n",
      "Pinball loss quantile 0.2 hour 72: 0.02413095201837975\n",
      "Pinball loss quantile 0.2 hour 73: 0.024808351793657743\n",
      "Pinball loss quantile 0.2 hour 74: 0.02459458483908752\n",
      "Pinball loss quantile 0.2 hour 75: 0.02438399953655733\n",
      "Pinball loss quantile 0.2 hour 76: 0.024884665228345964\n",
      "Pinball loss quantile 0.2 hour 77: 0.025061755792329655\n",
      "Pinball loss quantile 0.2 hour 78: 0.025596265094115073\n",
      "Pinball loss quantile 0.2 hour 79: 0.024760990150708834\n",
      "Pinball loss quantile 0.2 hour 80: 0.02503044398974722\n",
      "Pinball loss quantile 0.2 hour 81: 0.025625464149201145\n",
      "Pinball loss quantile 0.2 hour 82: 0.025326875841519214\n",
      "Pinball loss quantile 0.2 hour 83: 0.025469284577400594\n",
      "Pinball loss quantile 0.2 hour 84: 0.02620050477235219\n",
      "Pinball loss quantile 0.2 hour 85: 0.02618889259494381\n",
      "Pinball loss quantile 0.2 hour 86: 0.02674309851010617\n",
      "Pinball loss quantile 0.2 hour 87: 0.026379797618174827\n",
      "Pinball loss quantile 0.2 hour 88: 0.026653809974532993\n",
      "Pinball loss quantile 0.2 hour 89: 0.026691843485304956\n",
      "Pinball loss quantile 0.2 hour 90: 0.02672653976090519\n",
      "Pinball loss quantile 0.2 hour 91: 0.02674608485067049\n",
      "Pinball loss quantile 0.2 hour 92: 0.025716869701169064\n",
      "Pinball loss quantile 0.2 hour 93: 0.025918663686986067\n",
      "Pinball loss quantile 0.2 hour 94: 0.025688106469422406\n",
      "Pinball loss quantile 0.2 hour 95: 0.025974578644306037\n",
      "Pinball loss quantile 0.2 hour 96: 0.025408876741004886\n",
      "Pinball loss quantile 0.4 hour 1: 0.003961787876877082\n",
      "Pinball loss quantile 0.4 hour 2: 0.00564204186262108\n",
      "Pinball loss quantile 0.4 hour 3: 0.006945486975398072\n",
      "Pinball loss quantile 0.4 hour 4: 0.007989837180199993\n",
      "Pinball loss quantile 0.4 hour 5: 0.009129061511331076\n",
      "Pinball loss quantile 0.4 hour 6: 0.00990805561654312\n",
      "Pinball loss quantile 0.4 hour 7: 0.010787863123898539\n",
      "Pinball loss quantile 0.4 hour 8: 0.011780178071675862\n",
      "Pinball loss quantile 0.4 hour 9: 0.01238980787696913\n",
      "Pinball loss quantile 0.4 hour 10: 0.012763087699784107\n",
      "Pinball loss quantile 0.4 hour 11: 0.013405535585893986\n",
      "Pinball loss quantile 0.4 hour 12: 0.013717342700089206\n",
      "Pinball loss quantile 0.4 hour 13: 0.014030452401508776\n",
      "Pinball loss quantile 0.4 hour 14: 0.014523396110133433\n",
      "Pinball loss quantile 0.4 hour 15: 0.015029935634069791\n",
      "Pinball loss quantile 0.4 hour 16: 0.015228619949305654\n",
      "Pinball loss quantile 0.4 hour 17: 0.015498030137298273\n",
      "Pinball loss quantile 0.4 hour 18: 0.015537963898693294\n",
      "Pinball loss quantile 0.4 hour 19: 0.015650507184577195\n",
      "Pinball loss quantile 0.4 hour 20: 0.015805902008572546\n",
      "Pinball loss quantile 0.4 hour 21: 0.016565293560197376\n",
      "Pinball loss quantile 0.4 hour 22: 0.016739999531123394\n",
      "Pinball loss quantile 0.4 hour 23: 0.016972274146950288\n",
      "Pinball loss quantile 0.4 hour 24: 0.016756330345862054\n",
      "Pinball loss quantile 0.4 hour 25: 0.01782453824991247\n",
      "Pinball loss quantile 0.4 hour 26: 0.017634524427957807\n",
      "Pinball loss quantile 0.4 hour 27: 0.01857845024870283\n",
      "Pinball loss quantile 0.4 hour 28: 0.018565595151536083\n",
      "Pinball loss quantile 0.4 hour 29: 0.018741851080183197\n",
      "Pinball loss quantile 0.4 hour 30: 0.01894093602964658\n",
      "Pinball loss quantile 0.4 hour 31: 0.01932199199941078\n",
      "Pinball loss quantile 0.4 hour 32: 0.01989953587689023\n",
      "Pinball loss quantile 0.4 hour 33: 0.020567704307247234\n",
      "Pinball loss quantile 0.4 hour 34: 0.020801341784886296\n",
      "Pinball loss quantile 0.4 hour 35: 0.02092066484382189\n",
      "Pinball loss quantile 0.4 hour 36: 0.02168392602745628\n",
      "Pinball loss quantile 0.4 hour 37: 0.022964609100390436\n",
      "Pinball loss quantile 0.4 hour 38: 0.02197230955427451\n",
      "Pinball loss quantile 0.4 hour 39: 0.02305482597213114\n",
      "Pinball loss quantile 0.4 hour 40: 0.023503376415640157\n",
      "Pinball loss quantile 0.4 hour 41: 0.023270702583920676\n",
      "Pinball loss quantile 0.4 hour 42: 0.023286867080552248\n",
      "Pinball loss quantile 0.4 hour 43: 0.024079980415065497\n",
      "Pinball loss quantile 0.4 hour 44: 0.024422784689652133\n",
      "Pinball loss quantile 0.4 hour 45: 0.025052462808264044\n",
      "Pinball loss quantile 0.4 hour 46: 0.02600716416816124\n",
      "Pinball loss quantile 0.4 hour 47: 0.025171999815621383\n",
      "Pinball loss quantile 0.4 hour 48: 0.025620523494205485\n",
      "Pinball loss quantile 0.4 hour 49: 0.025794088245995817\n",
      "Pinball loss quantile 0.4 hour 50: 0.02582613849478938\n",
      "Pinball loss quantile 0.4 hour 51: 0.02675356351989256\n",
      "Pinball loss quantile 0.4 hour 52: 0.026856109250856804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinball loss quantile 0.4 hour 53: 0.027577140976017715\n",
      "Pinball loss quantile 0.4 hour 54: 0.028138814874009842\n",
      "Pinball loss quantile 0.4 hour 55: 0.027873316883092778\n",
      "Pinball loss quantile 0.4 hour 56: 0.028725571894296243\n",
      "Pinball loss quantile 0.4 hour 57: 0.028862612155102253\n",
      "Pinball loss quantile 0.4 hour 58: 0.02867603876861514\n",
      "Pinball loss quantile 0.4 hour 59: 0.02936139007431981\n",
      "Pinball loss quantile 0.4 hour 60: 0.02870376504266464\n",
      "Pinball loss quantile 0.4 hour 61: 0.029592164701530848\n",
      "Pinball loss quantile 0.4 hour 62: 0.030367289835012516\n",
      "Pinball loss quantile 0.4 hour 63: 0.030396896947860894\n",
      "Pinball loss quantile 0.4 hour 64: 0.030152818585491074\n",
      "Pinball loss quantile 0.4 hour 65: 0.03049618473950594\n",
      "Pinball loss quantile 0.4 hour 66: 0.03041591265394278\n",
      "Pinball loss quantile 0.4 hour 67: 0.030548437462580035\n",
      "Pinball loss quantile 0.4 hour 68: 0.030525612477907544\n",
      "Pinball loss quantile 0.4 hour 69: 0.03053611567497543\n",
      "Pinball loss quantile 0.4 hour 70: 0.03120450977446118\n",
      "Pinball loss quantile 0.4 hour 71: 0.030988913832557325\n",
      "Pinball loss quantile 0.4 hour 72: 0.03042678793188823\n",
      "Pinball loss quantile 0.4 hour 73: 0.03125828939993711\n",
      "Pinball loss quantile 0.4 hour 74: 0.031231556191057705\n",
      "Pinball loss quantile 0.4 hour 75: 0.031018772737364787\n",
      "Pinball loss quantile 0.4 hour 76: 0.03216311710341421\n",
      "Pinball loss quantile 0.4 hour 77: 0.030691742462847685\n",
      "Pinball loss quantile 0.4 hour 78: 0.031248842068662745\n",
      "Pinball loss quantile 0.4 hour 79: 0.03172348872278119\n",
      "Pinball loss quantile 0.4 hour 80: 0.03243981436608681\n",
      "Pinball loss quantile 0.4 hour 81: 0.03177399924403937\n",
      "Pinball loss quantile 0.4 hour 82: 0.032715634575398764\n",
      "Pinball loss quantile 0.4 hour 83: 0.032527356430112085\n",
      "Pinball loss quantile 0.4 hour 84: 0.032653579338426\n",
      "Pinball loss quantile 0.4 hour 85: 0.0332974738904833\n",
      "Pinball loss quantile 0.4 hour 86: 0.03293299568635331\n",
      "Pinball loss quantile 0.4 hour 87: 0.032825056143357345\n",
      "Pinball loss quantile 0.4 hour 88: 0.03312787508867203\n",
      "Pinball loss quantile 0.4 hour 89: 0.03369479828505473\n",
      "Pinball loss quantile 0.4 hour 90: 0.033576273322005336\n",
      "Pinball loss quantile 0.4 hour 91: 0.03306873346167567\n",
      "Pinball loss quantile 0.4 hour 92: 0.03252614209997003\n",
      "Pinball loss quantile 0.4 hour 93: 0.033040340514768465\n",
      "Pinball loss quantile 0.4 hour 94: 0.032367894070124435\n",
      "Pinball loss quantile 0.4 hour 95: 0.03266736626307819\n",
      "Pinball loss quantile 0.4 hour 96: 0.03339542250942188\n",
      "Pinball loss quantile 0.6 hour 1: 0.0038714794555684143\n",
      "Pinball loss quantile 0.6 hour 2: 0.005675994552406674\n",
      "Pinball loss quantile 0.6 hour 3: 0.007098187639573444\n",
      "Pinball loss quantile 0.6 hour 4: 0.008146658450036093\n",
      "Pinball loss quantile 0.6 hour 5: 0.009267993718240662\n",
      "Pinball loss quantile 0.6 hour 6: 0.010121413524838441\n",
      "Pinball loss quantile 0.6 hour 7: 0.01078814947473172\n",
      "Pinball loss quantile 0.6 hour 8: 0.011612726501747456\n",
      "Pinball loss quantile 0.6 hour 9: 0.012146345311985524\n",
      "Pinball loss quantile 0.6 hour 10: 0.01266280835402547\n",
      "Pinball loss quantile 0.6 hour 11: 0.013340450809117119\n",
      "Pinball loss quantile 0.6 hour 12: 0.013554599893013277\n",
      "Pinball loss quantile 0.6 hour 13: 0.013947212170722412\n",
      "Pinball loss quantile 0.6 hour 14: 0.01425994367346186\n",
      "Pinball loss quantile 0.6 hour 15: 0.014565988016529864\n",
      "Pinball loss quantile 0.6 hour 16: 0.014791269831315115\n",
      "Pinball loss quantile 0.6 hour 17: 0.015248928852378966\n",
      "Pinball loss quantile 0.6 hour 18: 0.015727572579215084\n",
      "Pinball loss quantile 0.6 hour 19: 0.0159750639747831\n",
      "Pinball loss quantile 0.6 hour 20: 0.01695333989615795\n",
      "Pinball loss quantile 0.6 hour 21: 0.016974423119432636\n",
      "Pinball loss quantile 0.6 hour 22: 0.017216472491632707\n",
      "Pinball loss quantile 0.6 hour 23: 0.01744409610853498\n",
      "Pinball loss quantile 0.6 hour 24: 0.016785887611179984\n",
      "Pinball loss quantile 0.6 hour 25: 0.017616698637125103\n",
      "Pinball loss quantile 0.6 hour 26: 0.01791813209362182\n",
      "Pinball loss quantile 0.6 hour 27: 0.018265010648256207\n",
      "Pinball loss quantile 0.6 hour 28: 0.018962790538937558\n",
      "Pinball loss quantile 0.6 hour 29: 0.01915814375254994\n",
      "Pinball loss quantile 0.6 hour 30: 0.018977124953119498\n",
      "Pinball loss quantile 0.6 hour 31: 0.0197370740208239\n",
      "Pinball loss quantile 0.6 hour 32: 0.020021951011694687\n",
      "Pinball loss quantile 0.6 hour 33: 0.020011271676983817\n",
      "Pinball loss quantile 0.6 hour 34: 0.020064054391805915\n",
      "Pinball loss quantile 0.6 hour 35: 0.0202588143677512\n",
      "Pinball loss quantile 0.6 hour 36: 0.020465740416928446\n",
      "Pinball loss quantile 0.6 hour 37: 0.020773491409537336\n",
      "Pinball loss quantile 0.6 hour 38: 0.020941671192010906\n",
      "Pinball loss quantile 0.6 hour 39: 0.021733804403625975\n",
      "Pinball loss quantile 0.6 hour 40: 0.022156871292264013\n",
      "Pinball loss quantile 0.6 hour 41: 0.022649164002065424\n",
      "Pinball loss quantile 0.6 hour 42: 0.02277827491095009\n",
      "Pinball loss quantile 0.6 hour 43: 0.023668434376298557\n",
      "Pinball loss quantile 0.6 hour 44: 0.02412830282803083\n",
      "Pinball loss quantile 0.6 hour 45: 0.023440380270719034\n",
      "Pinball loss quantile 0.6 hour 46: 0.023842954288017165\n",
      "Pinball loss quantile 0.6 hour 47: 0.024232971017763942\n",
      "Pinball loss quantile 0.6 hour 48: 0.025269862618658604\n",
      "Pinball loss quantile 0.6 hour 49: 0.024585334347040286\n",
      "Pinball loss quantile 0.6 hour 50: 0.025500439143260688\n",
      "Pinball loss quantile 0.6 hour 51: 0.025894402162498872\n",
      "Pinball loss quantile 0.6 hour 52: 0.025698747545087587\n",
      "Pinball loss quantile 0.6 hour 53: 0.026549085125024612\n",
      "Pinball loss quantile 0.6 hour 54: 0.02651985454329568\n",
      "Pinball loss quantile 0.6 hour 55: 0.02716904546670455\n",
      "Pinball loss quantile 0.6 hour 56: 0.027474033791348565\n",
      "Pinball loss quantile 0.6 hour 57: 0.02833796067188838\n",
      "Pinball loss quantile 0.6 hour 58: 0.02732878207872062\n",
      "Pinball loss quantile 0.6 hour 59: 0.027135784968763184\n",
      "Pinball loss quantile 0.6 hour 60: 0.028106479750212337\n",
      "Pinball loss quantile 0.6 hour 61: 0.028366493866341543\n",
      "Pinball loss quantile 0.6 hour 62: 0.029268221619046183\n",
      "Pinball loss quantile 0.6 hour 63: 0.028174772442446802\n",
      "Pinball loss quantile 0.6 hour 64: 0.028701392967573933\n",
      "Pinball loss quantile 0.6 hour 65: 0.029362442349525807\n",
      "Pinball loss quantile 0.6 hour 66: 0.029086166024878343\n",
      "Pinball loss quantile 0.6 hour 67: 0.030549725705853095\n",
      "Pinball loss quantile 0.6 hour 68: 0.02942112728204187\n",
      "Pinball loss quantile 0.6 hour 69: 0.031136938586808296\n",
      "Pinball loss quantile 0.6 hour 70: 0.031132542615626993\n",
      "Pinball loss quantile 0.6 hour 71: 0.031041898815904877\n",
      "Pinball loss quantile 0.6 hour 72: 0.03240649890841901\n",
      "Pinball loss quantile 0.6 hour 73: 0.0327740765431586\n",
      "Pinball loss quantile 0.6 hour 74: 0.03140225702669375\n",
      "Pinball loss quantile 0.6 hour 75: 0.03186760317656556\n",
      "Pinball loss quantile 0.6 hour 76: 0.03167889365926418\n",
      "Pinball loss quantile 0.6 hour 77: 0.0325176611620748\n",
      "Pinball loss quantile 0.6 hour 78: 0.03354538872804706\n",
      "Pinball loss quantile 0.6 hour 79: 0.03405469024069979\n",
      "Pinball loss quantile 0.6 hour 80: 0.03400426384900381\n",
      "Pinball loss quantile 0.6 hour 81: 0.03328521087670608\n",
      "Pinball loss quantile 0.6 hour 82: 0.03314063256297963\n",
      "Pinball loss quantile 0.6 hour 83: 0.03348375341101991\n",
      "Pinball loss quantile 0.6 hour 84: 0.03254340958299573\n",
      "Pinball loss quantile 0.6 hour 85: 0.033951317860743846\n",
      "Pinball loss quantile 0.6 hour 86: 0.033311449634355096\n",
      "Pinball loss quantile 0.6 hour 87: 0.03383782340017173\n",
      "Pinball loss quantile 0.6 hour 88: 0.03353166069116306\n",
      "Pinball loss quantile 0.6 hour 89: 0.033859850600115815\n",
      "Pinball loss quantile 0.6 hour 90: 0.03381400994969582\n",
      "Pinball loss quantile 0.6 hour 91: 0.03329109338074233\n",
      "Pinball loss quantile 0.6 hour 92: 0.03368374020696955\n",
      "Pinball loss quantile 0.6 hour 93: 0.033791354720353786\n",
      "Pinball loss quantile 0.6 hour 94: 0.03270858094828315\n",
      "Pinball loss quantile 0.6 hour 95: 0.032206888701673944\n",
      "Pinball loss quantile 0.6 hour 96: 0.030794652527688163\n",
      "Pinball loss quantile 0.8 hour 1: 0.0029324075540457743\n",
      "Pinball loss quantile 0.8 hour 2: 0.0043642476855881\n",
      "Pinball loss quantile 0.8 hour 3: 0.005377173392531774\n",
      "Pinball loss quantile 0.8 hour 4: 0.0062449683308785624\n",
      "Pinball loss quantile 0.8 hour 5: 0.007153189224490089\n",
      "Pinball loss quantile 0.8 hour 6: 0.0076831335050481335\n",
      "Pinball loss quantile 0.8 hour 7: 0.008352929105029466\n",
      "Pinball loss quantile 0.8 hour 8: 0.008911378913162115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinball loss quantile 0.8 hour 9: 0.009315896370814173\n",
      "Pinball loss quantile 0.8 hour 10: 0.009472579269814313\n",
      "Pinball loss quantile 0.8 hour 11: 0.009857506558892547\n",
      "Pinball loss quantile 0.8 hour 12: 0.010158869375565625\n",
      "Pinball loss quantile 0.8 hour 13: 0.010363451719883546\n",
      "Pinball loss quantile 0.8 hour 14: 0.01075345140935611\n",
      "Pinball loss quantile 0.8 hour 15: 0.010931864974769237\n",
      "Pinball loss quantile 0.8 hour 16: 0.011281129166467905\n",
      "Pinball loss quantile 0.8 hour 17: 0.01148352959592543\n",
      "Pinball loss quantile 0.8 hour 18: 0.011868749033004817\n",
      "Pinball loss quantile 0.8 hour 19: 0.011785019417743164\n",
      "Pinball loss quantile 0.8 hour 20: 0.012021563646825892\n",
      "Pinball loss quantile 0.8 hour 21: 0.012253420367902893\n",
      "Pinball loss quantile 0.8 hour 22: 0.012894859395340696\n",
      "Pinball loss quantile 0.8 hour 23: 0.012558218546389577\n",
      "Pinball loss quantile 0.8 hour 24: 0.013073992517748849\n",
      "Pinball loss quantile 0.8 hour 25: 0.013014967481242815\n",
      "Pinball loss quantile 0.8 hour 26: 0.013088903034105176\n",
      "Pinball loss quantile 0.8 hour 27: 0.01344140792481742\n",
      "Pinball loss quantile 0.8 hour 28: 0.013745102685612616\n",
      "Pinball loss quantile 0.8 hour 29: 0.013861735115195103\n",
      "Pinball loss quantile 0.8 hour 30: 0.013702596661300543\n",
      "Pinball loss quantile 0.8 hour 31: 0.013914318123293183\n",
      "Pinball loss quantile 0.8 hour 32: 0.014067592194507653\n",
      "Pinball loss quantile 0.8 hour 33: 0.014384747102455105\n",
      "Pinball loss quantile 0.8 hour 34: 0.014260892065764889\n",
      "Pinball loss quantile 0.8 hour 35: 0.01496993787382128\n",
      "Pinball loss quantile 0.8 hour 36: 0.01504604665259087\n",
      "Pinball loss quantile 0.8 hour 37: 0.0153320586819578\n",
      "Pinball loss quantile 0.8 hour 38: 0.015429280235123327\n",
      "Pinball loss quantile 0.8 hour 39: 0.01601186087284481\n",
      "Pinball loss quantile 0.8 hour 40: 0.016517533167058368\n",
      "Pinball loss quantile 0.8 hour 41: 0.016368391800907375\n",
      "Pinball loss quantile 0.8 hour 42: 0.017078593721666344\n",
      "Pinball loss quantile 0.8 hour 43: 0.01696211093852533\n",
      "Pinball loss quantile 0.8 hour 44: 0.016844042652224188\n",
      "Pinball loss quantile 0.8 hour 45: 0.01725018266440931\n",
      "Pinball loss quantile 0.8 hour 46: 0.017308250635404254\n",
      "Pinball loss quantile 0.8 hour 47: 0.017785599514778254\n",
      "Pinball loss quantile 0.8 hour 48: 0.01821355756316601\n",
      "Pinball loss quantile 0.8 hour 49: 0.017857843968207138\n",
      "Pinball loss quantile 0.8 hour 50: 0.01877403462109004\n",
      "Pinball loss quantile 0.8 hour 51: 0.01900704259732097\n",
      "Pinball loss quantile 0.8 hour 52: 0.018821982584199162\n",
      "Pinball loss quantile 0.8 hour 53: 0.019304318264247652\n",
      "Pinball loss quantile 0.8 hour 54: 0.020423784116860716\n",
      "Pinball loss quantile 0.8 hour 55: 0.019794805629199712\n",
      "Pinball loss quantile 0.8 hour 56: 0.020284035108194053\n",
      "Pinball loss quantile 0.8 hour 57: 0.02128067274111605\n",
      "Pinball loss quantile 0.8 hour 58: 0.02012077920075316\n",
      "Pinball loss quantile 0.8 hour 59: 0.02144324671246166\n",
      "Pinball loss quantile 0.8 hour 60: 0.020690360709688483\n",
      "Pinball loss quantile 0.8 hour 61: 0.02149251792419561\n",
      "Pinball loss quantile 0.8 hour 62: 0.021275980269420163\n",
      "Pinball loss quantile 0.8 hour 63: 0.021219773853446748\n",
      "Pinball loss quantile 0.8 hour 64: 0.02230196662176062\n",
      "Pinball loss quantile 0.8 hour 65: 0.023210934687131927\n",
      "Pinball loss quantile 0.8 hour 66: 0.021516835615023905\n",
      "Pinball loss quantile 0.8 hour 67: 0.020865081868507756\n",
      "Pinball loss quantile 0.8 hour 68: 0.02217901678672945\n",
      "Pinball loss quantile 0.8 hour 69: 0.023479575089286185\n",
      "Pinball loss quantile 0.8 hour 70: 0.02222033220473841\n",
      "Pinball loss quantile 0.8 hour 71: 0.02403577328340672\n",
      "Pinball loss quantile 0.8 hour 72: 0.023083589535045218\n",
      "Pinball loss quantile 0.8 hour 73: 0.02426108733897859\n",
      "Pinball loss quantile 0.8 hour 74: 0.0235102123747557\n",
      "Pinball loss quantile 0.8 hour 75: 0.023209261296952416\n",
      "Pinball loss quantile 0.8 hour 76: 0.024368708269665196\n",
      "Pinball loss quantile 0.8 hour 77: 0.02386525794084348\n",
      "Pinball loss quantile 0.8 hour 78: 0.023842556485361762\n",
      "Pinball loss quantile 0.8 hour 79: 0.025312166706096737\n",
      "Pinball loss quantile 0.8 hour 80: 0.024394846823472406\n",
      "Pinball loss quantile 0.8 hour 81: 0.024948891269689553\n",
      "Pinball loss quantile 0.8 hour 82: 0.024532496704229988\n",
      "Pinball loss quantile 0.8 hour 83: 0.025697557938256052\n",
      "Pinball loss quantile 0.8 hour 84: 0.02690182802409979\n",
      "Pinball loss quantile 0.8 hour 85: 0.024178653953900914\n",
      "Pinball loss quantile 0.8 hour 86: 0.025449149103518882\n",
      "Pinball loss quantile 0.8 hour 87: 0.026043309857790537\n",
      "Pinball loss quantile 0.8 hour 88: 0.02477648893188749\n",
      "Pinball loss quantile 0.8 hour 89: 0.027444048856603336\n",
      "Pinball loss quantile 0.8 hour 90: 0.02543615168334556\n",
      "Pinball loss quantile 0.8 hour 91: 0.025917344955266104\n",
      "Pinball loss quantile 0.8 hour 92: 0.026220766709178483\n",
      "Pinball loss quantile 0.8 hour 93: 0.02431987917419771\n",
      "Pinball loss quantile 0.8 hour 94: 0.02501774437697606\n",
      "Pinball loss quantile 0.8 hour 95: 0.023664539958925656\n",
      "Pinball loss quantile 0.8 hour 96: 0.02547086775003702\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "quantiles = [.2, .4, .6, .8]\n",
    "models = list()\n",
    "\n",
    "df_losses_per_quantile = DataFrame()\n",
    "l_preds_per_quant = list()\n",
    "l_actuals_per_quant = list()\n",
    "\n",
    "for quantile in quantiles:\n",
    "    losses_per_hour = list()\n",
    "    predict = DataFrame()\n",
    "    actual = DataFrame()\n",
    "    #preds = list()\n",
    "    for h in range(96):\n",
    "        loss, test, predictions, model= lgbm_final_fit(full_train_X, full_train_y, test_X, test_y, h, quant = quantile)\n",
    "        actual['pred_hour_{}_actual'.format(h+1)] = pd.Series(test)\n",
    "        predict['pred_hour_{}_quant_{}'.format(h+1,str(quantile)[-1])] = pd.Series(predictions)\n",
    "        print('Pinball loss quantile {} hour {}: {}'.format(quantile, h+1, loss))\n",
    "        losses_per_hour.append(loss)\n",
    "        models.append(model)\n",
    "    df_losses_per_quantile['loss_q{}'.format(str(quantile)[-1])] = losses_per_hour\n",
    "    l_preds_per_quant.append(predict)\n",
    "    l_actuals_per_quant.append(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load saved predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_h_J0hiT1ugs"
   },
   "outputs": [],
   "source": [
    "#with open(\"/content/drive/MyDrive/Colab Notebooks/Predictions/lightgbm_preds_{}.txt\".format(date), \"rb\") as fp:   # Unpickling\n",
    "#    preds_loaded_reg = pickle.load(fp)\n",
    "#\n",
    "#with open(\"/content/drive/MyDrive/Colab Notebooks/Predictions/lightgbm_actuals_{}.txt\".format(date), \"rb\") as fp:   # Unpickling\n",
    "#    actuals_loaded_reg = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse scale predictions and calculate accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply inverse scaling to data\n",
    "def extract_predictions(df_actuals, df_predictions1):\n",
    "    steps = 96\n",
    "    df_predictions = DataFrame()\n",
    "    l_actuals_inverse = list()\n",
    "    l_predictions_inverse = list()\n",
    "    for quantile in range(len(quantiles)):\n",
    "        df_inv_preds = DataFrame()\n",
    "        df_inv_acts = DataFrame()\n",
    "        for step in range(95):\n",
    "            inv_yhat = scaler1.inverse_transform(np.array(df_predictions1[quantile].iloc[:, step]).reshape(-1, 1))\n",
    "            df_inv_preds['pred_step_{}_quant_{}'.format(step, quantiles[quantile])] = pd.Series(inv_yhat.reshape(inv_yhat.shape[0]))\n",
    "\n",
    "            inv_act = scaler1.inverse_transform(np.array(df_actuals[quantile].iloc[:, step]).reshape(-1, 1))\n",
    "            df_inv_acts['actual_step_{}_quant_{}'.format(step, quantiles[quantile])] = pd.Series(inv_act.reshape(inv_act.shape[0]))\n",
    "        l_predictions_inverse.append(df_inv_preds)\n",
    "        l_actuals_inverse.append(df_inv_acts) \n",
    "        \n",
    "    return l_actuals_inverse, l_predictions_inverse\n",
    "\n",
    "def get_accuracies(df_actuals1, df_predictions1, quantiles):\n",
    "    df_loss_scores = DataFrame()\n",
    "    for quantile in range(len(quantiles)):\n",
    "        loss_scores = list()\n",
    "        for step in range(95):\n",
    "            ploss = np.mean(quantile_loss(df_actuals1[quantile].iloc[:,step], df_predictions1[0].iloc[:,step], quantiles[quantile]))\n",
    "            loss_scores.append(ploss)\n",
    "        df_loss_scores['scores_quant_{}'.format(str(quantiles[quantile]))] = pd.Series(loss_scores)\n",
    "    \n",
    "    return df_loss_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse scale predictions and get accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acts, df_preds = extract_predictions(l_actuals_per_quant, l_preds_per_quant)\n",
    "df_loss_all = get_accuracies(df_acts, df_preds, quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average pinball loss scores pre quantile: \n",
      "scores_quant_0.2    0.308928\n",
      "scores_quant_0.4    0.455814\n",
      "scores_quant_0.6    0.602700\n",
      "scores_quant_0.8    0.749587\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Average pinball loss scores pre quantile: ')\n",
    "print(df_loss_all.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall average loss score of the LGBM model is: 0.5293\n"
     ]
    }
   ],
   "source": [
    "avergage_loss_lgbm = round(df_loss_all.mean().mean(), 4)\n",
    "print('The overall average loss score of the LGBM model is: {}'.format(avergage_loss_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores per quantile:\n",
      "scores_quant_0.2    0.143092\n",
      "scores_quant_0.4    0.213841\n",
      "scores_quant_0.6    0.284590\n",
      "scores_quant_0.8    0.355339\n",
      "Name: 7, dtype: float64\n",
      "Average:  0.24921577566709896\n"
     ]
    }
   ],
   "source": [
    "# Loss scores 2 hours ahead\n",
    "l_2 = df_loss_all.iloc[7,:]\n",
    "print('Scores per quantile:')\n",
    "print(l_2)\n",
    "\n",
    "# Average loss score 2 hours ahead\n",
    "print('Average: ',l_2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores per quantile:\n",
      "scores_quant_0.2    0.258509\n",
      "scores_quant_0.4    0.387039\n",
      "scores_quant_0.6    0.515569\n",
      "scores_quant_0.8    0.644098\n",
      "Name: 31, dtype: float64\n",
      "Average:  0.45130385812507917\n"
     ]
    }
   ],
   "source": [
    "# Loss scores 8 hours ahead\n",
    "l_8 = df_loss_all.iloc[31,:]\n",
    "print('Scores per quantile:')\n",
    "print(l_8)\n",
    "\n",
    "# Average loss score 8 hours ahead\n",
    "print('Average: ',l_8.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores per quantile:\n",
      "scores_quant_0.2    0.360373\n",
      "scores_quant_0.4    0.526284\n",
      "scores_quant_0.6    0.692195\n",
      "scores_quant_0.8    0.858106\n",
      "Name: 51, dtype: float64\n",
      "Average:  0.6092393771452238\n"
     ]
    }
   ],
   "source": [
    "# Loss scores 14 hours ahead\n",
    "l_14 = df_loss_all.iloc[51,:]\n",
    "print('Scores per quantile:')\n",
    "print(l_14)\n",
    "\n",
    "# Average loss score 14 hours ahead\n",
    "print('Average: ',l_14.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores per quantile:\n",
      "scores_quant_0.2    0.416856\n",
      "scores_quant_0.4    0.606259\n",
      "scores_quant_0.6    0.795661\n",
      "scores_quant_0.8    0.985064\n",
      "Name: 79, dtype: float64\n",
      "Average:  0.7009599003241451\n"
     ]
    }
   ],
   "source": [
    "# Loss scores 20 hours ahead\n",
    "l_20 = df_loss_all.iloc[79,:]\n",
    "print('Scores per quantile:')\n",
    "print(l_20)\n",
    "\n",
    "# Average loss score 20 hours ahead\n",
    "print('Average: ',l_20.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUQN_JmrqplO"
   },
   "source": [
    "### Compute feature importance for predictions 2 hours ahead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "7wh90VZQ_Vbi",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA80AAAGxCAYAAABYyVRmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzde7xdVX33+88XomgCapXSICpEEMOtUNwFQbkVfdBTa1Xow6M0UfAYH9QDiPeqCPTBAJFWKr1AW4m22OOxoiC2YLHgNYAbrEADIlgURBSEQiDILb/zx5wbFit7Jnsle2fthM/79Vqv7DXHmGP81tr7n2/GnGOmqpAkSZIkSSvbaNgFSJIkSZI0XRmaJUmSJEnqYGiWJEmSJKmDoVmSJEmSpA6GZkmSJEmSOswYdgHSIF71qlfVhRdeOOwyJEmSJG1Y0tXgSrPWK3feeeewS5AkSZL0JGJoliRJkiSpg6FZkiRJkqQOqaph1yBN2KzZc2ruvBOGXYYkSZKkNXTlovnDLmE83tMsSZIkSdKgDM2SJEmSJHUwNEuSJEmS1MHQLEmSJElSB0OzJEmSJEkdDM2SJEmSJHUwNEuSJEmS1MHQLEmSJElSB0OzJEmSJEkdDM2SJEmSJHUwNG8gkuyfpJJsPgVj75fkhiQbT8HYuyT5WZJZkz22JEmSJK0tQ/N6KMnNSd7bd/i7wJbAr6ZgykXASVX1aDv/8UmunciJST6c5DtJ7k9S/e1VdQ1wGXDspFYsSZIkSZPA0LyBqKqHqur2qlopmK6NJHsDc4EvrOEQmwDnAp9cRZ+zgSOTzFjDOSRJkiRpSqy3oTmN9yT5UZIHk9yaZGHbtkuSi5M8kOSuJIuTPLPn3MVJLkhydHtp8N1Jzk4ys6fPvkkuS3JfknuSXJ5k57btLUnu66vnCZdHj/VJ8uok1ydZnuT8JM9Mckhb9z1J/iHJ03vGuTTJ3yQ5va3r7iSLkmw01g5sDSxq56vx5m+PvSHJNe33c0u76pue9puTfCTJmUnubb/D9/V91W8CLq6q5WOfC/gYsNPY/O2xcVXVcVV1GvD9Vfw6vwY8G9h/FX0kSZIkaZ1bb0Mz8HHgo8BCYCfgj4Bb2uB7IXAfsAfwemBv4NN95+8D7Ay8Aji07Xc0QLvieR7wbWBXYE/gdODRAWvcBHgPcBhwIDAC/DPwZuBg4HXAa4B39J13GM3vZi/g7cAC4Ji27Q3ArcCJNJdjbznexEleQrM6fC6wC/BB4EPAu/q6vhu4BtgdOAU4NclePe37AKM97z8PnAb8sGf+z6/qS1idqnoI+A9gv47PsiDJaJLRR5YvW5upJEmSJGkg6+XlsEk2pQl7x1TVWBi+EViS5G3ApsC8qlrW9l8AXJJku6q6se1/L3BkVT0CXJfkCzTBdiHwDOBZwFeq6qa2//VrUOoM4J1V9cO2js+1df9WVd3ZHjsPOIAmiI75OXBUe6n19Um2p7nn98+q6q4kjwLLqur2Vcx9LPCNqvpY+/6GJC8CPgB8qqff16rqjPbnTyU5iuZ7WNIe27qtB4CqeqBdZX9kNfMP6jZgm/Eaquos4CyAWbPnTOrl55IkSZK0KuvrSvOONKu4Xx+nbQfg6rHA3PousKI9b8zSNjCPuQ3YAqCq7gIWAxcl+WqSY5M8fw3qfHAsMLd+Adw+Fph7jm3Rd95lffcmLwG2SvKMAebeAfhO37FvjzPO1X19HvseWk8Hfr26ydpLyu8bew1Q55gH2rkkSZIkadpYX0NzVtPWtRrZe/zhcdoe+z6q6nCay7K/CbyWZqX2oLZ5xTg1PGWc+R7pe1+rm3cSTcr3ANwJ/MYE5jsO2K3nNahnA3eswXmSJEmSNGXW19C8FHiQ5jLi8dp2TbJZz7G9aT7rdYNMUlU/qKpTqmp/4FKae5GhCXcz+1Zs1yQodtmzd8Mu4KXAbVV1b/v+IWB1z0xeCry879jLgVv7VuFX5/s8cYV+3Pmr6pdVdePYa4Dxx+wMXLUG50mSJEnSlFkvQ3Mb+k4HFiY5PMm2SfZIciRwDnA/8Nl2F+19gTOBcyca5pLMSXJykr2TbJ3kAOC3aYIowOXtHAuTbJfkYFbezGttPBf4ZJIXJzkEeB/w5z3tNwP7JNmqd7fsPqcB+6V5pvL2SQ6j2ZTs1AFruYiVw/fNwNZJdk+yeZJNuk5O8oIku9Her5xkt/a1aU+fbYCtaHbRliRJkqRpY70Mza0P0ez2/FGaFeQvAs9rH410EM1mXlfQ7IK9BDhigLGXA9vT7D59A/AZmjB+Cjx2z/NhwCtpdp5e0NYxWc6hWcm9HPhb4O95Ymg+Dng+cBMdlzRX1VU0O4ofDFwLnNy+zhiv/yr8I7B9kp16jn0R+Beae8rvAN64ivNPpFmtXtS+/377Gunp80aaDcl+MmBtkiRJkjSl8sT9pjRs7XOYr62q/kdDDU2Sk4HfrKq3TsHYmwA/At5YVf0bl61k1uw5NXfeCZNdhiRJkqR15MpF84ddwng6981an1eate58HPhxktXdR70mtgZOmkhgliRJkqR1bb18TrPWrXYDspOmaOwbaC6BlyRJkqRpx9A8zbQ7dUuSJEmSpgEvz5YkSZIkqYOhWZIkSZKkDoZmSZIkSZI6GJolSZIkSepgaJYkSZIkqUOqatg1SBM2MjJSo6Ojwy5DkiRJ0oYlXQ2uNEuSJEmS1MHQLEmSJElSB0OzJEmSJEkdDM2SJEmSJHUwNEuSJEmS1MHQLEmSJElSBx85pfXKrNlzau68E4ZdhiRJWkNXLpo/7BIkaTw+ckqSJEmSpEEZmiVJkiRJ6mBoliRJkiSpg6FZkiRJkqQOhmZJkiRJkjoYmiVJkiRJ6mBoliRJkiSpg6FZkiRJkqQOhmZJkiRJkjoYmiVJkiRJ6mBoliRJkiSpg6FZACS5NMkZw65DkiRJkqYTQ7MkSZIkSR0MzSLJYmA/4J1Jqn1tk2THJF9NsizJL5P8U5LZvecluSDJB5LcnuSeJCcn2SjJ8e05tyf5QN98leRd7djLk/wkyR+v448tSZIkSatlaBbA0cAS4Gxgy/b1MPBN4FpgD+AVwKbA+Ul6/272BeYA+wP/G3g/8C/AJsDLgeOBk5O8pG/OE4Dzgd2As4DPJhkZr7gkC5KMJhl9ZPmytf2skiRJkjRhM4ZdgIavqu5J8hCwvKpuB0hyIvCDqnpslTjJfOAuYAS4oj18D/DOqnoUuD7Je4DnVtWr2vYbknwQOAC4smfac6vqzPbnk5IcABwDrLTiXFVn0QRrZs2eU5PyoSVJkiRpAgzN6vISYN8k943Tti2Ph+albWAe8wvgv/v6/wLYou/YknHe//4a1ipJkiRJU8LQrC4bAV8F3jtO2y96fn64r606jnkrgCRJkqT1jqFZYx4CNu55fxXwP4GfVFV/CJ4MLwU+3ff+uimYR5IkSZLWmKt/GnMzsEe7a/bmwF8CzwQ+n2TPJC9M8ookZyXZbBLme0OStyV5UZIPAQcCn5yEcSVJkiRp0hiaNeYTNKvNS4E7gKcCLwNWABcC/0kTpB9sX2vreOBg4GrgSODwqvreJIwrSZIkSZPGy7MFQFXdAOw1TtMhqzjnLeMce804x146zum39+ywLUmSJEnTkivNkiRJkiR1MDRLkiRJktTBy7O1zlVVhl2DJEmSJE2EK82SJEmSJHUwNEuSJEmS1MHQLEmSJElSB0OzJEmSJEkdUlXDrkGasJGRkRodHR12GZIkSZI2LJ2bFbvSLEmSJElSB0OzJEmSJEkdDM2SJEmSJHUwNEuSJEmS1MHQLEmSJElSB0OzJEmSJEkdDM2SJEmSJHXwOc1ar8yaPafmzjth2GVIkgZ05aL5wy5BkqRV8TnNkiRJkiQNytAsSZIkSVIHQ7MkSZIkSR0MzZIkSZIkdTA0S5IkSZLUwdAsSZIkSVIHQ7MkSZIkSR0MzZIkSZIkdTA0S5IkSZLUwdAsSZIkSVIHQ7NWKUklOWQdzHNGkkuneh5JkiRJGoShWZIkSZKkDoZmSZIkSZI6GJpFGu9J8qMkDya5NcnCjr67JLk4yQNJ7kqyOMkze9oXJ7mg75zjk1zb837jJJ9Icnf7+iSw8ZR9QEmSJElaQ4ZmAXwc+CiwENgJ+CPglv5OSWYCFwL3AXsArwf2Bj494HzvAd4GvB3YiyYwH9bVOcmCJKNJRh9ZvmzAqSRJkiRpzc0YdgEariSbAu8GjqmqsfB7I7BknO6HAZsC86pqWXv+AuCSJNtV1Y0TnPYY4NSq+v/aMY4GDurqXFVnAWcBzJo9pyY4hyRJkiStNVeatSOwCfD1CfTdAbh6LDC3vgusaMdZrfZS7i3pCeVVtQK4fKIFS5IkSdK6YmhWBuzbtdI7dnzFOGM+ZdCiJEmSJGk6MDRrKfAgcOAE++6aZLOeY3vT/B1d176/g2YludduYz9U1T3Az4GXjh1LEpp7pCVJkiRpWjE0P8m1l1qfDixMcniSbZPskeTIcbqfA9wPfLbdRXtf4Ezg3J77mf8d+J0kRyTZLsn7gZf1jXM68P4khyR5MfBJVg7akiRJkjR0hmYBfAg4hWYH7euALwLP6+9UVctpNux6BnAFcB7NvclH9PS5CDgBOAm4EtgG+Ku+oU4Dzgb+juZe5o1oArkkSZIkTSupcjNirT9mzZ5Tc+edMOwyJEkDunLR/GGXIEnSqnTu9eRKsyRJkiRJHQzNkiRJkiR1MDRLkiRJktTB0CxJkiRJUgdDsyRJkiRJHQzNkiRJkiR1MDRLkiRJktTB5zRrvTIyMlKjo6PDLkOSJEnShsXnNEuSJEmSNChDsyRJkiRJHQzNkiRJkiR1MDRLkiRJktTB0CxJkiRJUgdDsyRJkiRJHQzNkiRJkiR18DnNWq/Mmj2n5s47YdhlSNIG5cpF84ddgiRJw+ZzmiVJkiRJGpShWZIkSZKkDoZmSZIkSZI6GJolSZIkSepgaJYkSZIkqYOhWZIkSZKkDoZmSZIkSZI6GJolSZIkSepgaJYkSZIkqYOhWeNKcmmSM4ZdhyRJkiQNk6FZkiRJkqQOhmZNG0k2SrLxsOuQJEmSpDGGZq3KRkk+nuTOJL9M8okkGwEk+Y0kn0lyd5IHklycZKexE5O8Jcl9vYMl2T9JJdm8t0+S/yvJtcBDwA7r8gNKkiRJ0qoYmrUqhwGPAHsD7wKOAQ5t2xYDewJ/COwBLAcuTPL0Aed4GvAR4O3AjsBP1rpqSZIkSZokM4ZdgKa1pVV1XPvzDUneBhyYZBR4LbBfVX0TIMk84Kc0QfvvBphjY+D/qaoruzokWQAsAHjqZs8Z/FNIkiRJ0hpypVmrcnXf+9uALWguoV4BLBlrqKp7gGtoVosH8QjwH6vqUFVnVdVIVY3MmLnZgMNLkiRJ0pozNGtVHu57XzR/M1nFOdX+u2Kcfk8Zp/+DVfXompUnSZIkSVPL0Kw1sZTmb2evsQNJngHs0rYB3AHMbI+P2W2dVShJkiRJk8DQrIFV1Y+A84Azk+yTZBfgH4F7gc+13S4H7gcWJtkuycHAO4ZSsCRJkiStIUOz1tThwBXA+e2/M4FXVdUDAFV1F82mYK+kudd5AfDR4ZQqSZIkSWsmVbX6XtI0MWv2nJo774RhlyFJG5QrF80fdgmSJA1b575NrjRLkiRJktTB0CxJkiRJUgdDsyRJkiRJHQzNkiRJkiR1MDRLkiRJktTB0CxJkiRJUgdDsyRJkiRJHQzNkiRJkiR1SFUNuwZpwkZGRmp0dHTYZUiSJEnasKSrwZVmSZIkSZI6GJolSZIkSepgaJYkSZIkqYOhWZIkSZKkDoZmSZIkSZI6GJolSZIkSepgaJYkSZIkqYPPadZ6ZdbsOTV33gnDLkOS1rkrF80fdgmSJG3IfE6zJEmSJEmDMjRLkiRJktTB0CxJkiRJUgdDsyRJkiRJHQzNkiRJkiR1MDRLkiRJktRhjUJzko2S7JxkvySzJrsoSZIkSZKmg4FDc5J3ArcDPwD+HXhxe/zLSY6a3PIkSZIkSRqegUJzkrcBpwNfBg7liQ+A/hZw8OSVJkmSJEnScA260nwscFpVLQC+1Nd2Pe2qs54ckmyTpJKMrOU4hySpyapLkiRJkibLjAH7zwEu6mi7H3jW2pWj9cwtwJbAncMuRJIkSZKmwqCh+U5gm462FwM/W6tqtF6pqkdp7m+XJEmSpA3SoJdnfwU4LskLe45Vks2Bd9Pc66z1WJJXJ1mWZEb7/kXtJdh/3dPnpCT/1n95dpL92/cHJrk8yfIko0l275tjfpKftO0XAL+1Tj+kJEmSJE3QoKH5I8CDwLXAxUABfwFcBzwKnDip1WkYvgU8DRi7T3l/misMDujpsz9w6SrGWAh8ENgd+BVwTpIAJNkTWAycBexG8x8x/t1IkiRJmpYGCs1V9SuaMLUQeApwE80l3mcAe1XVPZNeodapqroPuIrHQ/L+NL/frZNsmWQm8LusOjR/tKouqarraQLxXGCrtu1o4OtVdVJV3VBVZ7LypnJPkGRBu2I9+sjyZWv60SRJkiRpYBMOzUk2TrIr8LSq+tOqenlVbV9Ve1XVCVV17xTWqXXrUpqwDLAf8K/AFe2xlwEPt++7XN3z823tv1u0/+4ALOnr3//+CarqrKoaqaqRGTM3W03pkiRJkjR5BllpLmAU+J0pqkXTx6XAy5LsCGwGXNkeO4AmOH+3qh5exfm9bWOPkhr7WwuSJEmStJ6YcGiuqhU0jxiaNXXlaJr4FrAJ8H7g2+0u2ZfyeGi+dC3GXgq8tO9Y/3tJkiRJmhYG3QjsTOCYJE+dimI0PfTc1/zHwCXt4SXA84E9WbvQ/BfAK5J8qN2Z+23A69diPEmSJEmaMoM+p3kzYFvgx0kuBH7O45ffAlRVfWyyitNQXQLsQRuQq+rXSS6j2QRsVfczr1JVXZbkrcAJwHHt+McDn1q7ciVJkiRp8qWqVt9rrHOyYjVdqqo2XruSpG6zZs+pufNOGHYZkrTOXblo/rBLkCRpQ9a599JAK81VNejl3JIkSZIkrbcMwZIkSZIkdTA0S5IkSZLUYaDLs9t7mld5E7T3NEuSJEmSNhSD7p59IiuH5ucA/4Pmub6LJ6EmSZIkSZKmhUE3Ajt+vONJNga+AtwzCTVJkiRJkjQtTMo9zVX1KPBXwDGTMZ4kSZIkSdPBZG4Etgnw7EkcT5IkSZKkoRp0I7AXjHP4qcDOwMnA6GQUJXXZ4XnPYXTR/GGXIUmSJOlJYtCNwG5m/N2zA9wEvHNtC5IkSZIkaboYNDQfwcqh+dfAT4Dvtfc2S5IkSZK0QRh09+zFU1SHJEmSJEnTzkAbgSX5cZJdO9p2TvLjySlLkiRJkqThG3T37G1odskez9OArdeqGkmSJEmSppE1eeTUeBuBAYwA/70WtUiSJEmSNK2s9p7mJO8G3t2+LeArSR7q6/Z0mmc0/7+TW54kSZIkScMzkY3Afgx8vf35zTTPYr6jr8+DwFLg7yavNGll1936K17yvs8OuwxJWsmVPkNekqQN0mpDc1WdB5wHkATgxKr6rymuS5IkSZKkoRv0kVOHT1UhkiRJkiRNNwOFZoAkTwVeDbyYZsfsXlVVfzoZhUmSJEmSNGwDheYkzwW+TfPoqQLSNvXuqG1oliRJkiRtEAZ95NQimk3AXkATmPcEXgicBNzY/ixJkiRJ0gZh0Muz9wHeC9zWvl9RVTcDxyXZGPgL4A8nrzxJkiRJkoZn0JXm5wC3VdUK4H7gN3ra/h3Yf5LqkiRJkiRp6AYNzbcCm7c/3wT8j562PYBfT0ZRkiRJkiRNB4Nenn0JsB/wZeBM4C+T7AY8DBzUHpMkSZIkaYMwaGj+CPBsgKr66yQzgEOBmcCpwImTW57WF0kuAO6sqrcMuxZJkiRJmiwDheaquhO4s+f9p4BPTXZRkiRJkiRNB4Pe0wxAko2S7JxkvySzJrsoSZIkSZKmg4FDc5J3ArcDP6DZMfvF7fEvJzlqcsvTdJRkZpLFSe5L8oskf9LX/sdJvpdkWZJfJvlCkq3atiS5Mcl7+855UZJKsvu6/CySJEmStCoDheYkbwNOp9kI7FAgPc3fAg6evNI0jX0CeCXN7/tA4HeAfXvanwp8DNgVeA3Njuv/BFBVBfw9cETfmEcA/1FVV01p5ZIkSZI0gEFXmo8FTquqBcCX+tqup1111oYryabAW4H3V9VFVXUtcDiwYqxPVX26qv6lqn5cVVcARwL7JHle2+Vs4EVJXtqOuTEwnyZMjzfngiSjSUYfWb5s6j6cJEmSJPUZNDTPAS7qaLsfeNbalaP1wLY0K8lLxg5U1X3ANWPvk+ye5LwkP0myDBhtm17Q9r8duIDHV5tfBTwHOGe8CavqrKoaqaqRGTM3m+zPI0mSJEmdBg3NdwLbdLS9GPjZWlWj9UFW2dhsDHcRsByYB/wuTSiGJmyP+Tvg0CQzacLzuVV19+SXK0mSJElrbtDQ/BXguCQv7DlWSTYH3k1zr7M2bDcCDwMvHTvQBuWd27dzae5h/pOq+mZVXQ9sMc44FwL3Av8b+APg01NZtCRJkiStiUFD80eAB4FrgYuBAv4CuA54FDhxUqvTtNNeiv33wClJXplkJ5rAu3Hb5ac0fyPvSvLCJL8P/Ok44zzanreQ5gqFr6+L+iVJkiRpEAOF5qr6FTBCE3SeAtwEzADOAPaqqnsmvUJNR+8FLqHZDO4Smv9E+SZAVd0BvBl4HbCUZhftYzvG+TTNJdtnt7tqS5IkSdK0MmN1HZL8HnBFu8JIVS2jWTlcafVQTw5VdT/NbtfzO9o/D3y+7/B490LPprlCYfFk1idJkiRJk2UiK83/Buw49ibJRkm+meRFU1eWNmRJNkmyHfB/gC9V1U+HXZMkSZIkjWciobl/hTDAywGf/aM19UbghzSPmeq6dFuSJEmShm7QjcCktVZVi6tq46ravapuGXY9kiRJktTF0CxJkiRJUofVbgTW2qrn2cwb9xz77/6OVfXjSalMkiRJkqQhm2ho/udxjn25o+/GHcclSZIkSVqvTCQ0Hz7lVUiSJEmSNA2tNjRX1WfWRSGSJEmSJE03qaph1yBN2MjISI2Ojg67DEmSJEkblv5HLT/G3bMlSZIkSepgaJYkSZIkqYOhWZIkSZKkDoZmSZIkSZI6GJolSZIkSepgaJYkSZIkqYOhWZIkSZKkDj6nWeuVWbPn1Nx5Jwy7DElT6MpF84ddgiRJevLxOc2SJEmSJA3K0CxJkiRJUgdDsyRJkiRJHQzNkiRJkiR1MDRLkiRJktTB0CxJkiRJUgdDsyRJkiRJHQzNkiRJkiR1MDRLkiRJktTB0Kx1Ksn+SSrJ5sOuRZIkSZJWx9CsKZPk0iRnDLsOSZIkSVpThmZJkiRJkjoYmp8E2hXfv05yWpK7ktyR5OgkmyT5yyT/neSnSeb1nLNLkouTPNCeszjJM3vaFye5oB3nZ0nuTnJ2kplj7cB+wDvby7EryTY9Ze2a5PIky5OMJtl93XwbkiRJkjRxhuYnj8OAZcCewMnAJ4EvAzcAI8BngL9L8tw2+F4I3AfsAbwe2Bv4dN+Y+wA7A68ADm37Hd22HQ0sAc4Gtmxft/ScuxD4ILA78CvgnCSZvI8rSZIkSWvP0Pzk8Z9VdXxV/Qj4M+BO4OGqOr2qbgROBEITjg8DNgXmVdU1VfUNYAHwhiTb9Yx5L3BkVV1XVV8DvgAcCFBV9wAPAcur6vb29WjPuR+tqkuq6vp27rnAVuMVnmRBuxo9+sjyZZP2hUiSJEnS6hianzyuHvuhqgr4JXBNz7GHgbuBLYAdgKurqjehfhdYAezYc2xpVT3S8/629vyB6mnPo+vcqjqrqkaqamTGzM0mOLwkSZIkrT1D85PHw33vq+PYRjQrztUxTu/xrvMHrWdsTP8eJUmSJE0rhhSNZynNRl29y7p70/y9XDfAOA8BG09mYZIkSZK0LhmaNZ5zgPuBz7a7aO8LnAmc297/PFE3A3sk2SbJ5kn8e5MkSZK0XjHEaCVVtRw4CHgGcAVwHs1O2EcMONQnaFablwJ3AC+YxDIlSZIkacql2RNKWj/Mmj2n5s47YdhlSJpCVy6aP+wSJEnSk0/n429daZYkSZIkqYOhWZIkSZKkDoZmSZIkSZI6GJolSZIkSepgaJYkSZIkqYOhWZIkSZKkDoZmSZIkSZI6GJolSZIkSeqQqhp2DdKEjYyM1Ojo6LDLkCRJkrRhSVeDK82SJEmSJHUwNEuSJEmS1MHQLEmSJElSB0OzJEmSJEkdDM2SJEmSJHUwNEuSJEmS1MHQLEmSJElSB5/TrPXKrNlzau68E4ZdhvSkdOWi+cMuQZIkaar4nGZJkiRJkgZlaJYkSZIkqYOhWZIkSZKkDoZmSZIkSZI6GJolSZIkSepgaJYkSZIkqYOhWZIkSZKkDoZmSZIkSZI6GJolSZIkSepgaB6yJPsnqSSbT8HY+yW5IcnGkz32ZEnyriTnD7sOSZIkSRqPoXkdSnJzkvf2Hf4usCXwqymYchFwUlU92s5/fJJrJ3Jikg8n+U6S+5NUR58Dk3w3ybIkP09ySpIZPe37JzmvbVue5OokR/QN87fASJJ91vAzSpIkSdKUMTQPWVU9VFW3V9W4wXRNJdkbmAt8YQ2H2AQ4F/hkx/i/DfwL8DXgd4D/BbwWOLmn297ANcAhwM7AXwNnJXnTWIeqehD4HHDUGtYpSZIkSVNmnYfmNN6T5EdJHkxya5KFbdsuSS5O8kCSu5IsTvLMnnMXJ7kgydFJfpbk7iRnJ5nZ02ffJJcluS/JPUkuT7Jz2/aWJPf11fOEy6PH+iR5dZLr2xXS85M8M8khbd33JPmHJE/vGefSJH+T5PS2rruTLEqy0Vg7sDWwqJ2vxpu/PfaGJNe0388t7apvetpvTvKRJGcmubf9Dt/X91W/Cbi4qpaPfS7gY8BOY/O3x8ZVVcdV1WnA9zu6/C9gaVUdX1U3VtU3gPcD70yyWTvGx6vqI1X1nar6cVX9NU0QP7hvrPOB1/b+HiVJkiRpOhjGSvPHgY8CC4GdgD8CbmkD04XAfcAewOtpVio/3Xf+PjSrlq8ADm37HQ3QXhp8HvBtYFdgT+B04M3tpYcAACAASURBVNEBa9wEeA9wGHAgMAL8M/BmmsD3OuA1wDv6zjuM5jvdC3g7sAA4pm17A3ArcCLN5dhbjjdxkpfQrA6fC+wCfBD4EPCuvq7vplnF3R04BTg1yV497fsAoz3vPw+cBvywZ/7Pr+pLWI1NgF/3HXsAeBrwklWc9wzg7r5jo8AMmu9NkiRJkqaNGavvMnmSbEoT9o6pqrEwfCOwJMnbgE2BeVW1rO2/ALgkyXZVdWPb/17gyKp6BLguyRdogu1CmkD2LOArVXVT2//6NSh1BvDOqvphW8fn2rp/q6rubI+dBxxAE0TH/Bw4qr3U+vok2wPHAn9WVXcleRRYVlW3r2LuY4FvVNXH2vc3JHkR8AHgUz39vlZVZ7Q/fyrJUTTfw5L22NZtPQBU1QPtKvsjq5l/oi4C3p1kHvBPwG8Bx7VtXf8h8Jq2xpf1Hq+q5UnuAbbpOG8BzX9A8NTNnjMJpUuSJEnSxKzrleYdaVYovz5O2w7A1WOBufVdYEV73pilbWAecxuwBUBV3QUsBi5K8tUkxyZ5/hrU+eBYYG79Arh9LDD3HNui77zL+u5NXgJsleQZA8y9A/CdvmPfHmecq/v6PPY9tJ7OyivBK2kvKb9v7DXRIqvqa8B7gTPaeW6guccZxlnZT/Iy2nuXq+qKcYZ8oK15vLnOqqqRqhqZMXOziZYoSZIkSWttXYfmrKatazOs3uMPj9P22OeoqsNpLsv+Js3GVDckOahtXjFODU8ZZ75H+t7X6uadRJPyPQB3Ar8xgfmOA3breU1YVf0Zzcr+C4DNaS6NB/iv3n5JXg78K3Bce1/zeJ4N3DHI/JIkSZI01dZ1aF4KPEhzie54bbuObSLV2pumxusGmaSqflBVp1TV/sClNPciQxPKZvat2A4UFFdjz94Nu4CXArdV1b3t+4eA1T0zeSnw8r5jLwdu7VuFX53v88QV+nHnr6pftht53dhzCfyEVeO2qnoAeCNwC3DVWHuSfWkC8wlV1bUT97Y090JfNV67JEmSJA3LOg3Nbeg7HViY5PAk2ybZI8mRwDnA/cBn21209wXOBM6daJhLMifJyUn2TrJ1kgOA36YJogCXt3MsTLJdkoNZeTOvtfFc4JNJXpzkEOB9wJ/3tN8M7JNkq97dsvucBuyX5pnK2yc5jGZTslMHrOUiVg7fNwNbJ9k9yeZJNuk6OckLkuxGe59xkt3a16Y9fd7X/q52SvJRmk3Ljup5LvT+NIH5b4BzksxuX7/ZN90+wI+r6kcDfkZJkiRJmlLD2D37QzS7PX+UZgX5i8Dz2kcjHUSzmdcVNJf6LgGOGGDs5cD2NLtP3wB8hiaMnwKP3fN8GPBKmp2nF7R1TJZzaFZyLwf+Fvh7nhiajwOeD9xEx6XIVXUVzY7iBwPX0jz3+GSae4cH8Y/A9kl26jn2RZr7jr/ezv/GVZx/Is1q9aL2/ffb10hPn1cD36LZ/fr3gT+sqi/3tL8FmElz7/PPe17f65vrjTTflyRJkiRNK3nivlVaU+1zmK+tqv5HQw1NkpOB36yqtw67li5pnqH9dWD7qrpndf1nzZ5Tc+edMPWFSVrJlYvmD7sESZKkqdK5/9YwVpq17nwc+HGS1d1HPUzPBeZPJDBLkiRJ0rq2Tp/TrHWr3YDspGHXsSrto6skSZIkaVoyNE+SdqduSZIkSdIGxMuzJUmSJEnqYGiWJEmSJKmDoVmSJEmSpA6GZkmSJEmSOhiaJUmSJEnqkKoadg3ShI2MjNTo6Oiwy5AkSZK0YUlXgyvNkiRJkiR1MDRLkiRJktTB0CxJkiRJUgdDsyRJkiRJHQzNkiRJkiR1MDRLkiRJktTB0CxJkiRJUgef06z1yqzZc2ruvBOGXYa03rhy0fxhlyBJkrQ+8DnNkiRJkiQNytAsSZIkSVIHQ7MkSZIkSR0MzZIkSZIkdTA0S5IkSZLUwdAsSZIkSVIHQ7MkSZIkSR0MzZIkSZIkdTA0S5IkSZLUwdA8BZIsTnLBsOsY1DDrTnJfkrcMY25JkiRJ6jJj2AVsoI4GMuwiJEmSJElrx9A8BarqnmHXIEmSJElae16ePQV6L3NOsm+Sy9rLj+9JcnmSnScwxu1JDu15/50ky5LMaN+/KEkl2ap9/9QkpyS5Ncn9Sb6X5KC+MXdM8tV2nF8m+acks1dRw65Jfp7kpJ5jf5DkyiS/TvJfSU5K8tSe9puTfCTJmUnubet5X9+42yW5tB3jh0les/pvVZIkSZLWPUPzFGoD7nnAt4FdgT2B04FHJ3D6N4AD2nFmAiPAg+2/APsDN1bVz9r3ZwP7AW8CdgE+A3wlya7tGFsC3wSuBfYAXgFsCpyfZKW/gyT7AJcAp1bVh9tjBwHnAGcAOwFHAIcAH+87/d3ANcDuwCnAqUn2asfYCPgSzd/eXu0YxwObTOA7kSRJkqR1ysuzp9YzgGcBX6mqm9pj10/w3EuBY9qfXwb8GLiCJkhfRhOaLwVIsi3wRmCbqvppe84ZSV4BvB14B3Ak8IOq+sDYBEnmA3fRBPEreo6/Bvgc8K6q+mxPTR8GFlXV2e37m5J8APjHJO+rqmqPf62qzmh//lSSo4ADgSU0YX1HYM5YrUmOAb7V9UUkWQAsAHjqZs/p/sYkSZIkaZK50jyFquouYDFwUXtZ9LFJnj/B0y8Ftk/yXJqAfEl7bP+2fb/2PTQrugGWtpeB35fkPuD3gW3bPi8B9u1rv6VtG+sz1u9LwFv7AvNY24f7xvgcMAvovcz76r7zbgO2aH/eAfhZT7gHuBxY0fVFVNVZVTVSVSMzZm7W1U2SJEmSJp0rzVOsqg5P8kngVcBrgZOSvK6qLlrNedcl+QVNSN4f+CTwPZqV2x2BrXg8NG8EFPC7wMN9Qz3Q0+erwHvHme4XPT//F/BL4Igk51fVgz1tGwEnAF8YZ4w7en7ur6F4/D9o3FVckiRJ0nrD0LwOVNUPgB8ApyT5V+DNwCpDc+sbNKvFI8A3quqXSe4E3s8T72f+Pk0YnV1Vl3SMdRXwP4GfVFV/qO11F024/zrwpSSv7wnOVwFzq+rGCdTeZSmwVZLnV9XYSvceeNWDJEmSpGnIoDKFksxJcnKSvZNsneQA4LdpguNEXAocCvyoqn7ZHvsG8Mc8vspMVd1As0HX4iSHJHlhkpEk703yhrbbXwLPBD6fZM+2zyuSnJXkCdc8V9WdNPcgPw84N8nYJl0nAm9KcmKSnZPMbec7dYCv5WKa+7o/m2S3doOwPwceGWAMSZIkSVonDM1TazmwPc3lzDfQ7Gh9Ds2O0hNxCbAxPQG54xjA4TQ7aJ9KE0ovAPYFfgJQVbfRbCi2ArgQ+E+aIP1g+3qCNjj/HvB84ItJNmkvKf99ms3IrmhfHwR+2n9+l6paAbye5m/vcuCzwP8ZrwZJkiRJGrY8vuGxNP3Nmj2n5s47YdhlSOuNKxfNH3YJkiRJ64POvZdcaZYkSZIkqYOheUiS/Gvvo5v6Xn8y7PokSZIkSe6ePUz/N/D0jra71mUhkiRJkqTxGZqHpOdxUZIkSZKkacrLsyVJkiRJ6mBoliRJkiSpg6FZkiRJkqQOhmZJkiRJkjqkqoZdgzRhIyMjNTo6OuwyJEmSJG1Y0tXgSrMkSZIkSR0MzZIkSZIkdTA0S5IkSZLUwdAsSZIkSVIHQ7MkSZIkSR0MzZIkSZIkdTA0S5IkSZLUwec0a70ya/acmjvvhGGXIU25KxfNH3YJkiRJTyY+p1mSJEmSpEEZmiVJkiRJ6mBoliRJkiSpg6FZkiRJkqQOhmZJkiRJkjoYmiVJkiRJ6mBoliRJkiSpg6FZkiRJkqQOhmZJkiRJkjoYmtcDSfZPUkk2n4Kx90tyQ5KNJ3vsCc7/miT/kcS/RUmSJEnTjkFlmklyc5L39h3+LrAl8KspmHIRcFJVPdrOf3ySaydyYpLtk3w5yZ1JliW5LMmr+vr8bpKLk9yd5L+TfD3JHmPtVXUB8Chw2CR+JkmSJEmaFIbm9UBVPVRVt1dVTea4SfYG5gJfWMMhLgCeBhwI/A7wbeC8JNu2428KXAjcBuwN7AX8HLgoyWY945wNHLWGNUiSJEnSlJmWoTmN9yT5UZIHk9yaZGHbtku7cvlAkruSLE7yzJ5zFye5IMnRSX7WrnCenWRmT59921XR+5Lck+TyJDu3bW9Jcl9fPU+4PHqsT5JXJ7k+yfIk5yd5ZpJD2rrvSfIPSZ7eM86lSf4myeltXXcnWTR2aXKSS4GtgUXtfDXe/O2xNyS5pv1+bkny4STpab85yUeSnJnk3vY7fF/fV/0m4OKqWj72uYCPATuNzd8eG+93tDnwIuCUqvpBVd0IfBCYQROgoQnkzwY+VlXXVdV1wEeBZwEv7hnufGAkyXbjzSVJkiRJwzItQzPwcZpwtRDYCfgj4JY2+F4I3AfsAbyeZgXz033n7wPsDLwCOLTtdzRAkhnAeTSrorsCewKn01wiPIhNgPfQXFZ8IDAC/DPwZuBg4HXAa4B39J13GM33vhfwdmABcEzb9gbgVuBEmsuxtxxv4iQvoVkdPhfYhSasfgh4V1/XdwPXALsDpwCnJtmrp30fYLTn/eeB04Af9sz/+Y7P/yvgOmBekk3be6IXAMuA77R9fgjcAbw1ySZJNgHeBvwU+M+xgarqp8AvgP065pIkSZKkoZgx7AL6tZf0vhs4pqrGwvCNwJIkbwM2BeZV1bK2/wLgkiTbtaudAPcCR1bVI8B1Sb5AE2wXAs+gWen8SlXd1Pa/fg1KnQG8s6p+2Nbxubbu36qqO9tj5wEH0ATRMT8Hjmovtb4+yfbAscCfVdVdSR4FllXV7auY+1jgG1X1sfb9DUleBHwA+FRPv69V1Rntz59KchTN97CkPbZ1Ww8AVfVAu8r+yGrmp6oqySuBL9F83yuAu4BXV9XP2z7LkuxP858UH2pPvRl4ZVU90DfkbcA2483V/o4XADx1s+esqixJkiRJmlTTcaV5R5pV3K+P07YDcPVYYG59lyaw7dhzbGkbmMfcBmwBUFV3AYtp7qv9apJjkzx/Dep8cCwwt34B3D4WmHuObdF33mV99yYvAbZK8owB5t6Bx1dzx3x7nHGu7uvz2PfQejrw69VN1l5Sft/Yqz0W4K9oVpz3oVn5/2fgi0m2avs8neYqgCXAS4GXAd+nue95Vt80D7T1rKSqzqqqkaoamTFzs/G6SJIkSdKUmI6hOatp69oMq/f4w+O0PfZZq+pwmsuyvwm8lmal9qC2ecU4NTxlnPke6Xtfq5t3Ek3K9wDcCfzGBOY7Dtit5wXwe8AfAG+squ9U1VVV9Q7gfuDwts+bgG2Bw6vqe1V1WXvsBTSXzPd6Ns2l3JIkSZI0bUzH0LwUeJDmMuLx2nbt23l5b5rPcd0gk7SbV51SVfsDl9LciwxNcJvZt2K7G5Nnz94Nu2hWYG+rqnvb9w8Bq3tm8lLg5X3HXg7c2rcKvzrf54kr9OPOX1W/rKobx17t4bGN1Vb0nb+Cx/+uZtIE9RV97U8I70meRhOurxqgdkmSJEmactMuNLeh73RgYZLDk2ybZI8kRwLn0KxkfrbdRXtf4Ezg3J4wt0pJ5iQ5OcneSbZOcgDw2zRBFODydo6FSbZLcjArb+a1Np4LfDLJi5McArwP+POe9puBfZJs1btbdp/TgP3SPFN5+ySH0WxKduqAtVzEyuH7ZmDrJLsn2bzdvGs8S2juYT47ya5tHYuAF9I8igrg32juIf+rJDsk2Ynm8VKPAv/eM9ZLaf6jpP+Sc0mSJEkaqmkXmlsfotnt+aM0K8hfBJ7XPhrpIJogdgXNBlNLgCMGGHs5sD3N7tM3AJ+hCeOnwGP3PB8GvJJm5+kFbR2T5RyaldzLgb8F/p4nhubjgOcDN9FxuXJVXUWzo/jBwLXAye3rjPH6r8I/Atu3YXbMF4F/obmn/A7gjR013Am8imZjtn+n2YV7X+B1bX1U1fU0l3DvQvN7+jbwPJrNwm7tGe6NwDn/f3v3HmxnVd9h/PmaUKCAIpcYK2pgsKCi9ZKORC5mGK1YWtuiU6hWTYuXDt7rKEVRY0eBqgUpKKPYGpUq03qhguViBgJV0BqQkWjBC/HG1YASkEsgrv6x1tHNy9nn7CRn731OeD4za/Z+11rv+673d9acc9Z7We/Eq68kSZIkabbIA+ek0jC19zCvKaV0Xw01NklOBHYvpRw1pv3vTp29fHEpZe109XdYuGfZ92XvGX7DpDG74gMvH3cTJEmSHkr6zq01W680a3SOB65r71kehz2BowcZMEuSJEnSqM269zRrtNoEZO8b4/7/l3qrvSRJkiTNOg6aR6jN1C1JkiRJmiO8PVuSJEmSpD4cNEuSJEmS1IeDZkmSJEmS+nDQLEmSJElSHw6aJUmSJEnqI6WUcbdBGtjixYvL6tWrx90MSZIkSVuX9CvwSrMkSZIkSX04aJYkSZIkqQ8HzZIkSZIk9eEzzZpTktwBXDvudjwE7AasG3cjHgKM8/AZ49EwzqNhnEfDOI+GcR4N4zy4daWUQycrmD/qlkhb6NpSyuJxN2Jrl2S1cR4+4zx8xng0jPNoGOfRMM6jYZxHwzjPDG/PliRJkiSpDwfNkiRJkiT14aBZc83Hxt2AhwjjPBrGefiM8WgY59EwzqNhnEfDOI+GcZ4BTgQmSZIkSVIfXmmWJEmSJKkPB82SJEmSJPXhoFmSJEmSpD4cNGtOSHJ0krVJ7klyRZKDxt2muSzJ8iSlk27qKU+rc0OSu5OsSvLkcbZ5LkhycJIvJbm+xXRZp3zauCZ5ZJJPJ7m9pU8n2XmkBzLLDRDnFZP076936myb5NQk65L8qm1vj5EeyCyW5Ngk30yyPsnPk5yTZL9OHfvzFhowzvbnLZTktUm+3eK8PsnlSQ7rKbcvz4AB4mxfnmFJ3t7ieFpPnv15CBw0a9ZLcgRwCnA88HTgMuC8JI8ba8PmvmuBR/ekp/SUvQ14C/B64A+BW4CvJNlp1I2cY3YE1gBvBO6epHyQuH4GeAbwAuDQ9v3TQ2zzXDRdnAFW8sD+/ced8g8BLwL+CjgIeDhwbpJ5w2jwHLQU+AjwbOAQ4H5gZZJdeurYn7fcUqaPM9ift9TPgGOo/W8xcBFwdpKntnL78syYLs5gX54xSfYHXgV8u1Nkfx6GUorJNKsT8A3gjE7e94ETxt22uZqA5cCaPmUBbgTe0ZO3PXAH8Jpxt32uJOBOYNmmxBV4IlCAA3rqHNjy9hn3Mc3G1I1zy1sBnDvFOo8ANgAv7cl7LPBr4PnjPqbZmKgnKjYCf9qW7c8jiHPLsz8PJ9a3Aa+xL48mzu27fXnm4voI4IfUk22rgNNavv15SMkrzZrVkvwO8Ezgwk7RhdQz89p8e7XbW9cmOSvJXi1/T2AhPTEvpdwNXIox3xKDxHUJdRB4Wc96XwN+hbHfVAcmuSXJ95KckWRBT9kzgW144M/ip8D/YZz72Yl6d9ov2rL9eTi6cZ5gf54hSeYlOZJ6guIy7MtDMUmcJ9iXZ8bHgM+VUi7q5Nufh2T+uBsgTWM3YB5wcyf/ZuC5o2/OVuMbwDLgGmABcBxwWXvmZWGrM1nMHzOqBm6FBonrQuDnpZ32BSillCS39Kyv6Z0PfAFYCywC3gtclOSZpZR7qbHcCKzrrHczxrmfU4CrgMvbsv15OLpxBvvzjEjyFGpct6MOGP6ilHJ1kolBgn15BvSLcyu2L8+AJK8C9gZeNkmxv5uHxEGz5orSWc4keRpQKeW83uU2Ecd1wCuAiUk5jPlwTBfXyWJs7DdBKeWsnsWrk1wB/Bg4jPoPWz/GeRJJTqLeundgKWVjp9j+PEP6xdn+PGOuBZ4G7Ex9ZvaTSZb2lNuXZ8akcS6lrLEvb7kk+1Dn+DmolLJhiqr25xnm7dma7dZRzzp2z3wt4MFn0bSZSil3At8BngBMzKJtzGfWIHG9CViQJBOF7fvuGPvNVkq5gTpBzRNa1k3UO1h261S1j3ckOZk6Ic8hpZTreorszzNoijg/iP1585RSNpRSflBKWV1KOZZ6Rf/N2Jdn1BRxnqyufXnTLaHGZ02S+5PcDzwHOLp9v7XVsz/PMAfNmtXaWbQrgOd1ip7HA5/F0BZIsh2wL3XyiLXUX6jP65QfhDHfEoPE9XLq819LetZbAuyAsd9sSXaj3pZ2Y8u6AriPB/4s9qBOjmKcmySnAC+hDuSu6RTbn2fINHGerL79eWY8DNgW+/KwTcT5QezLm+Vs6ttOntaTVgNnte/fw/48HOOeicxkmi4BR1BnU3wl9RfnKdTnZB4/7rbN1QR8kHpmck/gWcC5wPqJmFJfGbEeOBzYj/rL+AZgp3G3fTYn6h+hiT9idwHvat8fN2hcgfOAq4H9qX/ErgbOGfexzaY0VZxb2Qdb7BZRX+lzOfVqRm+cTweup86N8HTgYuoVkXnjPr7ZkIAPt756CPWKxUTasaeO/XnIcbY/z1icT6QOGhZRBxwnUGdkfkErty8POc725aHGfRVt9uy2bH8eRpzH3QCTaZAEHA38CLiXeiby4HG3aS6nnl+gG9ofp88DT+opD/W1VDcC9wCXAPuNu92zPbV/AsokacWgcQV2Ac5sf/DWt+87j/vYZlOaKs7UV2tcQH0v5Qbq83IrgMd2trEdcCr1Vra7gHO6dR7KqU98C7C8p479echxtj/PWJxXtNjd22K5kp5XGNmXhx9n+/JQ476KBw6a7c9DSGmBkyRJkiRJHT7TLEmSJElSHw6aJUmSJEnqw0GzJEmSJEl9OGiWJEmSJKkPB82SJEmSJPXhoFmSJEmSpD4cNEuStBVIsixJSbL3uNuyqZIsTbI8if+XSJJmHf84SZKkcVsKvBv/L5EkzUL+cZIkSWORZJskGXc7thZJth13GyRpa+SgWZKkrVSSVUm+muTQJFcluTvJt5I8K8n8JMcnuTHJbUlWJNmhZ91F7Xbvo5OclOSWJHclOTfJos5+tkny3iQ/SrKhfb43yTZ9tvf+JDcA9wIfol5lBriv1Sk9670nyZVJbk+yLslFSfbv7H9pW++FSU5r9X6e5MwkO3fqzk9yTJLvJrmn1Ts/yb49dXZLcnqS65Pcm+SaJK8eIN47Jjk1yU/aejcnWdnZ9iD73yfJF5P8sv3Mvp7k0M6+lrdj3i/JBUnuBP6jp/zwtt5dbTv/meRx0x2DJOnB5o+7AZIkaaj2Bj4AvA+4E3g/8KWW5gPLgCe2OrcAb+usfyxwFfA3wALgeODCJE8updzX6nwS+MtW9lVgCXAcsBfwks723gF8E3g1MA+4EtgBOAo4ENjYqf8Y4GTgZ63eXwOXJllcSvl2p+4pwLltn/u0Y90IvKKnzlnAn1MH6yuB7YCDgUcD1yR5OPA1YHtgObAWeD5wepJtSymn0t/JwAuBtwPfB3YFDgB6B+7T7f/3qDG8A3gdcDvwWuDLSf6klHJeZ5//Bfwr8E/ArwGS/B1wOvAJ4B+BndqxXJLkqaWUO6Y4BklSVynFZDKZTCbTHE/UwW8B9u7JWwXcB+zVk/fCVm9lZ/0vAGt7lhe1et8FHtaTf0DLP6ot79eWl3e2d1zLf2pne1cC6dRd3srmT3OM86gD/WuBU3ryl7b1P9mpfxpwz8T+gENavTdMsY93tnWe0Mk/A1g3VRuBNcBJU5QPsv8PAvd3fo7z2jFfOUnM3thZf0fqQPvfOvmLgA3Am8bdV00mk2muJW/PliRp6/a9Usp1PcvXtM8LOvWuAfaY5Bnjz5VSfj2xUEr5GvWq75KWdXD7PLOz3sTyczr5Z5dSCgNK8twkFye5lTqYvA/4feqV5K4vd5avBrYFHtWW/4g60Dxjil0eCnwDWNtupZ6fZD41XrsCT5pi3W8Cy5K8PcniJPM65YPs/2Dg66WUH0xklFI2Ap8FntauhPf6Ymd5CfBw4N877f8Z9Wd8MJKkTeLt2ZIkbd1+0VneMEX+fOpVzft78m+eZJs3U2+bBtilfd7YqXNTp5w+9fpK8gzgv6kD1qPauhuBj1Nva+66rbN8b/ucqLsrcFsp5e4pdruAekv7fX3Kd51i3ddTj/tvqbfD35bkU8A7Sil3Dbj/XYBvTZJ/ExDgkcD6nvxuPBe0z5V9tt/9uUuSpuGgWZIkTeVRffKuat8nBqoLgR/21FnYPm/trDvwVWbgRdQB/OHlt89Pk+SRwC83YTsT1gG7JNl+ioHrrdRnu9/Yp/zafhsvpdxJfQb82CSPB14MnEg9IXHMgPu/jd/GrtdCauy6Jwa68ZyI9zLgO5Nsx+eZJWkTeXu2JEmayouT/Ob/hSQHAHsAl7esS9rnkZ31Xto+Lx1gHxNXhLfv5P8u9cpy72zahwCbOwv0hdSrta+cos75wL7AT0opqydJAw06Syk/LqX8M/UW8f02Yf+XAPv3zlDebvM+AvjWAPu/jDow3rtP+/sO+iVJk/NKsyRJmspOwNlJPgrsDpxAnRn6UwCllO8k+SywvD07exn1udp3Ap8tD57hejLfbZ9vSXIesLGUspo6gH0TsCLJJ6jPMr8TuH5zDqSUcnGSzwMnJXkscBGwDfU53y+XUlZRZ8A+AvifJCdTryzvQB1IH1RK+bN+209yOXVW8qupM5U/B/gD6uzim7L/ZcBXkrybeiv20e3YDxvgGNcneSvw4SS7A+dRJwZ7TGvPqlLKZwYKmCQJcNAsSZKmdgL1Gd8V1MHjxcDrem+Xpr7S6Trqs7zHATdQX4H0ngH3cS7wEerg8F3Uq7EppVyQ5A3A31Nv1V4DvLztY3MdSb1V+hXUAfnt1Am8Pg5QSrk9ybNbO46hDjZ/SR08f36abV9KffXWP1D/x7oOeHMp5V82Yf83JDmQGr/TqROZXQUcVko5f5ADLKV8NMlPgbdSX7+1DfVEw6X89rZ6SdKAJl7BIEmS9Bvt9uC1wKtKKR8fb2skSRofn2mWJEmSjrzrNwAAAEZJREFUJKkPB82SJEmSJPXh7dmSJEmSJPXhlWZJkiRJkvpw0CxJkiRJUh8OmiVJkiRJ6sNBsyRJkiRJfTholiRJkiSpj/8HAS+rGP5fWKEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "feature_imp = pd.DataFrame(sorted(zip(models[8].feature_importances_,reframed_test.columns[:-n_out]))[-10:], columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False), color = 'tab:blue')\n",
    "plt.tight_layout()\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.xlabel(\"Importance score\", size=16)\n",
    "plt.ylabel(\"Feature\", size=16)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cttbk4v3vZTy"
   },
   "source": [
    "#### Save prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "8mkQx2BA2QBu"
   },
   "outputs": [],
   "source": [
    "# Accuracies\n",
    "df_losses_per_quantile.to_csv(\"{}/LGBM_losses.csv\".format(data_source_folder))\n",
    "\n",
    "# Predictions\n",
    "with open(\"{}/lightgbm_preds_final.txt\".format(data_source_folder), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(df_preds, fp)\n",
    "    \n",
    "with open(\"{}/lightgbm_actuals_final.txt\".format(data_source_folder), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(df_acts, fp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "x5KGEvaOvB42",
    "mzk6FyB_qnq_"
   ],
   "name": "LightGBM_new-dataipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
