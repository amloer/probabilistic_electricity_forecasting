{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3f3a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "\n",
    "# multivariate multi-step encoder-decoder lstm\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array , hstack\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "import time\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05ff900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date information\n",
    "def compute_weekend(row):\n",
    "    if row['weekday'] > 4:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def prepare_df (data):\n",
    "\n",
    "    #datetime(year, month, day, hour)\n",
    "    a = datetime.datetime(2008, 12, 31, 1)\n",
    "\n",
    "    numsteps = len(df)\n",
    "    dateList = []\n",
    "    for x in range (0, numsteps):\n",
    "        dateList.append(a + datetime.timedelta(hours = x))\n",
    "    start_date = \"10/10/2009\" # change when switching dataset\n",
    "\n",
    "    data['date'] = dateList\n",
    "    data.columns = ['slot', 'dow','hour', 'production', 'consumption', 'temp', 'wind', 'cloud', 'date']\n",
    "    data.drop('slot', axis = 1, inplace = True)\n",
    "    #data.drop('dow', axis = 1, inplace = True)\n",
    "    #data.drop('hour', axis = 1, inplace = True)\n",
    "    #data = data[['consumption', 'temp', 'wind', 'cloud', 'date']]\n",
    "    data['consumption'] = data['consumption']*(-1)\n",
    "\n",
    "    #df_indexed = df\n",
    "    data.set_index('date', inplace=True)\n",
    "    #df_indexed.tail()\n",
    "    data['weekday']= data.index.weekday\n",
    "    #data.set_index('date', inplace=True)\n",
    "    data['is_weekend'] = data.apply (lambda row:compute_weekend(row), axis = 1)\n",
    "    data = data[['consumption', 'temp', 'wind', 'cloud', 'hour', 'is_weekend']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d774fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/alexl/Desktop/MT/03_Data/Simulation_Alex/ProductionConsumption_first_11.csv\", sep=\",\")\n",
    "dataset = prepare_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d51e1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dataset.iloc[:-3112], dataset.iloc[-3112:-12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "689bc7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumption</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind</th>\n",
       "      <th>cloud</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-03-31 21:00:00</th>\n",
       "      <td>58.2904</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-31 22:00:00</th>\n",
       "      <td>50.9333</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-31 23:00:00</th>\n",
       "      <td>48.1341</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-01 00:00:00</th>\n",
       "      <td>40.4206</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-01 01:00:00</th>\n",
       "      <td>35.2484</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-11 05:00:00</th>\n",
       "      <td>31.5381</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.625</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-11 06:00:00</th>\n",
       "      <td>38.6563</td>\n",
       "      <td>20.4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-11 07:00:00</th>\n",
       "      <td>34.3562</td>\n",
       "      <td>19.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.875</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-11 08:00:00</th>\n",
       "      <td>50.2426</td>\n",
       "      <td>22.4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-11 09:00:00</th>\n",
       "      <td>46.9090</td>\n",
       "      <td>25.2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.125</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5389 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     consumption  temp  wind  cloud  hour  is_weekend\n",
       "date                                                                 \n",
       "2010-03-31 21:00:00      58.2904   3.9     3  0.625    21       False\n",
       "2010-03-31 22:00:00      50.9333   3.9     3  1.000    22       False\n",
       "2010-03-31 23:00:00      48.1341   4.2     4  1.000    23       False\n",
       "2010-04-01 00:00:00      40.4206   4.0     3  0.750     0       False\n",
       "2010-04-01 01:00:00      35.2484   4.1     5  0.750     1       False\n",
       "...                          ...   ...   ...    ...   ...         ...\n",
       "2010-11-11 05:00:00      31.5381  19.0     4  0.625     5       False\n",
       "2010-11-11 06:00:00      38.6563  20.4     3  1.000     6       False\n",
       "2010-11-11 07:00:00      34.3562  19.4     5  0.875     7       False\n",
       "2010-11-11 08:00:00      50.2426  22.4     4  0.750     8       False\n",
       "2010-11-11 09:00:00      46.9090  25.2     7  0.125     9       False\n",
       "\n",
       "[5389 rows x 6 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[-5389:,: ]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a29648e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a92dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into standard weeks\n",
    "    train, test = data[:-3184], data[-3184:-12]\n",
    "    # restructure into windows of daily data\n",
    "    #train = array(split(train, len(train)/24))\n",
    "    #test = array(split(test, len(test)/24))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5a97619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_cols_to_drop(n_input, n_out, n_vars):\n",
    "    drops = list()\n",
    "    for i in range(n_out):\n",
    "        lis = list()\n",
    "        for step in range(n_vars-1):\n",
    "            lis += [n_vars*n_input+step+1+n_vars*i]\n",
    "        drops = drops + lis\n",
    "    return drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "82bc8690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[433, 434, 435, 436, 437, 439, 440, 441, 442, 443, 445, 446, 447, 448, 449, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 463, 464, 465, 466, 467, 469, 470, 471, 472, 473, 475, 476, 477, 478, 479, 481, 482, 483, 484, 485, 487, 488, 489, 490, 491, 493, 494, 495, 496, 497, 499, 500, 501, 502, 503, 505, 506, 507, 508, 509, 511, 512, 513, 514, 515, 517, 518, 519, 520, 521, 523, 524, 525, 526, 527, 529, 530, 531, 532, 533, 535, 536, 537, 538, 539, 541, 542, 543, 544, 545, 547, 548, 549, 550, 551, 553, 554, 555, 556, 557, 559, 560, 561, 562, 563, 565, 566, 567, 568, 569, 571, 572, 573, 574, 575]\n",
      "    var1(t-72)  var2(t-72)  var3(t-72)  var4(t-72)  var5(t-72)  var6(t-72)  \\\n",
      "72    0.506726    0.506726       0.125         1.0    0.043478         0.0   \n",
      "\n",
      "    var1(t-71)  var2(t-71)  var3(t-71)  var4(t-71)  ...  var1(t+14)  \\\n",
      "72    0.506726    0.506726      0.0625         1.0  ...    0.504484   \n",
      "\n",
      "    var1(t+15)  var1(t+16)  var1(t+17)  var1(t+18)  var1(t+19)  var1(t+20)  \\\n",
      "72    0.457399    0.426009    0.426009    0.376682    0.381166    0.358744   \n",
      "\n",
      "    var1(t+21)  var1(t+22)  var1(t+23)  \n",
      "72    0.374439    0.363229    0.331839  \n",
      "\n",
      "[1 rows x 456 columns]\n"
     ]
    }
   ],
   "source": [
    "# prepare data for lstm\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=72, n_out=48, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# Define forecasting approach\n",
    "n_in = 72\n",
    "n_out = 24\n",
    "n_vars = 6\n",
    "\n",
    "# load dataset\n",
    "values = train.values\n",
    "\n",
    "encoder1 = LabelEncoder()\n",
    "values[:,4] = encoder1.fit_transform(values[:,4])\n",
    "encoder2 = LabelEncoder()\n",
    "values[:,5] = encoder2.fit_transform(values[:,5])\n",
    "\n",
    "# integer encode direction\n",
    "values = values.astype('float32')\n",
    "# convert col to [rows, columns]\n",
    "cons = values[:, 1].reshape((len(values[:, 1]), 1))\n",
    "weather = values[:,1:6]\n",
    "# normalize features\n",
    "scaler1 = MinMaxScaler()\n",
    "scaled_consumption = scaler1.fit_transform(cons)\n",
    "scaler2 = MinMaxScaler()\n",
    "scaled_weather = scaler2.fit_transform(weather)\n",
    "\n",
    "scaled = hstack((scaled_consumption, scaled_weather))\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_in, n_out)\n",
    "# drop columns we don't want to predict\n",
    "\n",
    "l_drop = select_cols_to_drop(n_in, n_out, n_vars)\n",
    "print(l_drop)\n",
    "reframed.drop(reframed.columns[l_drop], axis=1, inplace=True)\n",
    "print(reframed.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd9c52c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10845, 72, 6) (10845, 24) (5389, 72, 6) (5389, 24)\n"
     ]
    }
   ],
   "source": [
    "#Input numbers\n",
    "n_out = 24\n",
    "n_in = 72\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "train, val = values[:-5389, :], values[-5389:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-n_out], train[:, -n_out:]\n",
    "val_X, val_y = val[:, :-n_out], val[:, -n_out:]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_in, int(train_X.shape[1]/n_in)))\n",
    "val_X = val_X.reshape((val_X.shape[0], n_in, int(val_X.shape[1]/n_in)))\n",
    "print(train_X.shape, train_y.shape, val_X.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1281e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    var1(t-72)  var2(t-72)  var3(t-72)  var4(t-72)  var5(t-72)  var6(t-72)  \\\n",
      "72    0.825112    0.825112      0.4375        0.75    0.434783         0.0   \n",
      "\n",
      "    var1(t-71)  var2(t-71)  var3(t-71)  var4(t-71)  ...  var1(t+14)  \\\n",
      "72    0.804933    0.804933       0.375         1.0  ...    0.515695   \n",
      "\n",
      "    var1(t+15)  var1(t+16)  var1(t+17)  var1(t+18)  var1(t+19)  var1(t+20)  \\\n",
      "72         0.5    0.506726    0.513453    0.502242    0.558296    0.598655   \n",
      "\n",
      "    var1(t+21)  var1(t+22)  var1(t+23)  \n",
      "72    0.656951    0.690583    0.699552  \n",
      "\n",
      "[1 rows x 456 columns]\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "\n",
    "# Define forecasting approach\n",
    "n_in = 72\n",
    "n_out = 24\n",
    "n_vars = 6\n",
    "\n",
    "# load dataset\n",
    "values = test.values\n",
    "\n",
    "\n",
    "values[:,4] = encoder1.transform(values[:,4])\n",
    "\n",
    "values[:,5] = encoder2.transform(values[:,5])\n",
    "\n",
    "# integer encode direction\n",
    "values = values.astype('float32')\n",
    "# convert col to [rows, columns]\n",
    "cons = values[:, 1].reshape((len(values[:, 1]), 1))\n",
    "weather = values[:,1:6]\n",
    "# normalize features\n",
    "\n",
    "scaled_consumption = scaler1.transform(cons)\n",
    "\n",
    "scaled_weather = scaler2.transform(weather)\n",
    "\n",
    "scaled = hstack((scaled_consumption, scaled_weather))\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_in, n_out)\n",
    "# drop columns we don't want to predict\n",
    "\n",
    "l_drop = select_cols_to_drop(n_in, n_out, n_vars)\n",
    "reframed.drop(reframed.columns[l_drop], axis=1, inplace=True)\n",
    "print(reframed.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "efc15c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3005, 72, 6) (3005, 24)\n"
     ]
    }
   ],
   "source": [
    "#Input numbers\n",
    "n_out = 24\n",
    "n_in = 72\n",
    "\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "\n",
    "# split into input and outputs\n",
    "test_X, test_y = values[:, :-n_out], values[:, -n_out:]\n",
    "\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "test_X = test_X.reshape((test_X.shape[0], n_in, int(test_X.shape[1]/n_in)))\n",
    "print(test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e883056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tilted_loss(q,y,f):\n",
    "    e = (y-f)\n",
    "    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)\n",
    "\n",
    "def quantile_loss(y_true, y_pred, q):\n",
    "    return np.max([q*(y_true - y_pred), (1-q)*(y_pred-y_true)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "563338de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "def build_model(neurons = 200):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(RepeatVector(48))\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model_one_layer(neurons = 200):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.3, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(LSTM(72))\n",
    "    #model.add(RepeatVector(24))\n",
    "    #model.add(LSTM(neurons, activation='relu', return_sequences=True))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(24))\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_model(quantile, trainx, trainy, valx, valy, epoch = 200, batch_size = 700, verbose = 1):\n",
    "    model.compile(loss=lambda y,f: tilted_loss(quantile,y,f), optimizer='adam')\n",
    "    # fit network\n",
    "    history = model.fit(trainx, trainy, epochs=epoch, batch_size = batch_size, validation_data=(valx, valy), verbose=verbose, shuffle=False)\n",
    "    # make a prediction\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def forecast(model, testx):\n",
    "    yhat = model.predict(testx)\n",
    "    return yhat\n",
    "    \n",
    "def extract_predictions(yhat, quantile):\n",
    "    # design network\n",
    "    test_X_reshaped = test_X.reshape((test_X.shape[0], test_X.shape[1]*test_X.shape[2]))\n",
    "\n",
    "    # invert scaling for forecast\n",
    "    df_predictions = DataFrame()\n",
    "    for i in range(yhat.shape[1]):\n",
    "\n",
    "        inv_yhat = concatenate((yhat[:,i].reshape(yhat[:,i].shape[0], 1), test_X_reshaped[:, -3:]), axis=1)\n",
    "        inv_yhat = scaler1.inverse_transform(inv_yhat)\n",
    "        inv_yhat = inv_yhat[:,0]\n",
    "        df_predictions['pred_hour_{}_quant_{}'.format(i, str(quantile)[-1])] = inv_yhat\n",
    "\n",
    "  # invert scaling for actual\n",
    "    df_actuals = DataFrame()\n",
    "\n",
    "    for i in range(test_y.shape[1]):\n",
    "        inv_y = concatenate((test_y[:,i].reshape(test_y[:,i].shape[0], 1), test_X_reshaped[:, -3:]), axis=1)\n",
    "        inv_y = scaler1.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        df_actuals['actual_hour_{}_quant_{}'.format(i, str(quantile)[-1])] = inv_y\n",
    "\n",
    "    acts.append(df_actuals)\n",
    "    preds.append(df_predictions)\n",
    "\n",
    "    return df_actuals, df_predictions\n",
    "\n",
    "def get_accuracies(actual, predicted, quantile):\n",
    "    accuracies = list()\n",
    "    for i in range(actual.shape[1]):\n",
    "\n",
    "        preds_hourly = np.asarray(predicted.iloc[:, i])\n",
    "        acts_hourly = np.asarray(actual.iloc[:, i])\n",
    "        accuracies.append(quantile_loss(acts_hourly, preds_hourly, 0.1).mean())\n",
    "\n",
    "    df_accuracies['quantile_{}'.format(str(quantile)[-1])] = pd.Series(accuracies)\n",
    "\n",
    "    return df_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c0a5c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 [==============================] - 12s 773ms/step - loss: 0.0569 - val_loss: 0.0490\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 14s 845ms/step - loss: 0.0547 - val_loss: 0.0593\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 14s 857ms/step - loss: 0.0472 - val_loss: 0.0325\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 12s 772ms/step - loss: 0.0411 - val_loss: 0.0408\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 13s 796ms/step - loss: 0.0334 - val_loss: 0.0253\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 14s 881ms/step - loss: 0.0330 - val_loss: 0.0255\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 14s 856ms/step - loss: 0.0267 - val_loss: 0.0249\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0250 - val_loss: 0.0243\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0237 - val_loss: 0.0231\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0234 - val_loss: 0.0209\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0230 - val_loss: 0.0209\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0232 - val_loss: 0.0222\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0247 - val_loss: 0.0212\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0244 - val_loss: 0.0216\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0233 - val_loss: 0.0243\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0233 - val_loss: 0.0240\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0229 - val_loss: 0.0232\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0227 - val_loss: 0.0225\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0225 - val_loss: 0.0209\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0220 - val_loss: 0.0201\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0219 - val_loss: 0.0198\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0218 - val_loss: 0.0198\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0220 - val_loss: 0.0202\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0225 - val_loss: 0.0207\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0234 - val_loss: 0.0199\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0226 - val_loss: 0.0204\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0220 - val_loss: 0.0212\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0220 - val_loss: 0.0221\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0218 - val_loss: 0.0210\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0214 - val_loss: 0.0207\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0213 - val_loss: 0.0197\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0212 - val_loss: 0.0191\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0209 - val_loss: 0.0185\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0209 - val_loss: 0.0180\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0209 - val_loss: 0.0187\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0218 - val_loss: 0.0187\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0222 - val_loss: 0.0180\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0215 - val_loss: 0.0190\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0210 - val_loss: 0.0182\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0207 - val_loss: 0.0185\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0208 - val_loss: 0.0175\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0202 - val_loss: 0.0169\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0201 - val_loss: 0.0167\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0201 - val_loss: 0.0169\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0206 - val_loss: 0.0167\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0205 - val_loss: 0.0168\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0204 - val_loss: 0.0167\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0200 - val_loss: 0.0168\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0196 - val_loss: 0.0167\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0196 - val_loss: 0.0164\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0197 - val_loss: 0.0164\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0199 - val_loss: 0.0164\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0198 - val_loss: 0.0164\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0201 - val_loss: 0.0165\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0198 - val_loss: 0.0164\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0197 - val_loss: 0.0163\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0197 - val_loss: 0.0163\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0194 - val_loss: 0.0162\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0194 - val_loss: 0.0166\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0195 - val_loss: 0.0163\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0191 - val_loss: 0.0162\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0193 - val_loss: 0.0161\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0196 - val_loss: 0.0163\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0197 - val_loss: 0.0163\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0200 - val_loss: 0.0170\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0196 - val_loss: 0.0169\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0196 - val_loss: 0.0176\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0190 - val_loss: 0.0168\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0190 - val_loss: 0.0170\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0189 - val_loss: 0.0165\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0187 - val_loss: 0.0160\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0187 - val_loss: 0.0161\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0192 - val_loss: 0.0161\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0192 - val_loss: 0.0177\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0204 - val_loss: 0.0168\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0201 - val_loss: 0.0166\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0190 - val_loss: 0.0169\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0193 - val_loss: 0.0174\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0190 - val_loss: 0.0166\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0188 - val_loss: 0.0170\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0189 - val_loss: 0.0167\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0185 - val_loss: 0.0161\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0183 - val_loss: 0.0162\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0188 - val_loss: 0.0162\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0185 - val_loss: 0.0159\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0188 - val_loss: 0.0165\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0190 - val_loss: 0.0165\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0186 - val_loss: 0.0163\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0188 - val_loss: 0.0164 0s - loss: 0\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0191 - val_loss: 0.0163\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0187 - val_loss: 0.0162\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0189 - val_loss: 0.0164\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0189 - val_loss: 0.0164\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0184 - val_loss: 0.0162\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0187 - val_loss: 0.0167\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0185 - val_loss: 0.0164\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0182 - val_loss: 0.0161\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0185 - val_loss: 0.0172\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0184 - val_loss: 0.0169\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0182 - val_loss: 0.0171\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0187 - val_loss: 0.0166\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0182 - val_loss: 0.0161\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0184 - val_loss: 0.0168\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0187 - val_loss: 0.0168\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0181 - val_loss: 0.0163\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0182 - val_loss: 0.0173\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0182 - val_loss: 0.0170\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0179 - val_loss: 0.0163\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0183 - val_loss: 0.0170\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0181 - val_loss: 0.0166\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0177 - val_loss: 0.0164\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0181 - val_loss: 0.0167\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0180 - val_loss: 0.0162A: 1s - loss: 0.01 - ETA: 0s - loss: 0\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0187 - val_loss: 0.0176\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0183 - val_loss: 0.0165\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0180 - val_loss: 0.0179- loss: 0.\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0183 - val_loss: 0.0176\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0180 - val_loss: 0.0175\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0179 - val_loss: 0.0187\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0180 - val_loss: 0.0183\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0176 - val_loss: 0.0176\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0176 - val_loss: 0.0172\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0178 - val_loss: 0.0171\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0177 - val_loss: 0.0169s - loss: 0.0\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0176 - val_loss: 0.0167\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0178 - val_loss: 0.0169\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0180 - val_loss: 0.0165\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0180 - val_loss: 0.0168\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0181 - val_loss: 0.0167 ETA: 1s - loss\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0180 - val_loss: 0.0164\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0177 - val_loss: 0.0162\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0180 - val_loss: 0.0188\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0183 - val_loss: 0.0167\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0179 - val_loss: 0.0175\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0180 - val_loss: 0.0179A: 1s - loss: 0.01 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: \n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0174 - val_loss: 0.0170 ETA: 0s - loss: \n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0175 - val_loss: 0.0198\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0176 - val_loss: 0.0177\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0175 - val_loss: 0.0173A: 1s - loss: 0.017 - ETA: 0s - loss: \n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0178 - val_loss: 0.0170\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0172 - val_loss: 0.0165\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0172 - val_loss: 0.0176\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0172 - val_loss: 0.0169- loss: 0.\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0171 - val_loss: 0.0169\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0173 - val_loss: 0.0173\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0172 - val_loss: 0.0168\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0170 - val_loss: 0.0170\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0171 - val_loss: 0.0166TA: 0s - loss: 0\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0174 - val_loss: 0.0170\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0173 - val_loss: 0.01630s - loss: 0.01\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0175 - val_loss: 0.0172\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0175 - val_loss: 0.0174\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0176 - val_loss: 0.0170\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0176 - val_loss: 0.0169\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0171 - val_loss: 0.0168\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0173 - val_loss: 0.0173\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0172 - val_loss: 0.0167\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0170 - val_loss: 0.0167\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0169 - val_loss: 0.0171\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0169 - val_loss: 0.0174\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0171 - val_loss: 0.0179\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0174 - val_loss: 0.0168ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0. - ETA: 0s - loss: 0.\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0171 - val_loss: 0.0170\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0173 - val_loss: 0.0166\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0172 - val_loss: 0.0180\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0171 - val_loss: 0.0173\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0171 - val_loss: 0.0167\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0167 - val_loss: 0.0175\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0174 - val_loss: 0.0177\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0170 - val_loss: 0.0167\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0166 - val_loss: 0.0171\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0169 - val_loss: 0.0174\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0163 - val_loss: 0.0167s - loss: 0.01\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0164 - val_loss: 0.0191\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0164 - val_loss: 0.0195\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0167 - val_loss: 0.0188TA: 1s - loss: \n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0173 - val_loss: 0.0185: 0s - loss: 0.\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0172 - val_loss: 0.0178\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0171 - val_loss: 0.0175\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0165 - val_loss: 0.0167\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0165 - val_loss: 0.0166\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0165 - val_loss: 0.0173\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0170 - val_loss: 0.01821s - loss:\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0166 - val_loss: 0.0172\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0163 - val_loss: 0.0172\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0160 - val_loss: 0.0179\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0159 - val_loss: 0.01970s - loss: \n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0158 - val_loss: 0.0187\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0158 - val_loss: 0.0216\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0158 - val_loss: 0.0208\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0157 - val_loss: 0.0233\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0166 - val_loss: 0.0227\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0177 - val_loss: 0.0197\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0175 - val_loss: 0.0174\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 100, Quantile: 0.2, Accuracy: 0.01739262044429779\n",
      "--- 912.0218710899353 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 5s 286ms/step - loss: 0.1158 - val_loss: 0.0727\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0846 - val_loss: 0.0695\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0643 - val_loss: 0.0475\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0662 - val_loss: 0.0511\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0476 - val_loss: 0.0384\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0482 - val_loss: 0.0580\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0460 - val_loss: 0.0491\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0404 - val_loss: 0.0445\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0365 - val_loss: 0.0391\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0338 - val_loss: 0.0361\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0328 - val_loss: 0.0332\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0322 - val_loss: 0.0315\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0322 - val_loss: 0.0292\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0324 - val_loss: 0.0292\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0329 - val_loss: 0.0292\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0329 - val_loss: 0.0300\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0330 - val_loss: 0.0324\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0327 - val_loss: 0.0354\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0323 - val_loss: 0.0357\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0319 - val_loss: 0.0355\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0312 - val_loss: 0.0335\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0308 - val_loss: 0.0310\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0302 - val_loss: 0.0294\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0301 - val_loss: 0.0289\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0300 - val_loss: 0.0280\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0299 - val_loss: 0.0281\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0300 - val_loss: 0.0279\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0298 - val_loss: 0.0267\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0299 - val_loss: 0.0267\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0298 - val_loss: 0.0263\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0298 - val_loss: 0.0261\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0298 - val_loss: 0.0263\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0298 - val_loss: 0.0259\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0297 - val_loss: 0.0263\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0294 - val_loss: 0.0266\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0292 - val_loss: 0.0259\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0291 - val_loss: 0.0260\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0289 - val_loss: 0.0246\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0281 - val_loss: 0.0245\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0285 - val_loss: 0.0253\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0284 - val_loss: 0.0238\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0286 - val_loss: 0.0239\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0291 - val_loss: 0.0251\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0284 - val_loss: 0.0241\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0284 - val_loss: 0.0246\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0281 - val_loss: 0.0249\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0274 - val_loss: 0.0237\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0276 - val_loss: 0.0243\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0274 - val_loss: 0.0235\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0273 - val_loss: 0.0231\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0277 - val_loss: 0.0238\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0282 - val_loss: 0.0236\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0273 - val_loss: 0.0232\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0273 - val_loss: 0.0246\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0276 - val_loss: 0.0234\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0268 - val_loss: 0.0230\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0273 - val_loss: 0.0247\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0274 - val_loss: 0.0232\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0269 - val_loss: 0.0240\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0270 - val_loss: 0.0239\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0267 - val_loss: 0.0240\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0271 - val_loss: 0.0249\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0272 - val_loss: 0.0233\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0263 - val_loss: 0.0238\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0272 - val_loss: 0.0241\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0272 - val_loss: 0.0231\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0266 - val_loss: 0.0232\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0275 - val_loss: 0.0266\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0274 - val_loss: 0.0234\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0263 - val_loss: 0.0230\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0270 - val_loss: 0.0252\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0266 - val_loss: 0.0233\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0263 - val_loss: 0.0231\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0269 - val_loss: 0.0250\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0267 - val_loss: 0.0232\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0261 - val_loss: 0.0239\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0265 - val_loss: 0.0248\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0268 - val_loss: 0.0238\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0262 - val_loss: 0.0232\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0263 - val_loss: 0.0262\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0262 - val_loss: 0.0237\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0257 - val_loss: 0.0237\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0264 - val_loss: 0.0245\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0267 - val_loss: 0.0238\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0259 - val_loss: 0.0230\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0267 - val_loss: 0.0247\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0266 - val_loss: 0.0229\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0258 - val_loss: 0.0229\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0267 - val_loss: 0.0261\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0267 - val_loss: 0.0243\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0257 - val_loss: 0.0235\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0260 - val_loss: 0.0264\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0259 - val_loss: 0.0233\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0253 - val_loss: 0.0238\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0259 - val_loss: 0.0253\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0260 - val_loss: 0.0236\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0253 - val_loss: 0.0231\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0259 - val_loss: 0.0257\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0261 - val_loss: 0.0242\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0253 - val_loss: 0.0232\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0257 - val_loss: 0.0237\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0259 - val_loss: 0.0240\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0258 - val_loss: 0.0235\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0262 - val_loss: 0.0247\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0263 - val_loss: 0.0230\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0253 - val_loss: 0.0229\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0253 - val_loss: 0.0251\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0256 - val_loss: 0.0236\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0250 - val_loss: 0.0227\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0255 - val_loss: 0.0243\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0254 - val_loss: 0.0234\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0251 - val_loss: 0.0230\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0256 - val_loss: 0.0240\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0256 - val_loss: 0.0240\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0251 - val_loss: 0.0233\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0259 - val_loss: 0.0251\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0257 - val_loss: 0.0232\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0251 - val_loss: 0.0235\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0251 - val_loss: 0.0245\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0252 - val_loss: 0.0245\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0248 - val_loss: 0.0232\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0251 - val_loss: 0.0244\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0257 - val_loss: 0.0240\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0250 - val_loss: 0.0234\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0253 - val_loss: 0.0239\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0258 - val_loss: 0.0253\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0251 - val_loss: 0.0228\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0254 - val_loss: 0.0250\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0254 - val_loss: 0.0235\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0247 - val_loss: 0.0227\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0251 - val_loss: 0.0238\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0250 - val_loss: 0.0237\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0246 - val_loss: 0.0229A: 0s - loss: 0\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0249 - val_loss: 0.0247\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0250 - val_loss: 0.0234\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0245 - val_loss: 0.0231\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0250 - val_loss: 0.0241\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0250 - val_loss: 0.0237\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0246 - val_loss: 0.0233\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0250 - val_loss: 0.0238\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0246 - val_loss: 0.0231\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0245 - val_loss: 0.0236\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0246 - val_loss: 0.0238\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0246 - val_loss: 0.0234\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0242 - val_loss: 0.0232\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0246 - val_loss: 0.0253\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0247 - val_loss: 0.0234\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0240 - val_loss: 0.0231\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0246 - val_loss: 0.0239\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0247 - val_loss: 0.0241\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0241 - val_loss: 0.0232\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0245 - val_loss: 0.0245\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0247 - val_loss: 0.0232\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0241 - val_loss: 0.0228\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0244 - val_loss: 0.0239\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0244 - val_loss: 0.0231\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0239 - val_loss: 0.0232\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0248 - val_loss: 0.0238\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0243 - val_loss: 0.0245\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0253 - val_loss: 0.0247\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0255 - val_loss: 0.0231\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0243 - val_loss: 0.0229\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0245 - val_loss: 0.0250\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0245 - val_loss: 0.0229\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0237 - val_loss: 0.0231\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0241 - val_loss: 0.0231\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0242 - val_loss: 0.0236\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0237 - val_loss: 0.0231\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0240 - val_loss: 0.0238\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0242 - val_loss: 0.0237\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0240 - val_loss: 0.0233\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0239 - val_loss: 0.0231\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0243 - val_loss: 0.0239\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0240 - val_loss: 0.0229\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0236 - val_loss: 0.0236\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0238 - val_loss: 0.0233\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0238 - val_loss: 0.0235\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0239 - val_loss: 0.0234\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0238 - val_loss: 0.0240\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0239 - val_loss: 0.0235\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0240 - val_loss: 0.0238\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0244 - val_loss: 0.0230\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0241 - val_loss: 0.0236\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0239 - val_loss: 0.0231\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0236 - val_loss: 0.0234\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0239 - val_loss: 0.0239\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0237 - val_loss: 0.0232\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0236 - val_loss: 0.0233\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0237 - val_loss: 0.0239\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0238 - val_loss: 0.0235\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0239 - val_loss: 0.0247\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0241 - val_loss: 0.0238\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0238 - val_loss: 0.0244\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0240 - val_loss: 0.0235\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0238 - val_loss: 0.0242\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0235 - val_loss: 0.0229\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0233 - val_loss: 0.0243\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0235 - val_loss: 0.0234\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0232 - val_loss: 0.0235\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 100, Quantile: 0.4, Accuracy: 0.023505643010139465\n",
      "--- 1790.7503998279572 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 5s 283ms/step - loss: 0.1656 - val_loss: 0.1032\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0955 - val_loss: 0.0545\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0655 - val_loss: 0.0439\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0570 - val_loss: 0.0489\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0467 - val_loss: 0.0629\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0408 - val_loss: 0.0443\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0359 - val_loss: 0.0380\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0340 - val_loss: 0.0328\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0330 - val_loss: 0.0307\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0336 - val_loss: 0.0299\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0355 - val_loss: 0.0324TA: 1s - loss:\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0374 - val_loss: 0.0392\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0372 - val_loss: 0.0456\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0350 - val_loss: 0.0422\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0330 - val_loss: 0.0356\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0316 - val_loss: 0.0344\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0307 - val_loss: 0.0301\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0306 - val_loss: 0.0291\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0317 - val_loss: 0.0294\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0325 - val_loss: 0.0313\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0324 - val_loss: 0.0325\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0325 - val_loss: 0.0333\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0316 - val_loss: 0.0329\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0308 - val_loss: 0.0307\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0307 - val_loss: 0.0302\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0300 - val_loss: 0.0295\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0301 - val_loss: 0.0287 ETA: 0s - loss: 0.03\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0300 - val_loss: 0.0280\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0300 - val_loss: 0.0278\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0297 - val_loss: 0.02790s - loss: \n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0297 - val_loss: 0.0274\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0300 - val_loss: 0.0289\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0293 - val_loss: 0.0271\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0293 - val_loss: 0.0274\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0296 - val_loss: 0.0302\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0295 - val_loss: 0.0258\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0294 - val_loss: 0.0276\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0293 - val_loss: 0.0272\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0291 - val_loss: 0.0281\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0286 - val_loss: 0.0267\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0286 - val_loss: 0.0277\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0285 - val_loss: 0.0279\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0286 - val_loss: 0.0262\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0279 - val_loss: 0.0261\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0284 - val_loss: 0.0277\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0283 - val_loss: 0.0258\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0286 - val_loss: 0.0269\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0281 - val_loss: 0.0262\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0280 - val_loss: 0.0265\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0278 - val_loss: 0.0265\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0279 - val_loss: 0.0268\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0275 - val_loss: 0.0252\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0277 - val_loss: 0.0265\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0281 - val_loss: 0.0257\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0277 - val_loss: 0.0251\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0276 - val_loss: 0.0266\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0279 - val_loss: 0.0253\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0274 - val_loss: 0.0252\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0274 - val_loss: 0.0269\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0273 - val_loss: 0.0250TA: 0s - loss: 0.\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0270 - val_loss: 0.0252\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0276 - val_loss: 0.0274\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0274 - val_loss: 0.0259\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0266 - val_loss: 0.0260\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0268 - val_loss: 0.0249\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0271 - val_loss: 0.0238\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0284 - val_loss: 0.0289\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0281 - val_loss: 0.0243\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0280 - val_loss: 0.0244\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0280 - val_loss: 0.0278\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0275 - val_loss: 0.0261\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0269 - val_loss: 0.0280\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0264 - val_loss: 0.0253\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0259 - val_loss: 0.0255\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0263 - val_loss: 0.0260\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0267 - val_loss: 0.0260\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0267 - val_loss: 0.0254\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0265 - val_loss: 0.0252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0266 - val_loss: 0.0267\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0271 - val_loss: 0.0266\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0267 - val_loss: 0.0265\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0264 - val_loss: 0.0252\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0266 - val_loss: 0.0263\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0273 - val_loss: 0.0269\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0264 - val_loss: 0.0247\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0266 - val_loss: 0.0264\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0273 - val_loss: 0.0237\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0266 - val_loss: 0.0242\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0270 - val_loss: 0.0285\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0271 - val_loss: 0.0253\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0260 - val_loss: 0.0240\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0263 - val_loss: 0.0271\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0265 - val_loss: 0.0236\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0264 - val_loss: 0.0268\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0261 - val_loss: 0.0265\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0256 - val_loss: 0.0237\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0260 - val_loss: 0.0258\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0261 - val_loss: 0.0258\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0259 - val_loss: 0.0238\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0257 - val_loss: 0.0263\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0260 - val_loss: 0.0239\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0257 - val_loss: 0.0234\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0265 - val_loss: 0.0280\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0269 - val_loss: 0.0241\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0264 - val_loss: 0.0241\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0260 - val_loss: 0.0266\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0261 - val_loss: 0.0240\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0258 - val_loss: 0.0271\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0256 - val_loss: 0.0243\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0251 - val_loss: 0.0247\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0250 - val_loss: 0.0252\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0255 - val_loss: 0.0257\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0252 - val_loss: 0.0241\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0257 - val_loss: 0.0261A: 1s - l\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0261 - val_loss: 0.0245\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0255 - val_loss: 0.0247\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0256 - val_loss: 0.0272\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0256 - val_loss: 0.0239\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0255 - val_loss: 0.0248\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0256 - val_loss: 0.0263\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0259 - val_loss: 0.0242\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0252 - val_loss: 0.0264\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0252 - val_loss: 0.0240\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0249 - val_loss: 0.0239\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0253 - val_loss: 0.0269 ETA: 1s - loss: 0.0 - ETA: 1s - loss: \n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0258 - val_loss: 0.0242\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0255 - val_loss: 0.0248\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0257 - val_loss: 0.0259\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0255 - val_loss: 0.0243\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0248 - val_loss: 0.0247\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0248 - val_loss: 0.0251\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0247 - val_loss: 0.0240\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0246 - val_loss: 0.0245\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0252 - val_loss: 0.0271\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0255 - val_loss: 0.0246\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0264 - val_loss: 0.0290\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0266 - val_loss: 0.0264\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0252 - val_loss: 0.0233\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0253 - val_loss: 0.0259\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0251 - val_loss: 0.0238\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0256 - val_loss: 0.0269\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0254 - val_loss: 0.0258\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0247 - val_loss: 0.0237\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0257 - val_loss: 0.0276\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0260 - val_loss: 0.0238\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0260 - val_loss: 0.0260\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0258 - val_loss: 0.0253\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0252 - val_loss: 0.0254\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0247 - val_loss: 0.0252\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0244 - val_loss: 0.0240\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0245 - val_loss: 0.0249\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0248 - val_loss: 0.0253\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0244 - val_loss: 0.0238\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0245 - val_loss: 0.0250\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0248 - val_loss: 0.0242\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0244 - val_loss: 0.0240\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0250 - val_loss: 0.0262\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0251 - val_loss: 0.0242\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0249 - val_loss: 0.0244\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0252 - val_loss: 0.0262\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0250 - val_loss: 0.0242\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0244 - val_loss: 0.0252\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0242 - val_loss: 0.0242\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0242 - val_loss: 0.0256\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0248 - val_loss: 0.0245\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0248 - val_loss: 0.0250\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0248 - val_loss: 0.0244\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0245 - val_loss: 0.0245\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0246 - val_loss: 0.0264\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0245 - val_loss: 0.0243\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0240 - val_loss: 0.0247\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0246 - val_loss: 0.0258\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0246 - val_loss: 0.0251\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0242 - val_loss: 0.0247\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0251 - val_loss: 0.0258\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0251 - val_loss: 0.0244\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0247 - val_loss: 0.0248\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0244 - val_loss: 0.0247\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0241 - val_loss: 0.0241\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0248 - val_loss: 0.0252\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0248 - val_loss: 0.0253\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0258 - val_loss: 0.0254\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0253 - val_loss: 0.0238\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0249 - val_loss: 0.0253\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0247 - val_loss: 0.0264\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0241 - val_loss: 0.0240\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0244 - val_loss: 0.0254\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0245 - val_loss: 0.0242\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0237 - val_loss: 0.0241\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0240 - val_loss: 0.0255\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0241 - val_loss: 0.0244\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0237 - val_loss: 0.0245\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0241 - val_loss: 0.0248\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0240 - val_loss: 0.0246\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0240 - val_loss: 0.0244\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0243 - val_loss: 0.0270\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0243 - val_loss: 0.0243\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0238 - val_loss: 0.0244\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 100, Quantile: 0.6, Accuracy: 0.024424508213996887\n",
      "--- 2652.6745252609253 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 5s 287ms/step - loss: 0.2421 - val_loss: 0.1094\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0712 - val_loss: 0.0475\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0625 - val_loss: 0.0498\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0492 - val_loss: 0.0315\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0449 - val_loss: 0.0515\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0424 - val_loss: 0.0308\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0321 - val_loss: 0.0358\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0287 - val_loss: 0.0257\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0288 - val_loss: 0.0320\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0298 - val_loss: 0.0335\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0277 - val_loss: 0.0286\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0259 - val_loss: 0.0262\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0257 - val_loss: 0.0274\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0256 - val_loss: 0.0264\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0258 - val_loss: 0.0274\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0257 - val_loss: 0.0270\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0252 - val_loss: 0.0262\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0245 - val_loss: 0.0256\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0240 - val_loss: 0.0249\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0241 - val_loss: 0.0244\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0241 - val_loss: 0.0255\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0240 - val_loss: 0.0251\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0233 - val_loss: 0.0234\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0230 - val_loss: 0.0240\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0232 - val_loss: 0.0242\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0231 - val_loss: 0.0245\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0228 - val_loss: 0.0233\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0226 - val_loss: 0.0227\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0223 - val_loss: 0.0235\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0221 - val_loss: 0.0214\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0224 - val_loss: 0.0209\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0229 - val_loss: 0.02500s - loss: 0.\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0227 - val_loss: 0.0216\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0231 - val_loss: 0.0253\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0224 - val_loss: 0.0203\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0231 - val_loss: 0.0257\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0228 - val_loss: 0.0216\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0224 - val_loss: 0.0234\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0217 - val_loss: 0.0208\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0220 - val_loss: 0.0249\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0218 - val_loss: 0.0198\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0214 - val_loss: 0.0217TA: 0s - loss: 0.\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0212 - val_loss: 0.0205\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0210 - val_loss: 0.0215\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0210 - val_loss: 0.0206\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0209 - val_loss: 0.0213\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0209 - val_loss: 0.0211\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0209 - val_loss: 0.0206\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0206 - val_loss: 0.0207\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0208 - val_loss: 0.0207\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0205 - val_loss: 0.0209\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0206 - val_loss: 0.0205\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0206 - val_loss: 0.0207\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0204 - val_loss: 0.0198\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0206 - val_loss: 0.0207\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0207 - val_loss: 0.0207\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0203 - val_loss: 0.0188\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0213 - val_loss: 0.0253\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0215 - val_loss: 0.0192\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0224 - val_loss: 0.0240\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0214 - val_loss: 0.0252\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0204 - val_loss: 0.0209\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0195 - val_loss: 0.0201\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0193 - val_loss: 0.0200\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0190 - val_loss: 0.0177\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0191 - val_loss: 0.0190\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0196 - val_loss: 0.0200\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0198 - val_loss: 0.0192\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0198 - val_loss: 0.0226\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0198 - val_loss: 0.0193\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0194 - val_loss: 0.0186\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0200 - val_loss: 0.0228\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0200 - val_loss: 0.0187\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0196 - val_loss: 0.0196\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0203 - val_loss: 0.0255\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0208 - val_loss: 0.0182\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0221 - val_loss: 0.0253\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0205 - val_loss: 0.0207\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0195 - val_loss: 0.0211\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0192 - val_loss: 0.0206\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0189 - val_loss: 0.0177\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0188 - val_loss: 0.0191\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0186 - val_loss: 0.0194\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0185 - val_loss: 0.0177\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0191 - val_loss: 0.0218\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0196 - val_loss: 0.0188\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0194 - val_loss: 0.0185\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0199 - val_loss: 0.0248\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0205 - val_loss: 0.0183\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0201 - val_loss: 0.0229\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0194 - val_loss: 0.0207\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0189 - val_loss: 0.0188\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0186 - val_loss: 0.0200\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0186 - val_loss: 0.0182\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0185 - val_loss: 0.0186\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0185 - val_loss: 0.0194\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0185 - val_loss: 0.0180\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0188 - val_loss: 0.0208\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0191 - val_loss: 0.0190\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0188 - val_loss: 0.0187\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0190 - val_loss: 0.0216\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0189 - val_loss: 0.0187\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0187 - val_loss: 0.0191\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0188 - val_loss: 0.0213\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0188 - val_loss: 0.0182\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0186 - val_loss: 0.0201\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0189 - val_loss: 0.0192\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0185 - val_loss: 0.0181\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0189 - val_loss: 0.0218\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0193 - val_loss: 0.0187\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0193 - val_loss: 0.0200\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0196 - val_loss: 0.0234\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0196 - val_loss: 0.0183\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0192 - val_loss: 0.0227\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0190 - val_loss: 0.0196\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0183 - val_loss: 0.0178\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0185 - val_loss: 0.0211\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0186 - val_loss: 0.0180\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0186 - val_loss: 0.0200\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0188 - val_loss: 0.0209\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0186 - val_loss: 0.0179\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0193 - val_loss: 0.0228\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0194 - val_loss: 0.0182\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0190 - val_loss: 0.0186\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0195 - val_loss: 0.0233\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0193 - val_loss: 0.0183\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0186 - val_loss: 0.0203\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0183 - val_loss: 0.0196\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0181 - val_loss: 0.0178\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0183 - val_loss: 0.0203\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0185 - val_loss: 0.0184\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0184 - val_loss: 0.0184\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0187 - val_loss: 0.0216\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0190 - val_loss: 0.0188\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0188 - val_loss: 0.0199\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0185 - val_loss: 0.0208\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0182 - val_loss: 0.0183\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0183 - val_loss: 0.0206\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0184 - val_loss: 0.0182\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0182 - val_loss: 0.0189\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0185 - val_loss: 0.0208\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0186 - val_loss: 0.0185\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0184 - val_loss: 0.0197\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0182 - val_loss: 0.0195\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0179 - val_loss: 0.0178\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0178 - val_loss: 0.0201\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0181 - val_loss: 0.0183\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0182 - val_loss: 0.0191\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0183 - val_loss: 0.0196\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0183 - val_loss: 0.0186\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0187 - val_loss: 0.0214\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0188 - val_loss: 0.0186\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0184 - val_loss: 0.0193\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0188 - val_loss: 0.0215\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0187 - val_loss: 0.0186\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0183 - val_loss: 0.0197\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0182 - val_loss: 0.0206\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0180 - val_loss: 0.0185\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0181 - val_loss: 0.0204\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0180 - val_loss: 0.0191\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0176 - val_loss: 0.0179\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 0.0178 - val_loss: 0.0199\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0182 - val_loss: 0.0186\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0181 - val_loss: 0.0194\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 0.0184 - val_loss: 0.0191\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0182 - val_loss: 0.0188\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0187 - val_loss: 0.0220\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0189 - val_loss: 0.0185\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0181 - val_loss: 0.0183\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0185 - val_loss: 0.0217\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0190 - val_loss: 0.0186\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0189 - val_loss: 0.0208\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0187 - val_loss: 0.0205\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0181 - val_loss: 0.0187\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0177 - val_loss: 0.0190\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0176 - val_loss: 0.0186\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0176 - val_loss: 0.0185\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0175 - val_loss: 0.0195\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0176 - val_loss: 0.0184\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0181 - val_loss: 0.0208\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0183 - val_loss: 0.0188\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0186 - val_loss: 0.0202\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0186 - val_loss: 0.0188\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0180 - val_loss: 0.0195\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0177 - val_loss: 0.0204\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0180 - val_loss: 0.0187\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0177 - val_loss: 0.0193\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0176 - val_loss: 0.0182\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0176 - val_loss: 0.0193\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0174 - val_loss: 0.0185\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0173 - val_loss: 0.0181\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0177 - val_loss: 0.0204\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0183 - val_loss: 0.0196\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0192 - val_loss: 0.0222\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0190 - val_loss: 0.0186\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0183 - val_loss: 0.0204\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0177 - val_loss: 0.0196\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0178 - val_loss: 0.0188\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAriklEQVR4nO3dd5hU5d3/8fd3ZrbC0hcElmoQKSIgKooFYgMLFohBTdFoiO1nNI95NFWTPElMYnyMeVRiwagR0WiIxGAXLLGxKB2pUpa2S9++OzP3748zuztsgVnY3VkPn9d1ce3MKTPfOTN85p77nHMfc84hIiL+FUh2ASIi0rwU9CIiPqegFxHxOQW9iIjPKehFRHxOQS8i4nOhRBYys/HAn4Ag8Jhz7p5a868C7ojdLQJucM4tis1bDxQCESDsnBt1sOfr0qWL69u3b4IvQUREFixYsMM5l13fvIMGvZkFgQeBc4A8YL6ZzXbOLY9b7AvgTOfcbjObADwCnBw3f5xzbkeiBfft25fc3NxEFxcROeKZ2YaG5iXSdXMSsMY5t845VwHMBC6OX8A594Fzbnfs7kdAzqEWKyIiTSuRoO8JbIq7nxeb1pBrgVfi7jvgdTNbYGZTG1rJzKaaWa6Z5RYUFCRQloiIJCKRPnqrZ1q94yaY2Ti8oD8tbvIY59wWM+sKvGFmnzvn3q3zgM49gtflw6hRozQug4hIE0kk6POAXnH3c4AttRcys2HAY8AE59zOqunOuS2xv/lmNguvK6hO0IuIP1VWVpKXl0dZWVmyS/GF9PR0cnJySElJSXidRIJ+PjDAzPoBm4EpwJXxC5hZb+AfwDedc6viprcBAs65wtjtc4FfJlydiHzp5eXlkZWVRd++fTGrr4NAEuWcY+fOneTl5dGvX7+E1zto0DvnwmZ2M/Aa3uGV051zy8zs+tj8acDPgc7AQ7E3suowym7ArNi0EDDDOfdq416aiHyZlZWVKeSbiJnRuXNnGrsfM6Hj6J1zc4A5taZNi7t9HXBdPeutA45vVEUi4jsK+aZzKNvSV2fGPvDWat5ZpSN2RETi+SroH563lv+sSfi8LBE5AuzZs4eHHnqo0eudf/757Nmzp+kLSgJfBX3AIBLVkZkiUqOhoI9EIgdcb86cOXTo0KGZqmpZCfXRf1kEAqagF5H93Hnnnaxdu5bhw4eTkpJC27Zt6d69OwsXLmT58uVccsklbNq0ibKyMr7//e8zdap3XmfVUCxFRUVMmDCB0047jQ8++ICePXvy0ksvkZGRkeRXljhfBX0wYOgauCKt1y/+tYzlW/Y16WMO7tGOuy4a0uD8e+65h6VLl7Jw4ULmzZvHBRdcwNKlS6sPT5w+fTqdOnWitLSUE088kUmTJtG5c+f9HmP16tU8++yzPProo1x++eW8+OKLfOMb32jS19GcfBX0ATMiCnoROYCTTjppv2PQH3jgAWbNmgXApk2bWL16dZ2g79evH8OHDwfghBNOYP369S1VbpPwXdCr50ak9TpQy7ultGnTpvr2vHnzePPNN/nwww/JzMxk7Nix9Z7Bm5aWVn07GAxSWlraIrU2Fd/tjI0q6UUkTlZWFoWFhfXO27t3Lx07diQzM5PPP/+cjz76qIWraxm+atEHtTNWRGrp3LkzY8aMYejQoWRkZNCtW7fqeePHj2fatGkMGzaMgQMHMnr06CRW2nx8FfTquhGR+syYMaPe6Wlpabzyyiv1zqvqh+/SpQtLly6tnn777bc3eX3NzV9dNwGIamesiMh+fBX0QTMFvYhILb4Kep0wJSJSl7+CXi16EZE6fBX0QTOi0WRXISLSuvgq6M3QmbEiIrX4Kug11o2IHK62bdsCsGXLFiZPnlzvMmPHjiU3N/eAj3P//fdTUlJSfT+Zwx77Lui1M1ZEmkKPHj144YUXDnn92kGfzGGPfRX0ZkZEOS8ice644479xqO/++67+cUvfsFZZ53FyJEjOe6443jppZfqrLd+/XqGDh0KQGlpKVOmTGHYsGF8/etf32+smxtuuIFRo0YxZMgQ7rrrLsAbKG3Lli2MGzeOcePGAd6wxzt2eBdGuu+++xg6dChDhw7l/vvvr36+QYMG8d3vfpchQ4Zw7rnnNtmYOr46MzZoqOtGpDV75U7YtqRpH/Oo42DCPQ3OnjJlCrfeeis33ngjAM8//zyvvvoqt912G+3atWPHjh2MHj2aiRMnNng91ocffpjMzEwWL17M4sWLGTlyZPW8X//613Tq1IlIJMJZZ53F4sWLueWWW7jvvvuYO3cuXbp02e+xFixYwBNPPMHHH3+Mc46TTz6ZM888k44dOzbbcMi+atEHTF03IrK/ESNGkJ+fz5YtW1i0aBEdO3ake/fu/PjHP2bYsGGcffbZbN68me3btzf4GO+++2514A4bNoxhw4ZVz3v++ecZOXIkI0aMYNmyZSxfvvyA9bz//vtceumltGnThrZt23LZZZfx3nvvAc03HLKvWvQ6YUqklTtAy7s5TZ48mRdeeIFt27YxZcoUnnnmGQoKCliwYAEpKSn07du33uGJ49XX2v/iiy+49957mT9/Ph07duTqq68+6OMcqNehuYZD9lWLPmiGem5EpLYpU6Ywc+ZMXnjhBSZPnszevXvp2rUrKSkpzJ07lw0bNhxw/TPOOINnnnkGgKVLl7J48WIA9u3bR5s2bWjfvj3bt2/fb4C0hoZHPuOMM/jnP/9JSUkJxcXFzJo1i9NPP70JX21dPmvRQ4X2xopILUOGDKGwsJCePXvSvXt3rrrqKi666CJGjRrF8OHDOfbYYw+4/g033MA111zDsGHDGD58OCeddBIAxx9/PCNGjGDIkCH079+fMWPGVK8zdepUJkyYQPfu3Zk7d2719JEjR3L11VdXP8Z1113HiBEjmvWqVdYad16OGjXKHewY1fp88/GPKSoPM+vGMQdfWERaxIoVKxg0aFCyy/CV+rapmS1wzo2qb3lfdd0EzHSFKRGRWnwV9MGALg4uIlKbr4I+oEHNRFql1thF/GV1KNvSZ0GvK0yJtDbp6ens3LlTYd8EnHPs3LmT9PT0Rq3nq6NuggGNRy/S2uTk5JCXl0dBQUGyS/GF9PR0cnJyGrWOr4JeZ8aKtD4pKSn069cv2WUc0fzVdRMwlPMiIvvzVdAH1UcvIlKHr4JeXTciInX5K+gDGutGRKS2hILezMab2UozW2Nmd9Yz/yozWxz794GZHZ/ouk0pqBa9iEgdBw16MwsCDwITgMHAFWY2uNZiXwBnOueGAb8CHmnEuk0mENDFwUVEakukRX8SsMY5t845VwHMBC6OX8A594Fzbnfs7kdATqLrNqWA6eLgIiK1JRL0PYFNcffzYtMaci1QNShzwuua2VQzyzWz3EM9sUI7Y0VE6kok6Ou7iGK9aWpm4/CC/o7Gruuce8Q5N8o5Nyo7OzuBsuoK6jh6EZE6EjkzNg/oFXc/B9hSeyEzGwY8Bkxwzu1szLpNRcMUi4jUlUiLfj4wwMz6mVkqMAWYHb+AmfUG/gF80zm3qjHrNqWAaWesiEhtB23RO+fCZnYz8BoQBKY755aZ2fWx+dOAnwOdgYdiF9ANx7ph6l23mV6LBjUTEalHQoOaOefmAHNqTZsWd/s64LpE120upvHoRUTq8NWZscGAxroREanNX0FvupSgiEhtvgp6M2+sG500JSJSw1dBHwx4h+3rCEsRkRq+CvpYzuvsWBGROP4K+uoWvYJeRKSKr4I+aAp6EZHafBX0gVjQq+tGRKSGv4JeO2NFROrwV9DHdsZqYDMRkRq+CvqgdsaKiNThq6Cv7qNX0IuIVPNl0GtgMxGRGr4K+mDs1ajrRkSkhq+CXodXiojU5cugV4NeRKSGr4K+6qgb7YwVEanhq6A3DWomIlKHr4K+qkWv8ehFRGr4K+h1HL2ISB2+CnrTcfQiInX4Kug1BIKISF2+CnpdYUpEpC5/Bb1a9CIidfgq6HWFKRGRunwV9NWDminnRUSq+SvoY69GffQiIjX8FfTVh1cq6EVEqvgq6IO6ZqyISB2+CnpdYUpEpC6fBb33V103IiI1fBX0OjNWRKQuXwW9rjAlIlKXL4NeOS8iUsNXQa+uGxGRuhIKejMbb2YrzWyNmd1Zz/xjzexDMys3s9trzVtvZkvMbKGZ5TZV4fXRoGYiInWFDraAmQWBB4FzgDxgvpnNds4tj1tsF3ALcEkDDzPOObfjMGs9KA1qJiJSVyIt+pOANc65dc65CmAmcHH8As65fOfcfKCyGWpMmAY1ExGpK5Gg7wlsirufF5uWKAe8bmYLzGxqQwuZ2VQzyzWz3IKCgkY8fI2ArjAlIlJHIkFv9UxrTJN5jHNuJDABuMnMzqhvIefcI865Uc65UdnZ2Y14+BrVg5qpRS8iUi2RoM8DesXdzwG2JPoEzrktsb/5wCy8rqBmoUHNRETqSiTo5wMDzKyfmaUCU4DZiTy4mbUxs6yq28C5wNJDLfZgNKiZiEhdBz3qxjkXNrObgdeAIDDdObfMzK6PzZ9mZkcBuUA7IGpmtwKDgS7ALPNa2iFghnPu1WZ5JWhQMxGR+hw06AGcc3OAObWmTYu7vQ2vS6e2fcDxh1NgY1QdR+8U9CIi1Xx5ZqxOmBIRqeGroDcNaiYiUoevgr6qRa+eGxGRGv4Keu2MFRGpw1dBb1VXmFLQi4hU81XQVx9Hrz56EZFqvgr6mitMJbkQEZFWxGdB7/1V142ISA1fBb2ZETAFvYhIPF8FPXjdNwp6EZEa/gv6gKmPXkQkjv+CXl03IiL78V3QB810eKWISBzfBX0gYDozVkQkjv+C3kxj3YiIxPFd0AcDptErRUTi+C7oA6auGxGReD4Mel1hSkQknu+CXl03IiL7813Qe2fGJrsKEZHWw39BH9AwxSIi8XwX9EHtjBUR2Y/vgl5dNyIi+/Nf0Ac0BIKISDz/Bb0GNRMR2Y8Pg16HV4qIxPNd0AcDuvCIiEg83wW9dsaKiOzPf0GvM2NFRPbjr6B/4gIuLJ2trhsRkTihZBfQpLYuonuoi4JeRCSOv1r0oVRSCKvrRkQkjs+CPp1UV6GdsSIicfwV9MFUUqnUmbEiInH8FfShNFKoVB+9iEichILezMab2UozW2Nmd9Yz/1gz+9DMys3s9sas26SCqaS4MBHlvIhItYMGvZkFgQeBCcBg4AozG1xrsV3ALcC9h7Bu06lq0avrRkSkWiIt+pOANc65dc65CmAmcHH8As65fOfcfKCyses2qeqdsQp6EZEqiQR9T2BT3P282LREJLyumU01s1wzyy0oKEjw4WsJppJCpQ6vFBGJk0jQWz3TEk3ShNd1zj3inBvlnBuVnZ2d4MPXEkoj5CpRg15EpEYiQZ8H9Iq7nwNsSfDxD2fdxgumkuIqdSlBEZE4iQT9fGCAmfUzs1RgCjA7wcc/nHUbL5RGSDtjRUT2c9CxbpxzYTO7GXgNCALTnXPLzOz62PxpZnYUkAu0A6Jmdisw2Dm3r751m+m1VHfdaGesiEiNhAY1c87NAebUmjYt7vY2vG6ZhNZtNsE0UlyFum5EROL47szYkKskGk12ISIirYe/gj6YSiiq4+hFROL5K+hDaQSJEI1Ekl2JiEir4bugBwi52ifoiogcufwV9MGqoK9IciEiIq2Hv4I+lApAMKqgFxGp4q+gr2rR1xlbTUTkyOWvoK/qo1eLXkSkmr+CPuh13YRcOMmFiIi0Hv4K+lC690c7Y0VEqvks6L0WfYqCXkSkmr+CPrYzNkXH0YuIVPNX0IdqjrpxGgZBRATwW9DHdsamEdZVpkREYvwV9LGdsanoKlMiIlV8FvSxFr3pAuEiIlX8FfSxnbGp6roREanmr6APVQW9um5ERKr4K+hjO2NT0XVjRUSq+Cvoq3fGhimt0MVHRETAb0EfDOEsQKpVsmLrvmRXIyLSKvgr6AGCaaRRydLNe5NdiYhIq+C7oLdQKtkZsERBLyICQCjZBTS5YBpd0wIs3ayuGxER8GGLnlA62RmweU8pu4o1iqWIiA+DPpVO6d6hleq+ERHxY9AH0+iQEgXQDlkREfwY9KFUUlwlOR0zWLmtMNnViIgknf+CPpgGkXJ6d8pk0+6SZFcjIpJ0/gv6UBqEK7yg36WgFxHxZ9BHyunVKZMdRRWUVISTXZGISFL5L+iDqRCuIKdjBgB5u0uTXJCISHL5L+hDaRAuo3enTAA27lT3jYgc2fwX9ME0iFTQKxb02iErIkc6/wV9KA3C5XRuk0pmapCN2iErIke4hILezMab2UozW2Nmd9Yz38zsgdj8xWY2Mm7eejNbYmYLzSy3KYuvV2xnrJnRq2Mmm3apj15EjmwHDXozCwIPAhOAwcAVZja41mITgAGxf1OBh2vNH+ecG+6cG3X4JR9EbGcsQK9OGd4hlnN+CB//pdmfWkSkNUqkRX8SsMY5t845VwHMBC6utczFwFPO8xHQwcy6N3GtiYm16Nm2hIFZZWzaXYJb/hKsei0p5YiIJFsiQd8T2BR3Py82LdFlHPC6mS0ws6kNPYmZTTWzXDPLLSgoSKCsBgTTwEXhkXFctPOvlFZUQvEOKD6MxxQR+RJLJOitnmm1r7x9oGXGOOdG4nXv3GRmZ9T3JM65R5xzo5xzo7KzsxMoqwGhNO9vtJKegV20pxhzESjZeeiPKSLyJZZI0OcBveLu5wBbEl3GOVf1Nx+YhdcV1Hza9YBQBnQ6mqzKHZzW3fu+ccUF4Gp/P4mI+F8iQT8fGGBm/cwsFZgCzK61zGzgW7Gjb0YDe51zW82sjZllAZhZG+BcYGkT1l/XcZPhv9dBn1OhKJ/Jx6YDYJEKKNdoliJy5DnopQSdc2Ezuxl4DQgC051zy8zs+tj8acAc4HxgDVACXBNbvRswy8yqnmuGc+7VJn8VtaVmQttuUJzPqd0qa15LcQGW3q7Zn15EpDVJ6Jqxzrk5eGEeP21a3G0H3FTPeuuA4w+zxkOTdRS4KKm7VlVPevWTZUyYcHRSyhERSRb/nRlbpW1X7+/2ZdWTXv5gEfmFZUkqSEQkOXwc9Ed5f7ctpeqgoLbRvSzcuCdpJYmIJIOPgz7Wot+7ETr2BaAze1mdX5S8mkREksDHQd+t5naHXpDalt7pJazariNvROTI4t+gT82EtNgRNm2yIbMzvdNKWbVdLXoRObL4N+ihpvumTTa0yeaoUCFrC4qIRHXilIgcOXwe9LHumzbZ0KYLHdnHn+1eil+8Obl1iYi0oISOo//SqhX07Ure5bxgCSXrt+2/XDQKAX9/54nIkcvf6RYf9JldCIa9q01lFudBeayvfsGT8Ls+sOGDJBUpItK8fB70+/fRA1RU/Ygp+Bw+fAj+dQuU74Pc6UkqUkSkefk76LsMgEDIO7yyTRcA5ne5DIDd6xfCh/8HfU+HEd+Az/9d08oXEfERfwf9wAvgls+8cW/6ngaDL6H/pLspdmns+Ggm7NvMss7n8uvNI6CyBFbOOfhjioh8yfg76AMB6NDbu90+By5/ku7de7K7zdEMKJoPwHf/057HN3VjG12oyH1KY9aLiO/4+6ibBnTpPxyWLmd7am9uGD+W43Pa8+Qj47lj498o++QJ0lNTYdc6GH0TLJ8FGz+GniNh8MXehU1ERL5EzLXCFuyoUaNcbm5u8z3Bhw/Baz+C0TfC+N8C8PrSLWQ+/zVOC9RcF8VZEHMRKlI7klqxGxcIYUMug2GXQ/+xEExpvhrr4xyU7obMTi37vCLS6pnZAufcqPrm+bvrpiE9R3p/jxlfPencoT0oueAh5kWH86PKaxlffg8zKsdyS8XNHLPv/ziz/D6eCp9D8dJ/wzOTcfcOgNm3wOo34OXb4Hd94ZFx8OnT+z+Xc1CUDxUlide36wvIfcJbd+1c+OuF3vpLX4R7j4H8FfsvH66ARTMhUln/4zW1d/4Ar/+s5v62JfDCd6BgVcPrHIp/fR9euNa7/enT8Obdh/d425bA7g2HXZbIl80R2XVD79Fwc653VE6cc08+nvxBr9F+Tynnllayr3QiX2uTyrdTQ2zdW8pnG0/hgiWbOLrwYy51H3H2Z8+R/umTRAjwQeoYeu/YSp/ZN1O4eQVZrgi2fOqFdkWRd3jneb+F3V9AMNX7NVG6yzvSp8tX4Iv34It3YdyP4fWfwucve6NuvnkXbF0Eq16Fxc9BtBI+/gtcdL/Xus/oCJ8+CXNuBxeF4Vc277aLRuHjaVBRDON+Astfgpdu8uoKpMBlf2ma56ksg8V/BxeBcDl89JD3BTfqOzX7XRrDOfjbZOg6CL71z6apsSmU7ob0DuBdhU2kWRyZQQ91Qr5K13bpdG2XXs+cjlw4rAc/mnAs81YOZ+7KS/nblnxGRpawKy2Hbam92bG3mOtLf8uFCx6kiAwW2yBWh0+jIHQUl5XOo/8/riOKEcCx5z+Pk1W+DbMA+y6eTpt/30RK+W5W7XEM+PzfGFD47HfICu8iipH3xoP02LeQKCm4T2ewYnMRx297gfDlfyPlk0e8Epe+2PxBn78MSnZ4tze8T+StXxHNHkJK12Ng2SyvKyyRrqX170O7ntCpn3e/cBu8+wc4+25Iy4IN70NlsTdvzVuQv9y7vXAGjL2z8XVvXwpF27xgrSz1asVg+BWNf6ymsm0pPDIWLnnYu9bx/Mdg4PnQvmdi689/HLK6w7Hne/e3LoInLoCrX4Yew5urajlc0QhEwxBKa7GnPHKD/hCFggHOHtyNswdXDYN81n7zNxacwOwP3+SjshzKSSM7K419ZZXcu+/r9Nn5PvMrenNU8UruKH6aV6OncnpwCT3/cSXlLsR6141jFv+BsAswI+NKvlX2DDusEwvST+G8vf8G4KnOt/KtnfczfNvfKXZpBJ+/hhQqWEdPeq95m5/+5e98nTfYd8xlZPUcRL8d8wj1OYVQp16kbnyPQGZH7KihkJJxaBtg3TwAohZi04s/p0/pJn646yL6tj+RWyLPe11Ip9x44MfYswmevhSyj4Wp73hHR33wZy/oOvWHU26CVa9BMA0i5VTO+z0pwE7rRPsFTxM6+XteKz/rqIafY/On8NHDMPEB77WufdubHin3vjj+fbv3RZLeviYoG2vhs143YPbA2GNXer86ug+rf/kPH/JOzqv6opr7G++X0MJnvNcy53avK/Cq5735znnLp7ev+1ilu+HVO731jhnvbcPcJ6CiED57OrGgd877dXj0VyH7mEa//EYr3un9yh1wTs201W9Ax37er9rDVV4ET4yHk6/3zo0B7wpz8x/3GiAtGKwNikbgyYne5/DaN1rsl9yRuTM2yZxzFJaH2by7lN1r5zPynWvYMOQmMnOOo9fLV1A56BJSJj8Oz10FQy6Fzl+Bx87CZXXHblsO//4BlSltWdjhbEa8OomiQBbPfuX33LDqu5SQQSalhF2AIjLoYMVUuiB7aEu27QUg7AKscT2JBNNpG6ggJVpGBhVYIMCGtAEQjZDpivms2yQyKnYxbOcrRJyxJOt0BlUuJ1iYx+ZoZ84ILqHC0rl32MtMn1/Aiyk/Y0DqToJn/pC0vid7dWd0qHnhpXtwoXTC//4hKQufAqDykkdJGXIR7o/HYmV7KM/qw87vfEj3J09hR0Z/orvW0a18A6UulV/xXX5jD9Y8XtfBcPH/Qc8T6m7kv02GNW943Utn/jc8OZHI7o3Y3k3sTelKx4otRLJ6Eqwo9P7DdT3WC6I2net/0xY/7/1iuvghb5m1b3tfVp0HwA0feDvm/zEVljwPV/4dvnK29yuk62AvhIvy4f7jIFwG330bMHh0nNciL9oO/c6o/hLlWy95O/vf+qUXxDd84B0evGgm/Od+7+iv9r28s7oBvv0y5JwIfzwGyvZCZme4bbnX3df9+JpfTbs3wO710P9M7/6Kl73PWPYg+N67XlfiB3+GLQvh8qe8X27zH4Ov/tT7solUwqzrIb0dXHBf/SG1Z5P3S6322FHOwVMTve7Jb7zobZ/P58DMK7wv9xs+9Lo0V86B1a/BV3/uDTW+aKb3f6DqV2Ik7H051tdQmXcPzPutN/TJ9xd5y/z1Qlj/ntdt2m0wvHGXd/LkyG952/FA9m2BWd+DU26GY86DTfO9rr+0tt78LZ95v8qGX3XwsbK+eNf7ct69Ad6I7d+6YiakZHqNg+FXetv1MBxoZ6yCvjWIhCEY+3G19EXoNXr/n+/OwaNfhQHnwrgf7b/u6je9D3SfU+Gh0bBzLeUTp1G26m3Ce/JY1PNKum2bR0bpNpZmXwguQteiz8kuWklFZSVF0RRcKJOiaArhilKOjawhbCmEqKR3ZBMAK0KDCAQCDKzwrr/7fqdJDBo0hM7/+SUc9zWY9BjrdxTz7Esvc86GexkV8HbKlgazmHv6TMrTu5Cz6AFGbJ3JbrLo4AqZEfkqJwZWkWUlfJQxlq+V/Z2nw2fzzdCbvBg5nUnB9/hx5bWMSNnA13iT4p6nk3f+Eyx54vtsKM2ga6f2XFI2G5fRgc2Xv0aXbe/RcdciQmmZMOhCeGg0FcE2RKNhftDhz/xp9408FTmXobaOkwOfszbanW9X/oiXM39BakqI6NFn0XbZDBhzq9d9FC6HRTOgZBccNxn38KlYRTF72w1k9cm/ZugndxAo3UlqxR42Db2R1KDRbdGDuFAG4ayehHufSsaipyjvMoR9Y35Cl50L4P37qEhpx+5QV9IiRaRGivlVu7u4Z/d/AfBxx4kMLs0lmN6W9CueJvDIGV7Lb9jXvdbop08RTWuPVRQR6dCPktIyUit2sbT9meztMZazlt3Ba20v4byif7I9ayjdCmNHkA28wPtl89jZ3j6iyx6FIZfBw6fgindgpbvYk/NV2m3/CAuXY2ZeEO9YDbvWekE76XF46WZvmwCM/RHk5Xot5m6D4Zxfws418Py3YOgkmPhnWPZPb19TRZH3Zfz+/+JSMolkdiV69i9Iffn/EUnvSHDvBsLDriRUsMzrfgLofUrNL7Gs7nDJQ9D7VO/LdcdKuPB+77UUrIQux3hfFrOux3Xsg+Uvp2DM3WR/5QR48iJcWjsizvtScqltCYZSsT3rsYkPePt7gqnQY4T3fBs/8r5cj78CPnjA+5IIZcAx53r7ozoPgNP/C1bMrjnBcuS3YMB53rYa8U3vi3v1GzB4orefbctn8Ph53nsJ7OxxJim71hAMGJnl+Vikwrt2xonXwsk3QFbcRZMaQUF/pChY6fU/N0X/bDQKa970Whm9R3v3X7rJ+49+5fPePo5pZ8CVz0HfMdWrfbZxN7NefwvbuZbbSh6gwLUnhTC9LZ95GWfRO2UvPcrX8eKJM2hfuonxC28hNVrC1pQ+fHTePzn/rfNIK81nTdsT2HD2XzjT5RJ66XoY91M484eUVUZ44j/r+deiLfQpeJuHQ/exNNqXoYH1hF2AkEUpJJN0V87XK37GzLRfk4p3NNJzx/6J0zPW0+Oz/6XgpDt5InApC3PfZ1rlT2lLGUs4muNtDQsZSF820wFvSIwS0jEX5e7wt/lF6K+km/d4V1f8kCuDb3NucAEAL0dG81LkVB5NvQ+Af0VGM8zW0SeQT6UL8nr0BP4TPY7fpDzOZrK5K+UHFGeP5A8F3yOncj2XB/+XjNJtPJZyL1ELEiDK3MCpnBt9F4CHwhN5NHw+b6fdTkcr4r7KyfRP3c149x77XCYRAtzY+TH+uuvbtKeQ6ZEJpLXpwBVlMykPZpEWKWRjqB854Q1sDOTQP7qBqZW3cVngfcYH5/N2ZDi/iV7N5DYLub7ir0QJ8GHGmYwpnUuhZZHlCnk6/Ur6lX3OaXxKKel8mjGa4yoWk2aVBIkSSWlLWul2KgNppETL2R3qinOOTpECltkx/KZ8Es+keoc0byWbSWU/486UZ5kY/JCd1onZ2VNJCYX4xpb/AeD9HtcyeM/bdCr5gvJOA0nbtZLCjByySvMAKAp2oG1kDwARgkx09/GT6F84PrCWimAbMLih8gfMsJ+xj0wuqvgfClwHZmT+kZHR+EOpA7jMLgSK86sPqwb4sP8tDNj0Al0qt/B3zmGs+4Rs20txqCPbBn6TDKukx9KHa/7bpHXAKouwaBgs6J2Rv2M1EQJ80v9mile+xY/3XMJZwU/5bcrjrHB9ebffbVwSeZWuea8RSW1H6PaVkFLffsIDU9BL04iEvT7WnBMT6luMrH6LwDOTiGT1IHzJI6QffVrdhcoLvZZStyFeq2r7Mu+Inl4nefNLdnmtxIv+BJ2P3n/VyjAVT15GVt47LP/K93i72zXkbJzFxE2/54ujJpAxZTo9ipZ5XRjlRV6rc+8m73DYSY9BWy+E1i6bz+rNBcwv78NZmx7g6ML5bM4YyIKOE2hfmc8lm//Ix31vpGTkdzkmsxi34QOKS0uwYVcQKttB+qrZ5Lc/nm2ZAyipiDJ62S8oDLTjze7X06dDCl9ZM53eG//BJyc9QL/BJ3L0rncI9j+zpltrxb8gbz6c80t2F1ewat7fOGH+7bzZ9iLe6XEdP/riGja3H8kbx/6Kru0y6LX+BU5e8Vu2XDWP3m3C8Nw3qeg0kPDJN5A58Kvw6dPs2raeJ4JfY2HeXoZu/Qd3hKfx18Ak5na5gpvKHqVN5U7Wpw1k7dBbGdjJ6FaymuWhIWzcXcqmnYWcv/GPrAp+hf9kjeeWoj+RFt7Hv2wsm7t9laOzKhm741neyhzPkpLOVO5az5/K7ybLSriw/DdMCH7CmOAy/h66kBXpI+iQEubsirfJ73YGPfsN5Phdr7K5JMg7jGRITme6BkvosO4lZpSdwoaiECUVEU4rfIVyF+KVwBkQLuPO0LNcE3qN31VOYXpkPF9L/ZAV9GNt6GhGdHF0KFpDSSRIp4GnclqnfQxY8RCl+Wv5Z9pFVB57MZem5hLo1Jf1aQPI31fO/FWbyFr/GvmuPelUcFzgC7qzi00um6cj5/CN4JukWpj/DU9icFY5IzoU43qMoGugkPKty5m+sSvl0SAA5wc+opgMdrh2/HfoOfJcNk9FzuGKtA8YHVhO5+guri2/jcXuaPp1acP3zujPucd2pvCTp3m8YBAvriiluCJCH9vGKRl53PPzuw7pv6eCXpJn+zKvP/kw+x8bVLbXO4u5x4iaafkrvOes6ks9XPFday1l31Zv9NVA0OtGqr0jsaIYUtsk/nh7N3tndTfTzr/8XTvZWrCbomAHenfKJKdjhtcFdIgqI1GizpEWClJaEWFtQRHrNubRqUs3BnRrS9estMN6fICSijArthayeU8pe0oqqkc/MQMDOrdN4+R+nejctu5O3L0llazbUcS+sjA9O6Szu6SSjTtLKA9HKQ9HKC4Ps3VvGUXlYQwY1L0dpx7dhaE929Wpu6wywqcbd1NWGSE1GOS0AV0O6fUo6EVEfE5nxoqIHMEU9CIiPqegFxHxOQW9iIjPKehFRHxOQS8i4nMKehERn1PQi4j4XKs8YcrMCoBDvRRQF2BHE5bTVFRX47XW2lRX46iuxjuU2vo457Lrm9Eqg/5wmFluQ2eHJZPqarzWWpvqahzV1XhNXZu6bkREfE5BLyLic34M+keSXUADVFfjtdbaVFfjqK7Ga9LafNdHLyIi+/Nji15EROIo6EVEfM43QW9m481spZmtMbM7k1hHLzOba2YrzGyZmX0/Nv1uM9tsZgtj/85PUn3rzWxJrIbc2LROZvaGma2O/e3YwjUNjNsuC81sn5ndmoxtZmbTzSzfzJbGTWtw+5jZj2KfuZVmdl4SavuDmX1uZovNbJaZdYhN72tmpXHbbloL19Xge9dS26yBup6Lq2m9mS2MTW/J7dVQRjTf58w596X/BwSBtUB/IBVYBAxOUi3dgZGx21nAKmAwcDdweyvYVuuBLrWm/R64M3b7TuB3SX4vtwF9krHNgDOAkcDSg22f2Pu6CEgD+sU+g8EWru1cIBS7/bu42vrGL5eEbVbve9eS26y+umrN/yPw8yRsr4Yyotk+Z35p0Z8ErHHOrXPOVQAzgYuTUYhzbqtz7tPY7UJgBdAzGbU0wsXAk7HbTwKXJK8UzgLWOucO9czow+KcexfYVWtyQ9vnYmCmc67cOfcFsAbvs9hitTnnXnfOhWN3PwJymuv5G1PXAbTYNjtQXeZduPVy4NnmeO4DOUBGNNvnzC9B3xPYFHc/j1YQrmbWFxgBfBybdHPsJ/b0lu4eieOA181sgZlNjU3r5pzbCt6HEOiapNoAprD/f77WsM0a2j6t7XP3HeCVuPv9zOwzM3vHzE5PQj31vXetZZudDmx3zq2Om9bi26tWRjTb58wvQV/f5eCTetyombUFXgRudc7tAx4GjgaGA1vxfjYmwxjn3EhgAnCTmZ2RpDrqMLNUYCLw99ik1rLNGtJqPndm9hMgDDwTm7QV6O2cGwH8AJhhZu1asKSG3rvWss2uYP8GRYtvr3oyosFF65nWqG3ml6DPA3rF3c8BtiSpFswsBe8NfMY59w8A59x251zEORcFHqUZf+IfiHNuS+xvPjArVsd2M+seq707kJ+M2vC+fD51zm2P1dgqthkNb59W8bkzs28DFwJXuVinbuxn/s7Y7QV4/brHtFRNB3jvkr7NzCwEXAY8VzWtpbdXfRlBM37O/BL084EBZtYv1iqcAsxORiGxvr/HgRXOufvipnePW+xSYGntdVugtjZmllV1G29H3lK8bfXt2GLfBl5q6dpi9mtltYZtFtPQ9pkNTDGzNDPrBwwAPmnJwsxsPHAHMNE5VxI3PdvMgrHb/WO1rWvBuhp675K+zYCzgc+dc3lVE1pyezWUETTn56wl9jK30J7s8/H2Xq8FfpLEOk7D+1m1GFgY+3c+8DSwJDZ9NtA9CbX1x9t7vwhYVrWdgM7AW8Dq2N9OSagtE9gJtI+b1uLbDO+LZitQideSuvZA2wf4SewztxKYkITa1uD131Z91qbFlp0Ue48XAZ8CF7VwXQ2+dy21zeqrKzb9r8D1tZZtye3VUEY02+dMQyCIiPicX7puRESkAQp6ERGfU9CLiPicgl5ExOcU9CIiPqegFxHxOQW9iIjP/X900DZw0DAXogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 100, Quantile: 0.8, Accuracy: 0.01881629042327404\n",
      "--- 3528.6272099018097 seconds ---\n",
      "--- 3528.6282069683075 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from numpy import concatenate\n",
    "quantiles = [0.2, 0.4, 0.6, 0.8]\n",
    "acts = list()\n",
    "preds = list()\n",
    "df_accuracies = DataFrame()\n",
    "histories = list()\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "for quantile in quantiles:\n",
    "    model = build_model_one_layer()\n",
    "    model, history = fit_model(quantile, train_X, train_y, val_X, val_y, verbose = 1)\n",
    "    score = history.history.get('val_loss')[-1]\n",
    "    histories.append(history)\n",
    "\n",
    "    # Plot train and validation error\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='validation')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "    print('Epochs: {}, Quantile: {}, Accuracy: {}'.format(i, quantile, score))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7630fd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02392"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100\n",
    "(0.02389+ 0.02395)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0be89f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020053"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 200: \n",
    "(0.020327+ 0.019779)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13399d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0199818"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 500: \n",
    "(0.02068+ 0.0192836)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44ba8eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0217585"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1000: \n",
    "(0.02314+ 0.020377)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d9d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 200!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c32857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d0e16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 14s 914ms/step - loss: 0.0557 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 14s 921ms/step - loss: 0.0540 - val_loss: 0.0492\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 13s 890ms/step - loss: 0.0575 - val_loss: 0.0436\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 13s 895ms/step - loss: 0.0472 - val_loss: 0.0516\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 13s 900ms/step - loss: 0.0382 - val_loss: 0.0467\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 13s 897ms/step - loss: 0.0339 - val_loss: 0.0422\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 14s 958ms/step - loss: 0.0318 - val_loss: 0.0351\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 14s 941ms/step - loss: 0.0297 - val_loss: 0.0323\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 14s 930ms/step - loss: 0.0290 - val_loss: 0.0362\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 14s 962ms/step - loss: 0.0327 - val_loss: 0.0379\n",
      "Batch size: 723, Quantile: 0.3, Accuracy: 0.03789162635803223\n",
      "--- 174604.01819729805 seconds ---\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 14s 949ms/step - loss: 0.0877 - val_loss: 0.0988\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 13s 897ms/step - loss: 0.0687 - val_loss: 0.0546\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 13s 893ms/step - loss: 0.0779 - val_loss: 0.0716\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 14s 955ms/step - loss: 0.0600 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 14s 958ms/step - loss: 0.0499 - val_loss: 0.0656\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 14s 953ms/step - loss: 0.0445 - val_loss: 0.0563\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 14s 932ms/step - loss: 0.0382 - val_loss: 0.0423\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 14s 905ms/step - loss: 0.0341 - val_loss: 0.0451\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 14s 904ms/step - loss: 0.0335 - val_loss: 0.0449\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 14s 937ms/step - loss: 0.0361 - val_loss: 0.0379\n",
      "Batch size: 723, Quantile: 0.6, Accuracy: 0.03793132305145264\n",
      "--- 174754.35718917847 seconds ---\n"
     ]
    }
   ],
   "source": [
    "quantiles = [0.3, 0.6]\n",
    "acts = list()\n",
    "preds = list()\n",
    "df_accuracies = DataFrame()\n",
    "histories = list()\n",
    "bs_grid = [723]\n",
    "\n",
    "for bs in bs_grid:\n",
    "    for quantile in quantiles:\n",
    "        model = build_model()\n",
    "        model, history = fit_model(quantile, train_X, train_y, val_X, val_y,epoch = 10, batch_size = bs, verbose = 1)\n",
    "        score = history.history.get('val_loss')[-1]\n",
    "        histories.append(history)\n",
    "        print('Batch size: {}, Quantile: {}, Accuracy: {}'.format(bs, quantile, score))\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb8c4887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.087084525"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs_grid: 1\n",
    "(0.0669266+0.10724245)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a95f1522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09465979999999999"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs_grid: 2\n",
    "(0.066937+0.1223826)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3287a033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0939546475"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs_grid:4\n",
    "(0.0585146+0.129394695)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e46fdfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07253861799836159"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs_grid:8\n",
    "(0.06369557976722717+0.081381656229496)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "feb2e800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04387648589909077"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs_grid:241\n",
    "(0.041346460580825806+0.04640651121735573)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83d526f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030573565512895584"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs_grid:723\n",
    "(0.030573565512895584+0.030573565512895584)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "032a3c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03581627085804939"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.0351857990026474+0.036446742713451385)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "636fb85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03978128917515278"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bs2169\n",
    "(0.036737628281116486+0.04282495006918907)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1cd40e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04546285793185234"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs 800\n",
    "(0.03623247519135475+0.05469324067234993)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1ed1a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03489692136645317"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bs = 700\n",
    "(0.03185953572392464+0.037934307008981705)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> select bs = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "822b0cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 0.0823 - val_loss: 0.0564\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0685 - val_loss: 0.0712\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0501 - val_loss: 0.0444\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0531 - val_loss: 0.0507\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0380 - val_loss: 0.0390\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0379 - val_loss: 0.0422\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0355 - val_loss: 0.0385\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0329 - val_loss: 0.0348\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0307 - val_loss: 0.0338\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0295 - val_loss: 0.0346\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0289 - val_loss: 0.0348\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0284 - val_loss: 0.0336\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0278 - val_loss: 0.0320\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0273 - val_loss: 0.0309\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0269 - val_loss: 0.0301\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0265 - val_loss: 0.0295\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0263 - val_loss: 0.0290\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0260 - val_loss: 0.0286\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0257 - val_loss: 0.0283\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0255 - val_loss: 0.0280\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0253 - val_loss: 0.0278\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0251 - val_loss: 0.0275\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0249 - val_loss: 0.0273\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0247 - val_loss: 0.0271\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0245 - val_loss: 0.0274\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0245 - val_loss: 0.0276\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0245 - val_loss: 0.0268\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0241 - val_loss: 0.0265\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0238 - val_loss: 0.0271\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0239 - val_loss: 0.0279\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0239 - val_loss: 0.0283\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0239 - val_loss: 0.0290\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0240 - val_loss: 0.0320\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0252 - val_loss: 0.0349\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0301 - val_loss: 0.0319\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0344 - val_loss: 0.0372\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0306 - val_loss: 0.0525\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0286 - val_loss: 0.0305\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0257 - val_loss: 0.0272\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0245 - val_loss: 0.0259\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0236 - val_loss: 0.0258\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0230 - val_loss: 0.0268\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0229 - val_loss: 0.0267\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0227 - val_loss: 0.0264\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0226 - val_loss: 0.0270\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0224 - val_loss: 0.0256\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0222 - val_loss: 0.0262\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0221 - val_loss: 0.0254\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0220 - val_loss: 0.0255\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0218 - val_loss: 0.0246\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0217 - val_loss: 0.0244\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0216 - val_loss: 0.0242\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0215 - val_loss: 0.0239\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0214 - val_loss: 0.0237\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0213 - val_loss: 0.0235\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0212 - val_loss: 0.0234\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0211 - val_loss: 0.0233\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0211 - val_loss: 0.0230\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0210 - val_loss: 0.0229\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0209 - val_loss: 0.0227\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0207 - val_loss: 0.0225\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0207 - val_loss: 0.0225\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0207 - val_loss: 0.0225\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0209 - val_loss: 0.0227\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0205 - val_loss: 0.0224\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0202 - val_loss: 0.0225\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0201 - val_loss: 0.0246\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0211 - val_loss: 0.0252\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0215 - val_loss: 0.0265\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0214 - val_loss: 0.0279\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0239 - val_loss: 0.0276\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0268 - val_loss: 0.0235\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0250 - val_loss: 0.0298\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0228 - val_loss: 0.0352\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0216 - val_loss: 0.0282\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0206 - val_loss: 0.0264\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0203 - val_loss: 0.0224\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0199 - val_loss: 0.0242\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0195 - val_loss: 0.0221\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0192 - val_loss: 0.0205\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0193 - val_loss: 0.0211\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0194 - val_loss: 0.0206\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0190 - val_loss: 0.0213\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0194 - val_loss: 0.0234\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0211 - val_loss: 0.0221\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0209 - val_loss: 0.0220\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0217 - val_loss: 0.0219\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0205 - val_loss: 0.0220\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0196 - val_loss: 0.0210\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0187 - val_loss: 0.0197\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0186 - val_loss: 0.0201\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0186 - val_loss: 0.0204\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0188 - val_loss: 0.0215\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0198 - val_loss: 0.0222\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0210 - val_loss: 0.0221\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0222 - val_loss: 0.0212\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0206 - val_loss: 0.0238\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0192 - val_loss: 0.0220\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0184 - val_loss: 0.0207\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0184 - val_loss: 0.0198\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0182 - val_loss: 0.0200\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.0180 - val_loss: 0.0191\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.0179 - val_loss: 0.0192\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.0181 - val_loss: 0.0199\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0185 - val_loss: 0.0206\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0193 - val_loss: 0.0209\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0203 - val_loss: 0.0203\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0205 - val_loss: 0.0200\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.0193 - val_loss: 0.0205\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.0183 - val_loss: 0.0200\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0180 - val_loss: 0.0195\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0178 - val_loss: 0.0191\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0176 - val_loss: 0.0187\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0176 - val_loss: 0.0186\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0177 - val_loss: 0.0189\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0178 - val_loss: 0.0192\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0181 - val_loss: 0.0197\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.0186 - val_loss: 0.0196\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0191 - val_loss: 0.0196\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0193 - val_loss: 0.0192\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0186 - val_loss: 0.0191\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0180 - val_loss: 0.0190\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0177 - val_loss: 0.0188\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0174 - val_loss: 0.0187\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0174 - val_loss: 0.0185\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0173 - val_loss: 0.0184\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0174 - val_loss: 0.0185\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0173 - val_loss: 0.0186\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0175 - val_loss: 0.0189\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0178 - val_loss: 0.0189\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0179 - val_loss: 0.0192\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0184 - val_loss: 0.0193\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0187 - val_loss: 0.0188\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0180 - val_loss: 0.0189\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0177 - val_loss: 0.0186\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0176 - val_loss: 0.0191\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0174 - val_loss: 0.0184\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0170 - val_loss: 0.0183\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0172 - val_loss: 0.0185\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0172 - val_loss: 0.0185\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0170 - val_loss: 0.0184\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0171 - val_loss: 0.0181\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0172 - val_loss: 0.0187\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0173 - val_loss: 0.0185\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0171 - val_loss: 0.0187\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0176 - val_loss: 0.0187\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0178 - val_loss: 0.0187\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0175 - val_loss: 0.0187\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0177 - val_loss: 0.0187\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0176 - val_loss: 0.0184\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0174 - val_loss: 0.0187\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0176 - val_loss: 0.0186\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0171 - val_loss: 0.0184\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0169 - val_loss: 0.0184\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0170 - val_loss: 0.0187\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0169 - val_loss: 0.0182\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0166 - val_loss: 0.0182\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0170 - val_loss: 0.0186\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0168 - val_loss: 0.0183\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0165 - val_loss: 0.0182\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0168 - val_loss: 0.0184\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0168 - val_loss: 0.0182\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0165 - val_loss: 0.0181\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0166 - val_loss: 0.0184\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.0170 - val_loss: 0.0183\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.0167 - val_loss: 0.0181\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0166 - val_loss: 0.0186\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0174 - val_loss: 0.0188\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0171 - val_loss: 0.0182\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0168 - val_loss: 0.0190\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.0177 - val_loss: 0.0187\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.0170 - val_loss: 0.0183\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0169 - val_loss: 0.0188\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0175 - val_loss: 0.0183\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0169 - val_loss: 0.0182\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0168 - val_loss: 0.0184\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0172 - val_loss: 0.0184\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0167 - val_loss: 0.0179\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0167 - val_loss: 0.0184\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0170 - val_loss: 0.0187\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0166 - val_loss: 0.0179\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0164 - val_loss: 0.0183\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0169 - val_loss: 0.0190\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0165 - val_loss: 0.0180\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0162 - val_loss: 0.0183\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0168 - val_loss: 0.0192\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0166 - val_loss: 0.0184\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0162 - val_loss: 0.0183\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0166 - val_loss: 0.0186\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0165 - val_loss: 0.0185\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0161 - val_loss: 0.0181A: 0s - loss\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0164 - val_loss: 0.0184\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0164 - val_loss: 0.0185\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0161 - val_loss: 0.0181\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0161 - val_loss: 0.0181\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0163 - val_loss: 0.0184\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0161 - val_loss: 0.0179\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0159 - val_loss: 0.0180\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0163 - val_loss: 0.0182\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0162 - val_loss: 0.0180\n",
      "Neurons: 24, Quantile: 0.3, Accuracy: 0.017987430095672607\n",
      "--- 348.86760544776917 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.1233 - val_loss: 0.0812\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0915 - val_loss: 0.0621\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0595 - val_loss: 0.0581\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0537 - val_loss: 0.0605\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0489 - val_loss: 0.0535\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0436 - val_loss: 0.0470\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.0387 - val_loss: 0.0418\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.0351 - val_loss: 0.0393\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0332 - val_loss: 0.0414\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0328 - val_loss: 0.0421\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0334 - val_loss: 0.0396\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0354 - val_loss: 0.0339\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0380 - val_loss: 0.0417: 0s - loss: 0.\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0367 - val_loss: 0.0492\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0327 - val_loss: 0.0389 ETA: 0s - loss: 0.0\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0300 - val_loss: 0.0331\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0281 - val_loss: 0.0318\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0272 - val_loss: 0.0311\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0267 - val_loss: 0.0303\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0265 - val_loss: 0.0299\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0264 - val_loss: 0.0301\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0266 - val_loss: 0.0303\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0271 - val_loss: 0.0302\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0279 - val_loss: 0.0292\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0288 - val_loss: 0.0289\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0288 - val_loss: 0.0304\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0275 - val_loss: 0.0312\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0263 - val_loss: 0.0306\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0255 - val_loss: 0.0295\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0250 - val_loss: 0.0287\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.0248 - val_loss: 0.0282\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.0248 - val_loss: 0.0280\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0250 - val_loss: 0.0280\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0253 - val_loss: 0.0281 0s - loss: 0.0 - ETA: 0s - loss: 0.02\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0257 - val_loss: 0.0279\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0260 - val_loss: 0.0279\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0263 - val_loss: 0.0279\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0261 - val_loss: 0.0280\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0256 - val_loss: 0.0279\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.0252 - val_loss: 0.0276\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.0248 - val_loss: 0.0273\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.0248 - val_loss: 0.0271\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0247 - val_loss: 0.0272\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0247 - val_loss: 0.0268\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0247 - val_loss: 0.0266\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0249 - val_loss: 0.0264\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0249 - val_loss: 0.0265\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0251 - val_loss: 0.0265\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0247 - val_loss: 0.0266\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.0244 - val_loss: 0.0262\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0241 - val_loss: 0.0259\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0240 - val_loss: 0.0263 ETA: 0s - loss: 0.0\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0240 - val_loss: 0.0257\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0238 - val_loss: 0.0254\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0239 - val_loss: 0.0252\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.0237 - val_loss: 0.0259\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.0238 - val_loss: 0.0251\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0237 - val_loss: 0.0250\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0238 - val_loss: 0.0248\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0236 - val_loss: 0.0262\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0236 - val_loss: 0.0251\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0235 - val_loss: 0.0245ETA: 0s - loss: 0. - ETA: 0s - loss: 0.02\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0233 - val_loss: 0.0249\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0231 - val_loss: 0.0247\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0228 - val_loss: 0.0243\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0229 - val_loss: 0.0242\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0230 - val_loss: 0.0244\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0229 - val_loss: 0.0242\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0228 - val_loss: 0.0239\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0229 - val_loss: 0.0237\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0227 - val_loss: 0.0244\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0228 - val_loss: 0.0236\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0224 - val_loss: 0.0238\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0226 - val_loss: 0.0251\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0226 - val_loss: 0.0244\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0226 - val_loss: 0.0237loss: 0.0\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0222 - val_loss: 0.0246\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0232 - val_loss: 0.0232\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0225 - val_loss: 0.0238\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0223 - val_loss: 0.0231: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0222 - val_loss: 0.0236\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0218 - val_loss: 0.0230\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0220 - val_loss: 0.0237\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0219 - val_loss: 0.0230\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0220 - val_loss: 0.0238\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0221 - val_loss: 0.0228\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0221 - val_loss: 0.0233ETA: 0s - los\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0218 - val_loss: 0.0226 ETA: 0s - loss: 0.02 - ETA: 0s - loss: 0\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0219 - val_loss: 0.0231\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0215 - val_loss: 0.0229\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0219 - val_loss: 0.0231\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0218 - val_loss: 0.0229\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0213 - val_loss: 0.0225\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0219 - val_loss: 0.0229\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0218 - val_loss: 0.0227\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0212 - val_loss: 0.0224\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0217 - val_loss: 0.0226 0s - loss: 0.02 - ETA: 0s - loss:\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0211 - val_loss: 0.0225\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0217 - val_loss: 0.0227\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0211 - val_loss: 0.0227\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0217 - val_loss: 0.0226\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0210 - val_loss: 0.0226\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0217 - val_loss: 0.0227\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0210 - val_loss: 0.0227\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0217 - val_loss: 0.0228\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0211 - val_loss: 0.0227\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0214 - val_loss: 0.0222\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0207 - val_loss: 0.0252\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0215 - val_loss: 0.0225\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0214 - val_loss: 0.0225\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0213 - val_loss: 0.0215\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0209 - val_loss: 0.0223\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0211 - val_loss: 0.0212\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0210 - val_loss: 0.0221\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0215 - val_loss: 0.0212\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0209 - val_loss: 0.0214\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0209 - val_loss: 0.0209\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0207 - val_loss: 0.0214\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0208 - val_loss: 0.0207\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0206 - val_loss: 0.0213\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0207 - val_loss: 0.0206\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0204 - val_loss: 0.0212\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0206 - val_loss: 0.0205ETA: 0s - los\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0203 - val_loss: 0.0210\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0204 - val_loss: 0.0204\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0201 - val_loss: 0.0209\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0203 - val_loss: 0.0204\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0201 - val_loss: 0.0209\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0203 - val_loss: 0.0203\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0201 - val_loss: 0.0208\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0202 - val_loss: 0.0203\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0200 - val_loss: 0.0208\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0201 - val_loss: 0.0202\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0200 - val_loss: 0.0207\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0201 - val_loss: 0.0201\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0199 - val_loss: 0.0207\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0200 - val_loss: 0.0201\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0198 - val_loss: 0.0206\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 0.0199 - val_loss: 0.0200\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 0.0197 - val_loss: 0.0205\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0198 - val_loss: 0.0200\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0196 - val_loss: 0.0205\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0197 - val_loss: 0.0199\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0196 - val_loss: 0.0204\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0197 - val_loss: 0.0198\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0195 - val_loss: 0.0204 ETA: 0s - loss: 0.019 - ETA: 0s - loss\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0196 - val_loss: 0.0198\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0194 - val_loss: 0.0203\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0195 - val_loss: 0.0197\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0194 - val_loss: 0.0202\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0195 - val_loss: 0.0197\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0193 - val_loss: 0.0202\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0194 - val_loss: 0.0197A: 0s - loss: 0.01\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0192 - val_loss: 0.0201\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0194 - val_loss: 0.0196\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0192 - val_loss: 0.0200\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0194 - val_loss: 0.0196\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0191 - val_loss: 0.0200\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0193 - val_loss: 0.0196\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0190 - val_loss: 0.0200\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0193 - val_loss: 0.0197\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0190 - val_loss: 0.0199\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0193 - val_loss: 0.0197\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0189 - val_loss: 0.0198\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0193 - val_loss: 0.0198\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0189 - val_loss: 0.0198\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0192 - val_loss: 0.0197\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0188 - val_loss: 0.0198\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0192 - val_loss: 0.0198\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0188 - val_loss: 0.0197A: 0s - loss: 0.\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0191 - val_loss: 0.0197\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0188 - val_loss: 0.0198\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0190 - val_loss: 0.0196\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0190 - val_loss: 0.0198\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0188 - val_loss: 0.0196\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0189 - val_loss: 0.0197\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0187 - val_loss: 0.0197\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0190 - val_loss: 0.0197\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0186 - val_loss: 0.0195 0s - loss:\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0187 - val_loss: 0.0196\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0188 - val_loss: 0.0196\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0185 - val_loss: 0.0195\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0187 - val_loss: 0.0197\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0186 - val_loss: 0.0196\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.0186 - val_loss: 0.0195\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.0184 - val_loss: 0.0195\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.0187 - val_loss: 0.0197\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0184 - val_loss: 0.0195\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0185 - val_loss: 0.0195\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0183 - val_loss: 0.0196\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0186 - val_loss: 0.0197\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0184 - val_loss: 0.0194\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0184 - val_loss: 0.0196\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0184 - val_loss: 0.0197\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0185 - val_loss: 0.0195\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0183 - val_loss: 0.0195\n",
      "Neurons: 24, Quantile: 0.6, Accuracy: 0.01954585686326027\n",
      "--- 719.8840327262878 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 3s 206ms/step - loss: 0.0758 - val_loss: 0.0575\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0688 - val_loss: 0.0766\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0590 - val_loss: 0.0479\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0595 - val_loss: 0.0613\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0488 - val_loss: 0.0416\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0488 - val_loss: 0.0405\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0364 - val_loss: 0.0434\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0336 - val_loss: 0.0386\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0318 - val_loss: 0.0366\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0307 - val_loss: 0.0358\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0297 - val_loss: 0.0331\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0286 - val_loss: 0.0315\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0280 - val_loss: 0.0306\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0275 - val_loss: 0.0300\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0270 - val_loss: 0.0297\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0267 - val_loss: 0.0289\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0264 - val_loss: 0.0290\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0261 - val_loss: 0.0283\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0255 - val_loss: 0.0276\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0252 - val_loss: 0.0278\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0253 - val_loss: 0.0275\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0249 - val_loss: 0.0268\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0243 - val_loss: 0.0263\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0241 - val_loss: 0.0264\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0241 - val_loss: 0.0262\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0240 - val_loss: 0.0259\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0236 - val_loss: 0.0255\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.0232 - val_loss: 0.0252\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.0229 - val_loss: 0.0253\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0229 - val_loss: 0.0256\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.0231 - val_loss: 0.0255\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0232 - val_loss: 0.0249\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0228 - val_loss: 0.0247\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0223 - val_loss: 0.0243\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0220 - val_loss: 0.0244\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0220 - val_loss: 0.0247\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0222 - val_loss: 0.0243\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0221 - val_loss: 0.0245\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0222 - val_loss: 0.0240\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0217 - val_loss: 0.0242\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0216 - val_loss: 0.0236\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0213 - val_loss: 0.0243\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0216 - val_loss: 0.0242\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0214 - val_loss: 0.0246\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0216 - val_loss: 0.0241\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0214 - val_loss: 0.0245\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0215 - val_loss: 0.0231\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0209 - val_loss: 0.0240\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0209 - val_loss: 0.0238\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0210 - val_loss: 0.0254\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0213 - val_loss: 0.0229\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0207 - val_loss: 0.0250\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0210 - val_loss: 0.0240\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0215 - val_loss: 0.0242\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0213 - val_loss: 0.0239\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0211 - val_loss: 0.0233\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0204 - val_loss: 0.0238\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0203 - val_loss: 0.0227\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0202 - val_loss: 0.0257\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0206 - val_loss: 0.0240\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0201 - val_loss: 0.0257\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0205 - val_loss: 0.0295\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0221 - val_loss: 0.0339\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0249 - val_loss: 0.0502\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0371 - val_loss: 0.0342\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0379 - val_loss: 0.0331\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0321 - val_loss: 0.0389\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0283 - val_loss: 0.0297\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0247 - val_loss: 0.0266\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0230 - val_loss: 0.0256\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0218 - val_loss: 0.0278\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0209 - val_loss: 0.0243\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0201 - val_loss: 0.0222\n",
      "Epoch 74/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0196 - val_loss: 0.0213\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0195 - val_loss: 0.0212\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0197 - val_loss: 0.0212\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0196 - val_loss: 0.0213\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0195 - val_loss: 0.0242\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0214 - val_loss: 0.0229\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0216 - val_loss: 0.0233\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0225 - val_loss: 0.0236\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0227 - val_loss: 0.0221\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0213 - val_loss: 0.0233\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0201 - val_loss: 0.0213\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0194 - val_loss: 0.0207\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0189 - val_loss: 0.0198\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0186 - val_loss: 0.0197\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0186 - val_loss: 0.0201\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0187 - val_loss: 0.0211\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0195 - val_loss: 0.0225\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0213 - val_loss: 0.0231\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0242 - val_loss: 0.0214\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0214 - val_loss: 0.0245\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0195 - val_loss: 0.0220\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0186 - val_loss: 0.0199\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0180 - val_loss: 0.0193\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0182 - val_loss: 0.0189\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0177 - val_loss: 0.0195\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0185 - val_loss: 0.0183\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0180 - val_loss: 0.0207\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0197 - val_loss: 0.0190\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0185 - val_loss: 0.0212\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0203 - val_loss: 0.0199\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0195 - val_loss: 0.0190\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0190 - val_loss: 0.0193\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0190 - val_loss: 0.0189\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0190 - val_loss: 0.0190\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0185 - val_loss: 0.0186\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0182 - val_loss: 0.0190\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0182 - val_loss: 0.0191\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0188 - val_loss: 0.0185\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0179 - val_loss: 0.0187\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0183 - val_loss: 0.0184\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0178 - val_loss: 0.0186\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0181 - val_loss: 0.0182\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0178 - val_loss: 0.0185\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0181 - val_loss: 0.0184\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0181 - val_loss: 0.0185\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0182 - val_loss: 0.0186\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0184 - val_loss: 0.0182\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0182 - val_loss: 0.0184\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0180 - val_loss: 0.0181\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0179 - val_loss: 0.0182\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0177 - val_loss: 0.0179\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0174 - val_loss: 0.0183\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0176 - val_loss: 0.0178\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0174 - val_loss: 0.0182\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0176 - val_loss: 0.0179\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0172 - val_loss: 0.0184\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0176 - val_loss: 0.0177\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0174 - val_loss: 0.0181\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0175 - val_loss: 0.0178\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0171 - val_loss: 0.0182\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0175 - val_loss: 0.0179\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0174 - val_loss: 0.0179\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0174 - val_loss: 0.0180\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0173 - val_loss: 0.0181\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0177 - val_loss: 0.0179\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0173 - val_loss: 0.0180\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0175 - val_loss: 0.0179\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0172 - val_loss: 0.0181\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0176 - val_loss: 0.0183\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0171 - val_loss: 0.0180\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0171 - val_loss: 0.0180\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0172 - val_loss: 0.0181\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0173 - val_loss: 0.0181\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0169 - val_loss: 0.0182\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0170 - val_loss: 0.0178\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0170 - val_loss: 0.0179\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0169 - val_loss: 0.0176\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0166 - val_loss: 0.0182\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0171 - val_loss: 0.0176\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0168 - val_loss: 0.0179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0170 - val_loss: 0.0177\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0168 - val_loss: 0.0182\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0172 - val_loss: 0.0179\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0171 - val_loss: 0.0184\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0175 - val_loss: 0.0180\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0175 - val_loss: 0.0187\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0179 - val_loss: 0.0180\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0174 - val_loss: 0.0186\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0172 - val_loss: 0.0178\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0170 - val_loss: 0.0179\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0168 - val_loss: 0.0180\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0165 - val_loss: 0.0178\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0167 - val_loss: 0.0179\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0166 - val_loss: 0.0181\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0165 - val_loss: 0.0179\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0164 - val_loss: 0.0179\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0165 - val_loss: 0.0180\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0165 - val_loss: 0.0179\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0164 - val_loss: 0.0177\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0163 - val_loss: 0.0178\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0165 - val_loss: 0.0177\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0165 - val_loss: 0.0177\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0163 - val_loss: 0.0177\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0164 - val_loss: 0.0177\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0165 - val_loss: 0.0179\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0166 - val_loss: 0.0182\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0168 - val_loss: 0.0185\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0171 - val_loss: 0.0186\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 0.0172 - val_loss: 0.0189\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0181 - val_loss: 0.0188\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0181 - val_loss: 0.0187\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0173 - val_loss: 0.0179\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.0176 - val_loss: 0.0191\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.0168 - val_loss: 0.0182\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0165 - val_loss: 0.0184\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 0.0166 - val_loss: 0.0184\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0163 - val_loss: 0.0185\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0163 - val_loss: 0.0184\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 3s 157ms/step - loss: 0.0164 - val_loss: 0.0190\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0164 - val_loss: 0.0181\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0162 - val_loss: 0.0188\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0163 - val_loss: 0.0188\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0164 - val_loss: 0.0189\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 0.0163 - val_loss: 0.0183\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0161 - val_loss: 0.0190\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0161 - val_loss: 0.0182\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0162 - val_loss: 0.0190\n",
      "Neurons: 47, Quantile: 0.3, Accuracy: 0.01900758035480976\n",
      "--- 1274.8523292541504 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 3s 203ms/step - loss: 0.0883 - val_loss: 0.0938\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0918 - val_loss: 0.0639\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0707 - val_loss: 0.0586\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0573 - val_loss: 0.0715\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0509 - val_loss: 0.0560\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0426 - val_loss: 0.0474\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0382 - val_loss: 0.0431\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0355 - val_loss: 0.0389\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0327 - val_loss: 0.0439\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0323 - val_loss: 0.0448\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0330 - val_loss: 0.0419\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0346 - val_loss: 0.0385\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0397 - val_loss: 0.0379\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0430 - val_loss: 0.0588\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0371 - val_loss: 0.0506\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0330 - val_loss: 0.0368\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0298 - val_loss: 0.0342\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0286 - val_loss: 0.0325\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0279 - val_loss: 0.0316\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0277 - val_loss: 0.0317\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 3s 189ms/step - loss: 0.0281 - val_loss: 0.0322\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0292 - val_loss: 0.0315\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0307 - val_loss: 0.0304\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0314 - val_loss: 0.0322\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0296 - val_loss: 0.0324\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0279 - val_loss: 0.0311\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0266 - val_loss: 0.0295\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0259 - val_loss: 0.0287\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0257 - val_loss: 0.0287\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0259 - val_loss: 0.0289\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0264 - val_loss: 0.0288\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0271 - val_loss: 0.0285\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0277 - val_loss: 0.0287\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0273 - val_loss: 0.0291\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0266 - val_loss: 0.0293\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0258 - val_loss: 0.0288\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0253 - val_loss: 0.0283\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0249 - val_loss: 0.0277\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0249 - val_loss: 0.0276\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0249 - val_loss: 0.0277\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0251 - val_loss: 0.0299\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0245 - val_loss: 0.0275\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0252 - val_loss: 0.0266\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0252 - val_loss: 0.0267\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0256 - val_loss: 0.0259\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0249 - val_loss: 0.0278\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0263 - val_loss: 0.0278\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0248 - val_loss: 0.0257\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0234 - val_loss: 0.0255\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0236 - val_loss: 0.0240\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0226 - val_loss: 0.0259\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0241 - val_loss: 0.0252\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0243 - val_loss: 0.0245\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0239 - val_loss: 0.0249\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0237 - val_loss: 0.0241\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0226 - val_loss: 0.0234\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0225 - val_loss: 0.0239\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0231 - val_loss: 0.0229\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0224 - val_loss: 0.0228\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0229 - val_loss: 0.0233\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0219 - val_loss: 0.0225\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0222 - val_loss: 0.0228\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0214 - val_loss: 0.0222\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0216 - val_loss: 0.0221\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0213 - val_loss: 0.0234\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0220 - val_loss: 0.0223\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0215 - val_loss: 0.0222\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0216 - val_loss: 0.0217\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0211 - val_loss: 0.0218\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0212 - val_loss: 0.0220\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0212 - val_loss: 0.0217\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0204 - val_loss: 0.0218\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0216 - val_loss: 0.0216\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0204 - val_loss: 0.0210\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0208 - val_loss: 0.0214\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0203 - val_loss: 0.0214\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0210 - val_loss: 0.0212\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0201 - val_loss: 0.0210\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0207 - val_loss: 0.0214\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0202 - val_loss: 0.0211\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0199 - val_loss: 0.0210\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0206 - val_loss: 0.0212\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0198 - val_loss: 0.0207\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0204 - val_loss: 0.0210\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0196 - val_loss: 0.0208\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0204 - val_loss: 0.0210\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0195 - val_loss: 0.0206\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0202 - val_loss: 0.0208\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0194 - val_loss: 0.0205\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0202 - val_loss: 0.0206\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0193 - val_loss: 0.0203\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0200 - val_loss: 0.0207\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0194 - val_loss: 0.0204\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0202 - val_loss: 0.0207\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0192 - val_loss: 0.0204\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0200 - val_loss: 0.0206\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0193 - val_loss: 0.0204\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0199 - val_loss: 0.0208\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0191 - val_loss: 0.0203\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0197 - val_loss: 0.0204\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0191 - val_loss: 0.0202\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0197 - val_loss: 0.0207\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0191 - val_loss: 0.0202\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0196 - val_loss: 0.0204\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0190 - val_loss: 0.0202\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0195 - val_loss: 0.0205\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0190 - val_loss: 0.0199\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 3s 165ms/step - loss: 0.0191 - val_loss: 0.0202\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0188 - val_loss: 0.0200\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0193 - val_loss: 0.0203\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0186 - val_loss: 0.0199\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0190 - val_loss: 0.0207\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0192 - val_loss: 0.0200\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0186 - val_loss: 0.0198\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0185 - val_loss: 0.0205\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0196 - val_loss: 0.0209\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0188 - val_loss: 0.0196\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0185 - val_loss: 0.0206\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0190 - val_loss: 0.0204\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0192 - val_loss: 0.0197\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 3s 191ms/step - loss: 0.0182 - val_loss: 0.0200\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 3s 188ms/step - loss: 0.0189 - val_loss: 0.0209\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0190 - val_loss: 0.0201\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0188 - val_loss: 0.0206\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0185 - val_loss: 0.0205\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0198 - val_loss: 0.0210\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0188 - val_loss: 0.0196\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0188 - val_loss: 0.0205\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0187 - val_loss: 0.0202\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 0.0189 - val_loss: 0.0197\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0180 - val_loss: 0.0200\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0189 - val_loss: 0.0208\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0186 - val_loss: 0.0199\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0184 - val_loss: 0.0205\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0184 - val_loss: 0.0202\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0193 - val_loss: 0.0202\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0181 - val_loss: 0.0197\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0183 - val_loss: 0.0205\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0188 - val_loss: 0.0205\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0187 - val_loss: 0.0199\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0178 - val_loss: 0.0202\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0188 - val_loss: 0.0210\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0189 - val_loss: 0.0200\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0180 - val_loss: 0.0199\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0183 - val_loss: 0.0207\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0190 - val_loss: 0.0208\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0181 - val_loss: 0.0197\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0178 - val_loss: 0.0204\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0191 - val_loss: 0.0216\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0184 - val_loss: 0.0199\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0180 - val_loss: 0.0205\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0186 - val_loss: 0.0200\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0184 - val_loss: 0.0198\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0176 - val_loss: 0.0200\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0184 - val_loss: 0.0206\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0183 - val_loss: 0.0198\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0175 - val_loss: 0.0201\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0183 - val_loss: 0.0206\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0187 - val_loss: 0.0204\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0177 - val_loss: 0.0200\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0182 - val_loss: 0.0203\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0183 - val_loss: 0.0201\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0177 - val_loss: 0.0200\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0178 - val_loss: 0.0201\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0178 - val_loss: 0.0203\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0182 - val_loss: 0.0207\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0174 - val_loss: 0.0199\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0179 - val_loss: 0.0203\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0176 - val_loss: 0.0205\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0173 - val_loss: 0.0202\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0178 - val_loss: 0.0208\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0175 - val_loss: 0.0204\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0175 - val_loss: 0.0212\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0174 - val_loss: 0.0202\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0178 - val_loss: 0.0212\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0175 - val_loss: 0.0210\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0176 - val_loss: 0.0203\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0173 - val_loss: 0.0207\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0177 - val_loss: 0.0219\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0173 - val_loss: 0.0210\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0177 - val_loss: 0.0209\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0172 - val_loss: 0.0208\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0175 - val_loss: 0.0213\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0171 - val_loss: 0.0212\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0178 - val_loss: 0.0218\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0173 - val_loss: 0.0213\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0175 - val_loss: 0.0217\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0173 - val_loss: 0.0212\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0175 - val_loss: 0.0213\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 0.0173 - val_loss: 0.0224\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0176 - val_loss: 0.0222\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0172 - val_loss: 0.0219\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0174 - val_loss: 0.0227\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0174 - val_loss: 0.0238\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0184 - val_loss: 0.0248\n",
      "Neurons: 47, Quantile: 0.6, Accuracy: 0.02484528347849846\n",
      "--- 1856.6281204223633 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0594 - val_loss: 0.0685\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.0634 - val_loss: 0.0448\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.0526 - val_loss: 0.0433\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0385 - val_loss: 0.0420\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.0342 - val_loss: 0.0435\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.0322 - val_loss: 0.0363\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.0301 - val_loss: 0.0331\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.0284 - val_loss: 0.0309\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0274 - val_loss: 0.0323\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0273 - val_loss: 0.0314\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0268 - val_loss: 0.0312\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.0260 - val_loss: 0.0322\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0261 - val_loss: 0.0353\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0273 - val_loss: 0.0399\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.0363 - val_loss: 0.0327\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0370 - val_loss: 0.0347\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.0310 - val_loss: 0.0502\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.0295 - val_loss: 0.0374\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 0.0265 - val_loss: 0.0338\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0253 - val_loss: 0.0309\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0246 - val_loss: 0.0293\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0241 - val_loss: 0.0297\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.0239 - val_loss: 0.0281\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0234 - val_loss: 0.0277\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0231 - val_loss: 0.0263\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0227 - val_loss: 0.0255\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0224 - val_loss: 0.0243\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.0220 - val_loss: 0.0239\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.0218 - val_loss: 0.0238\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 0.0216 - val_loss: 0.0244\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 0.0214 - val_loss: 0.0259\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0217 - val_loss: 0.0284\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0234 - val_loss: 0.0298\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.0257 - val_loss: 0.0283\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.0258 - val_loss: 0.0273\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0249 - val_loss: 0.0310\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0289 - val_loss: 0.0260\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.0258 - val_loss: 0.0272\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.0232 - val_loss: 0.0262\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.0224 - val_loss: 0.0264\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0220 - val_loss: 0.0255\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0215 - val_loss: 0.0247\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0211 - val_loss: 0.0243\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0208 - val_loss: 0.0236\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.0205 - val_loss: 0.0225\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 0.0201 - val_loss: 0.0217\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0198 - val_loss: 0.0214\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0197 - val_loss: 0.0223\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0200 - val_loss: 0.0250\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0221 - val_loss: 0.0256\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.0244 - val_loss: 0.0243\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.0238 - val_loss: 0.0239\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0224 - val_loss: 0.0221\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.0210 - val_loss: 0.0229\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0203 - val_loss: 0.0230\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0198 - val_loss: 0.0213\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0193 - val_loss: 0.0206\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0187 - val_loss: 0.0206\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.0185 - val_loss: 0.0196\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.0185 - val_loss: 0.0215\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.0194 - val_loss: 0.0246\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0226 - val_loss: 0.0236\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.0245 - val_loss: 0.0241\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0219 - val_loss: 0.0211\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0200 - val_loss: 0.0213\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0190 - val_loss: 0.0196\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0182 - val_loss: 0.0192\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0182 - val_loss: 0.0193\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0181 - val_loss: 0.0199\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 276ms/step - loss: 0.0184 - val_loss: 0.0198\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0192 - val_loss: 0.0219\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0210 - val_loss: 0.0220\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0218 - val_loss: 0.0198\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0201 - val_loss: 0.0228\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0193 - val_loss: 0.0216\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0184 - val_loss: 0.0191\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0178 - val_loss: 0.0184\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0176 - val_loss: 0.0189\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0174 - val_loss: 0.0184\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0177 - val_loss: 0.0198\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0190 - val_loss: 0.0192\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0188 - val_loss: 0.0199\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0198 - val_loss: 0.0213\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0196 - val_loss: 0.0185\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0184 - val_loss: 0.0185\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0179 - val_loss: 0.0189\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0180 - val_loss: 0.0189\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0176 - val_loss: 0.0179\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0174 - val_loss: 0.0184\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0173 - val_loss: 0.0184\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0174 - val_loss: 0.0186\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0177 - val_loss: 0.0183\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0178 - val_loss: 0.0195\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0183 - val_loss: 0.0193\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0187 - val_loss: 0.0204\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0198 - val_loss: 0.0187\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0184 - val_loss: 0.0191\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0186 - val_loss: 0.0192\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0177 - val_loss: 0.0192\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0176 - val_loss: 0.0200\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0172 - val_loss: 0.0187\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0170 - val_loss: 0.0192\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0169 - val_loss: 0.0185\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0169 - val_loss: 0.0197\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0167 - val_loss: 0.0181\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0165 - val_loss: 0.0181\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0168 - val_loss: 0.0185\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0169 - val_loss: 0.0191\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0167 - val_loss: 0.0179\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0164 - val_loss: 0.0182\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0170 - val_loss: 0.0186\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0173 - val_loss: 0.0194\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0171 - val_loss: 0.0181\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0170 - val_loss: 0.0198\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0181 - val_loss: 0.0191\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0175 - val_loss: 0.0194\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0183 - val_loss: 0.0196\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0178 - val_loss: 0.0185\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0173 - val_loss: 0.0185\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0170 - val_loss: 0.0182\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0168 - val_loss: 0.0187\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0168 - val_loss: 0.0185\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0169 - val_loss: 0.0190\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0163 - val_loss: 0.0186\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0165 - val_loss: 0.0189\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0168 - val_loss: 0.0191\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0167 - val_loss: 0.0194\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0162 - val_loss: 0.0197\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0165 - val_loss: 0.0199\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0165 - val_loss: 0.0198\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0165 - val_loss: 0.0191\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0163 - val_loss: 0.0221\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0166 - val_loss: 0.0200\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0162 - val_loss: 0.0205\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0165 - val_loss: 0.0182\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0165 - val_loss: 0.0218\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0167 - val_loss: 0.0202\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0162 - val_loss: 0.0197\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0164 - val_loss: 0.0186\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0167 - val_loss: 0.0215\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0163 - val_loss: 0.0196\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0162 - val_loss: 0.0189\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0163 - val_loss: 0.0186\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0164 - val_loss: 0.0208\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0163 - val_loss: 0.0204\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0163 - val_loss: 0.0180\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0160 - val_loss: 0.0189\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0161 - val_loss: 0.0204\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0159 - val_loss: 0.0194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0161 - val_loss: 0.0187\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0160 - val_loss: 0.0185\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0164 - val_loss: 0.0201\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0162 - val_loss: 0.0193\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0160 - val_loss: 0.0191\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0155 - val_loss: 0.0186\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0155 - val_loss: 0.0190\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0157 - val_loss: 0.0185\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0159 - val_loss: 0.0197\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0157 - val_loss: 0.0190\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0154 - val_loss: 0.0187\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0155 - val_loss: 0.0184\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0157 - val_loss: 0.0195\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0159 - val_loss: 0.0204\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0155 - val_loss: 0.0186\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0153 - val_loss: 0.0183\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0160 - val_loss: 0.0192\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0155 - val_loss: 0.0191\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0154 - val_loss: 0.0184\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0153 - val_loss: 0.0186\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0155 - val_loss: 0.0187\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 4s 279ms/step - loss: 0.0155 - val_loss: 0.0194\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0156 - val_loss: 0.0209\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0157 - val_loss: 0.0187\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0153 - val_loss: 0.0195\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0156 - val_loss: 0.0198\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0155 - val_loss: 0.0193\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0152 - val_loss: 0.0187\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0150 - val_loss: 0.0190\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0153 - val_loss: 0.0195\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0154 - val_loss: 0.0189\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0152 - val_loss: 0.0188\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0150 - val_loss: 0.0190\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0151 - val_loss: 0.0191\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0148 - val_loss: 0.0195\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0148 - val_loss: 0.0194\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0150 - val_loss: 0.0187\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0147 - val_loss: 0.0190\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0152 - val_loss: 0.0189\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 4s 276ms/step - loss: 0.0156 - val_loss: 0.0192\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0154 - val_loss: 0.0187\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0154 - val_loss: 0.0198\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0156 - val_loss: 0.0200\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0151 - val_loss: 0.0190\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0155 - val_loss: 0.0180\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0149 - val_loss: 0.0196\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0152 - val_loss: 0.0192\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0150 - val_loss: 0.0196\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0150 - val_loss: 0.0195\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0148 - val_loss: 0.0199\n",
      "Neurons: 72, Quantile: 0.3, Accuracy: 0.01989971660077572\n",
      "--- 2711.195243358612 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 5s 297ms/step - loss: 0.1078 - val_loss: 0.1070\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0963 - val_loss: 0.0717\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0739 - val_loss: 0.0577\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0543 - val_loss: 0.0789\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0481 - val_loss: 0.0538\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0417 - val_loss: 0.0507\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0379 - val_loss: 0.0396\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0349 - val_loss: 0.0453\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0344 - val_loss: 0.0468\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0362 - val_loss: 0.0406\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0414 - val_loss: 0.0401\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0454 - val_loss: 0.0633\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0387 - val_loss: 0.0472\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0339 - val_loss: 0.0420\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 4s 278ms/step - loss: 0.0313 - val_loss: 0.0351\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0295 - val_loss: 0.0332\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0288 - val_loss: 0.0325\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 4s 275ms/step - loss: 0.0287 - val_loss: 0.0331\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0293 - val_loss: 0.0332\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0309 - val_loss: 0.0318\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0331 - val_loss: 0.0339\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0330 - val_loss: 0.0392\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0309 - val_loss: 0.0385\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0288 - val_loss: 0.0344\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0275 - val_loss: 0.0317\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0266 - val_loss: 0.0298\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0263 - val_loss: 0.0293\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0262 - val_loss: 0.0292\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0265 - val_loss: 0.0293\n",
      "Epoch 30/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0271 - val_loss: 0.0291\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0280 - val_loss: 0.0286\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 276ms/step - loss: 0.0285 - val_loss: 0.0293\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 0.0281 - val_loss: 0.0308\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0271 - val_loss: 0.0311\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0261 - val_loss: 0.0297\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0254 - val_loss: 0.0285\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0249 - val_loss: 0.0276\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0247 - val_loss: 0.0272\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0247 - val_loss: 0.0270\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0250 - val_loss: 0.0270\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0254 - val_loss: 0.0267\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0260 - val_loss: 0.0268\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 276ms/step - loss: 0.0262 - val_loss: 0.0274\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0260 - val_loss: 0.0276\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0250 - val_loss: 0.0268\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0245 - val_loss: 0.0273\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0242 - val_loss: 0.0261\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0242 - val_loss: 0.0257\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0234 - val_loss: 0.0246\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0234 - val_loss: 0.0252\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0238 - val_loss: 0.0246\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0236 - val_loss: 0.0244\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0237 - val_loss: 0.0245\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0234 - val_loss: 0.0248\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0235 - val_loss: 0.0243\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0229 - val_loss: 0.0237\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0225 - val_loss: 0.0233\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0223 - val_loss: 0.0228\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0220 - val_loss: 0.0231\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0223 - val_loss: 0.0240\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0228 - val_loss: 0.0236\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0227 - val_loss: 0.0232\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0222 - val_loss: 0.0235\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0226 - val_loss: 0.0236\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0222 - val_loss: 0.0226\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0213 - val_loss: 0.0222\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 4s 281ms/step - loss: 0.0215 - val_loss: 0.0224\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0216 - val_loss: 0.0214\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0210 - val_loss: 0.0224\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0220 - val_loss: 0.0223\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0211 - val_loss: 0.0217\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0203 - val_loss: 0.0212\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0201 - val_loss: 0.0211\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0208 - val_loss: 0.0215\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0200 - val_loss: 0.0211\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0207 - val_loss: 0.0212\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0200 - val_loss: 0.0209\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0204 - val_loss: 0.0208\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0199 - val_loss: 0.0207\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0202 - val_loss: 0.0204\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0200 - val_loss: 0.0209\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0202 - val_loss: 0.0205\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 0.0197 - val_loss: 0.0210\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0203 - val_loss: 0.0209\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0196 - val_loss: 0.0208\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0204 - val_loss: 0.0213\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0198 - val_loss: 0.0208\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0203 - val_loss: 0.0209\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0195 - val_loss: 0.0205\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 5s 282ms/step - loss: 0.0200 - val_loss: 0.0211\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0193 - val_loss: 0.0207\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0197 - val_loss: 0.0209\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0190 - val_loss: 0.0204\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0197 - val_loss: 0.0207\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0192 - val_loss: 0.0202\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0191 - val_loss: 0.0203\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0192 - val_loss: 0.0203\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0192 - val_loss: 0.0202\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0192 - val_loss: 0.0203\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0188 - val_loss: 0.0202\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0193 - val_loss: 0.0209\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0192 - val_loss: 0.0204\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0194 - val_loss: 0.0207\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0187 - val_loss: 0.0207\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0197 - val_loss: 0.0208\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0194 - val_loss: 0.0213\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 275ms/step - loss: 0.0196 - val_loss: 0.0206\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 5s 283ms/step - loss: 0.0191 - val_loss: 0.0205\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0194 - val_loss: 0.0205\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0192 - val_loss: 0.0198\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0188 - val_loss: 0.0203\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0188 - val_loss: 0.0213\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0193 - val_loss: 0.0207\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0186 - val_loss: 0.0205\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0187 - val_loss: 0.0218\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0195 - val_loss: 0.0207\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0186 - val_loss: 0.0199\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0186 - val_loss: 0.0212\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0189 - val_loss: 0.0199\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0186 - val_loss: 0.0202\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0183 - val_loss: 0.0203\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0184 - val_loss: 0.0199\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0183 - val_loss: 0.0200\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0185 - val_loss: 0.0204\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0184 - val_loss: 0.0198\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0183 - val_loss: 0.0206\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0186 - val_loss: 0.0204\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0183 - val_loss: 0.0199\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 4s 275ms/step - loss: 0.0179 - val_loss: 0.0203\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0187 - val_loss: 0.0216\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0186 - val_loss: 0.0202\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0178 - val_loss: 0.0200\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0183 - val_loss: 0.0212\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0191 - val_loss: 0.0212\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0178 - val_loss: 0.0197\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0178 - val_loss: 0.0201\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0182 - val_loss: 0.0212\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0186 - val_loss: 0.0207\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0179 - val_loss: 0.0208\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0183 - val_loss: 0.0199\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0184 - val_loss: 0.0208\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0177 - val_loss: 0.0199\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0178 - val_loss: 0.0204\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0182 - val_loss: 0.0208\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0178 - val_loss: 0.0204\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0176 - val_loss: 0.0212\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0184 - val_loss: 0.0214\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0184 - val_loss: 0.0212\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0176 - val_loss: 0.0206\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0182 - val_loss: 0.0202\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0181 - val_loss: 0.0200\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0175 - val_loss: 0.0203\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0174 - val_loss: 0.0200\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 5s 283ms/step - loss: 0.0184 - val_loss: 0.0213\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0179 - val_loss: 0.0206\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0174 - val_loss: 0.0205\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0178 - val_loss: 0.0202\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0183 - val_loss: 0.0205\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0175 - val_loss: 0.0200\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0171 - val_loss: 0.0205\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0182 - val_loss: 0.0212\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0181 - val_loss: 0.0205\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0181 - val_loss: 0.0209\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0177 - val_loss: 0.0205\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 0.0181 - val_loss: 0.0211\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0189 - val_loss: 0.0218\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0179 - val_loss: 0.0201\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0173 - val_loss: 0.0198\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0177 - val_loss: 0.0211\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0174 - val_loss: 0.0205\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0172 - val_loss: 0.0216\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0172 - val_loss: 0.0215\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0175 - val_loss: 0.0208\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0179 - val_loss: 0.0205\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.0171 - val_loss: 0.0206\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 0.0177 - val_loss: 0.0208\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 4s 279ms/step - loss: 0.0175 - val_loss: 0.0217\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 0.0174 - val_loss: 0.0205\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 0.0177 - val_loss: 0.0218\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0174 - val_loss: 0.0207\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0173 - val_loss: 0.0214\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0171 - val_loss: 0.0212\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0171 - val_loss: 0.0212\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0175 - val_loss: 0.0215\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0173 - val_loss: 0.0206\n",
      "Epoch 189/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 4s 274ms/step - loss: 0.0169 - val_loss: 0.0230\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0175 - val_loss: 0.0217\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0173 - val_loss: 0.0204\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0177 - val_loss: 0.0207\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0183 - val_loss: 0.0209\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0173 - val_loss: 0.0203\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 0.0171 - val_loss: 0.0204\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 0.0171 - val_loss: 0.0219\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 0.0171 - val_loss: 0.0208\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0169 - val_loss: 0.0214\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 0.0166 - val_loss: 0.0217\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0168 - val_loss: 0.0222\n",
      "Neurons: 72, Quantile: 0.6, Accuracy: 0.022210771217942238\n",
      "--- 3615.075154066086 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0614 - val_loss: 0.0755\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 9s 536ms/step - loss: 0.0677 - val_loss: 0.0446\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0571 - val_loss: 0.0449\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0402 - val_loss: 0.0436\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0347 - val_loss: 0.0349\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0321 - val_loss: 0.0355\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0314 - val_loss: 0.0383\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0347 - val_loss: 0.0385\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0397 - val_loss: 0.0358\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0350 - val_loss: 0.0544\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0340 - val_loss: 0.0464\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0311 - val_loss: 0.0433\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0297 - val_loss: 0.0392\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0284 - val_loss: 0.0334\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0271 - val_loss: 0.0299\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0264 - val_loss: 0.0286\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 9s 571ms/step - loss: 0.0257 - val_loss: 0.0282\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0252 - val_loss: 0.0281\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0249 - val_loss: 0.0293\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0247 - val_loss: 0.0303\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0244 - val_loss: 0.0317\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0248 - val_loss: 0.0352\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0250 - val_loss: 0.0450\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0289 - val_loss: 0.0428\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0387 - val_loss: 0.0340\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0395 - val_loss: 0.0368\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0318 - val_loss: 0.0480\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0294 - val_loss: 0.0397\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0258 - val_loss: 0.0343\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0245 - val_loss: 0.0291\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0232 - val_loss: 0.0279\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0228 - val_loss: 0.0262\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0223 - val_loss: 0.0249\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 9s 575ms/step - loss: 0.0218 - val_loss: 0.0231\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0212 - val_loss: 0.0236\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0215 - val_loss: 0.0234\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 9s 572ms/step - loss: 0.0211 - val_loss: 0.0247\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0218 - val_loss: 0.0275\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0234 - val_loss: 0.0311\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0271 - val_loss: 0.0287\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0278 - val_loss: 0.0273\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0262 - val_loss: 0.0241\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0232 - val_loss: 0.0255\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0230 - val_loss: 0.0231\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0210 - val_loss: 0.0216\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 9s 589ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0202 - val_loss: 0.0231\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0214 - val_loss: 0.0240\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0234 - val_loss: 0.0244\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0257 - val_loss: 0.0240\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0224 - val_loss: 0.0271\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0210 - val_loss: 0.0248\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0194 - val_loss: 0.0204\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0188 - val_loss: 0.0201\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 9s 581ms/step - loss: 0.0192 - val_loss: 0.0194\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 10s 612ms/step - loss: 0.0187 - val_loss: 0.0191\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0181 - val_loss: 0.0197\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0185 - val_loss: 0.0216\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0194 - val_loss: 0.0228\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0216 - val_loss: 0.0243\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0231 - val_loss: 0.0238\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0224 - val_loss: 0.0205\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0201 - val_loss: 0.0219\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0198 - val_loss: 0.0194\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0186 - val_loss: 0.0190\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0180 - val_loss: 0.0199\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0190 - val_loss: 0.0187\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0181 - val_loss: 0.0202\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0194 - val_loss: 0.0203\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0200 - val_loss: 0.0211\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 9s 578ms/step - loss: 0.0213 - val_loss: 0.0192\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0202 - val_loss: 0.0205\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 9s 579ms/step - loss: 0.0194 - val_loss: 0.0188\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0182 - val_loss: 0.0189\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0182 - val_loss: 0.0184\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0180 - val_loss: 0.0186\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0179 - val_loss: 0.0182\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0178 - val_loss: 0.0190\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0184 - val_loss: 0.0190\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 9s 577ms/step - loss: 0.0185 - val_loss: 0.0191\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 10s 595ms/step - loss: 0.0191 - val_loss: 0.0192\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0189 - val_loss: 0.0190\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 9s 571ms/step - loss: 0.0189 - val_loss: 0.0184\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0181 - val_loss: 0.0188\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0180 - val_loss: 0.0184\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 9s 580ms/step - loss: 0.0180 - val_loss: 0.0186\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0182 - val_loss: 0.0186\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 10s 598ms/step - loss: 0.0176 - val_loss: 0.0186\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 9s 593ms/step - loss: 0.0176 - val_loss: 0.0184\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0177 - val_loss: 0.0183\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 10s 595ms/step - loss: 0.0173 - val_loss: 0.0178\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0169 - val_loss: 0.0185\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0174 - val_loss: 0.0180\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0174 - val_loss: 0.0178\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0172 - val_loss: 0.0182\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 9s 587ms/step - loss: 0.0171 - val_loss: 0.0190\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0180 - val_loss: 0.0186\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0180 - val_loss: 0.0192\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0190 - val_loss: 0.0208\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 9s 571ms/step - loss: 0.0187 - val_loss: 0.0221\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0214 - val_loss: 0.0209\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0202 - val_loss: 0.0206\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0203 - val_loss: 0.0196\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0186 - val_loss: 0.0199\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0175 - val_loss: 0.0200\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0171 - val_loss: 0.0185\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0168 - val_loss: 0.0183\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0167 - val_loss: 0.0181\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0168 - val_loss: 0.0185\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0166 - val_loss: 0.0182\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0165 - val_loss: 0.0177\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0165 - val_loss: 0.0177\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 9s 577ms/step - loss: 0.0166 - val_loss: 0.0177\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 9s 587ms/step - loss: 0.0164 - val_loss: 0.0175\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0164 - val_loss: 0.0180\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0165 - val_loss: 0.0177\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0165 - val_loss: 0.0176\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0165 - val_loss: 0.0183\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0169 - val_loss: 0.0188\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0174 - val_loss: 0.0197\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 9s 593ms/step - loss: 0.0183 - val_loss: 0.0202\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0191 - val_loss: 0.0197\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0191 - val_loss: 0.0196\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0198 - val_loss: 0.0198\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0186 - val_loss: 0.0181\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 9s 580ms/step - loss: 0.0172 - val_loss: 0.0185\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0166 - val_loss: 0.0188\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 9s 581ms/step - loss: 0.0163 - val_loss: 0.0182\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0163 - val_loss: 0.0184\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0165 - val_loss: 0.0189\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0164 - val_loss: 0.0187\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0160 - val_loss: 0.0182\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 9s 571ms/step - loss: 0.0162 - val_loss: 0.0186\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0164 - val_loss: 0.0193\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 9s 575ms/step - loss: 0.0163 - val_loss: 0.0189\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 9s 577ms/step - loss: 0.0163 - val_loss: 0.0191\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0161 - val_loss: 0.0181\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0160 - val_loss: 0.0184\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0162 - val_loss: 0.0191\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0161 - val_loss: 0.0179\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0161 - val_loss: 0.0200\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0162 - val_loss: 0.0186\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0157 - val_loss: 0.0177\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0157 - val_loss: 0.0176\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0159 - val_loss: 0.0177\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 9s 585ms/step - loss: 0.0160 - val_loss: 0.0188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0158 - val_loss: 0.0182\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0157 - val_loss: 0.0179\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0160 - val_loss: 0.0177\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0157 - val_loss: 0.0180\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0160 - val_loss: 0.0194\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0160 - val_loss: 0.0183\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0156 - val_loss: 0.0182\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 9s 575ms/step - loss: 0.0156 - val_loss: 0.0183\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 9s 581ms/step - loss: 0.0162 - val_loss: 0.0181\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0158 - val_loss: 0.0186\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0157 - val_loss: 0.0180\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0157 - val_loss: 0.0177\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0157 - val_loss: 0.0188\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0161 - val_loss: 0.0185\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0158 - val_loss: 0.0184\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0154 - val_loss: 0.0188\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0163 - val_loss: 0.0186\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 9s 544ms/step - loss: 0.0162 - val_loss: 0.0192\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0165 - val_loss: 0.0214\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0186 - val_loss: 0.0243\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0206 - val_loss: 0.0248\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 9s 575ms/step - loss: 0.0210 - val_loss: 0.0279\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0256 - val_loss: 0.0246\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0254 - val_loss: 0.0230\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0216 - val_loss: 0.0231\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0220 - val_loss: 0.0199\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0200 - val_loss: 0.0306\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0187 - val_loss: 0.0250\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0173 - val_loss: 0.0210\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0173 - val_loss: 0.0207\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0171 - val_loss: 0.0194\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0167 - val_loss: 0.0191\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0166 - val_loss: 0.0182\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 9s 578ms/step - loss: 0.0165 - val_loss: 0.0187\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0160 - val_loss: 0.0181\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0158 - val_loss: 0.0179\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0158 - val_loss: 0.0180\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0158 - val_loss: 0.0182\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0156 - val_loss: 0.0182\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 9s 536ms/step - loss: 0.0155 - val_loss: 0.0178\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0155 - val_loss: 0.0185\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 9s 589ms/step - loss: 0.0154 - val_loss: 0.0185\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0153 - val_loss: 0.0181\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 9s 579ms/step - loss: 0.0157 - val_loss: 0.0192\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0155 - val_loss: 0.0183\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0151 - val_loss: 0.0182\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0153 - val_loss: 0.0175\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0153 - val_loss: 0.0186\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 9s 578ms/step - loss: 0.0152 - val_loss: 0.0182\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0150 - val_loss: 0.0182\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0151 - val_loss: 0.0183\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0151 - val_loss: 0.0182\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0149 - val_loss: 0.0180\n",
      "Neurons: 148, Quantile: 0.3, Accuracy: 0.01797252520918846\n",
      "--- 5524.673108100891 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 9s 591ms/step - loss: 0.0894 - val_loss: 0.0843\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 9s 542ms/step - loss: 0.0545 - val_loss: 0.0601\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 0.0553 - val_loss: 0.0498\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 9s 542ms/step - loss: 0.0595 - val_loss: 0.0802\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 8s 531ms/step - loss: 0.0535 - val_loss: 0.0724\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0439 - val_loss: 0.0541\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0375 - val_loss: 0.0392\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0332 - val_loss: 0.0573\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0341 - val_loss: 0.0364\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 9s 547ms/step - loss: 0.0320 - val_loss: 0.0470\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0320 - val_loss: 0.0463\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0318 - val_loss: 0.0486\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0344 - val_loss: 0.0425\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 9s 536ms/step - loss: 0.0382 - val_loss: 0.0356\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0434 - val_loss: 0.0568\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0399 - val_loss: 0.0371\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0394 - val_loss: 0.0540\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0353 - val_loss: 0.0391\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 8s 519ms/step - loss: 0.0314 - val_loss: 0.0364\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0294 - val_loss: 0.0319\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0290 - val_loss: 0.0319\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 9s 585ms/step - loss: 0.0289 - val_loss: 0.0330\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0303 - val_loss: 0.0316\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0317 - val_loss: 0.0315\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0334 - val_loss: 0.0384\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 9s 531ms/step - loss: 0.0318 - val_loss: 0.0363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "16/16 [==============================] - 9s 540ms/step - loss: 0.0297 - val_loss: 0.0354\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 9s 540ms/step - loss: 0.0280 - val_loss: 0.0309\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0271 - val_loss: 0.0298\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0269 - val_loss: 0.0300\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0273 - val_loss: 0.0304\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0281 - val_loss: 0.0297\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0294 - val_loss: 0.0295\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0302 - val_loss: 0.0325\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0294 - val_loss: 0.0332\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0278 - val_loss: 0.0320\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0264 - val_loss: 0.0292\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 8s 530ms/step - loss: 0.0257 - val_loss: 0.0279\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0251 - val_loss: 0.0278\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0251 - val_loss: 0.0280\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0253 - val_loss: 0.0279\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0263 - val_loss: 0.0270\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 9s 541ms/step - loss: 0.0267 - val_loss: 0.0403\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0253 - val_loss: 0.0262\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 8s 526ms/step - loss: 0.0245 - val_loss: 0.0267\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0242 - val_loss: 0.0267\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 9s 542ms/step - loss: 0.0246 - val_loss: 0.0266\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0260 - val_loss: 0.0256\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0261 - val_loss: 0.0261\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0264 - val_loss: 0.0267\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0270 - val_loss: 0.0289\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0249 - val_loss: 0.0252\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 9s 536ms/step - loss: 0.0240 - val_loss: 0.0260\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0245 - val_loss: 0.0254\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0241 - val_loss: 0.0245\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 9s 589ms/step - loss: 0.0234 - val_loss: 0.0244\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0233 - val_loss: 0.0252\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0235 - val_loss: 0.0240\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 9s 541ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 0.0225 - val_loss: 0.0237\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 9s 571ms/step - loss: 0.0220 - val_loss: 0.0246\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0236 - val_loss: 0.0241\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0229 - val_loss: 0.0239\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0234 - val_loss: 0.0237\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 9s 536ms/step - loss: 0.0224 - val_loss: 0.0247\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0232 - val_loss: 0.0229\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0227 - val_loss: 0.0237\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0229 - val_loss: 0.0230\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 9s 540ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0230 - val_loss: 0.0236\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0223 - val_loss: 0.0243\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0219 - val_loss: 0.0225\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 9s 579ms/step - loss: 0.0220 - val_loss: 0.0230\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0219 - val_loss: 0.0228\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 9s 542ms/step - loss: 0.0218 - val_loss: 0.0230\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0221 - val_loss: 0.0224\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 9s 547ms/step - loss: 0.0212 - val_loss: 0.0215\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0206 - val_loss: 0.0216\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0209 - val_loss: 0.0213\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0209 - val_loss: 0.0212\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0213 - val_loss: 0.0212\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 9s 547ms/step - loss: 0.0205 - val_loss: 0.0215\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0206 - val_loss: 0.0207\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 8s 525ms/step - loss: 0.0204 - val_loss: 0.0210\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0208 - val_loss: 0.0208\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 0.0204 - val_loss: 0.0204\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0201 - val_loss: 0.0221\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 9s 586ms/step - loss: 0.0210 - val_loss: 0.0213\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0201 - val_loss: 0.0220\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0199 - val_loss: 0.0207\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0201 - val_loss: 0.0225\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0203 - val_loss: 0.0213\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 10s 603ms/step - loss: 0.0195 - val_loss: 0.0204\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0195 - val_loss: 0.0205\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 9s 542ms/step - loss: 0.0197 - val_loss: 0.0212\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 9s 582ms/step - loss: 0.0194 - val_loss: 0.0203\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 9s 592ms/step - loss: 0.0191 - val_loss: 0.0215\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 10s 615ms/step - loss: 0.0196 - val_loss: 0.0203\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 10s 615ms/step - loss: 0.0195 - val_loss: 0.0215\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 9s 572ms/step - loss: 0.0192 - val_loss: 0.0207\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 9s 571ms/step - loss: 0.0194 - val_loss: 0.0209\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0190 - val_loss: 0.0205\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0198 - val_loss: 0.0213\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0194 - val_loss: 0.0207\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0194 - val_loss: 0.0203\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0186 - val_loss: 0.0202\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0189 - val_loss: 0.0199\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0191 - val_loss: 0.0212\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0187 - val_loss: 0.0209\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0193 - val_loss: 0.0224\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0204 - val_loss: 0.0214\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0188 - val_loss: 0.0204\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 9s 534ms/step - loss: 0.0187 - val_loss: 0.0219\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0206 - val_loss: 0.0209\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 9s 547ms/step - loss: 0.0198 - val_loss: 0.0229\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0194 - val_loss: 0.0210\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0189 - val_loss: 0.0209\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0197 - val_loss: 0.0206\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0192 - val_loss: 0.0209\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0200 - val_loss: 0.0205\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 9s 592ms/step - loss: 0.0200 - val_loss: 0.0219\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 9s 572ms/step - loss: 0.0207 - val_loss: 0.0210\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0200 - val_loss: 0.0209\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 9s 584ms/step - loss: 0.0206 - val_loss: 0.0216\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0197 - val_loss: 0.0207\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0196 - val_loss: 0.0205\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0196 - val_loss: 0.0220\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0198 - val_loss: 0.0207\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0190 - val_loss: 0.0205\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0187 - val_loss: 0.0205\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0187 - val_loss: 0.0201\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0184 - val_loss: 0.0198\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0185 - val_loss: 0.0205\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0180 - val_loss: 0.0201\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0180 - val_loss: 0.0202\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0185 - val_loss: 0.0220\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0190 - val_loss: 0.0217\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0182 - val_loss: 0.0208\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0186 - val_loss: 0.0203\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0182 - val_loss: 0.0203\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0176 - val_loss: 0.0198\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 0.0178 - val_loss: 0.0212\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0188 - val_loss: 0.0216\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0182 - val_loss: 0.0197\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 9s 572ms/step - loss: 0.0178 - val_loss: 0.0201\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 9s 579ms/step - loss: 0.0180 - val_loss: 0.0210\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0182 - val_loss: 0.0206\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0176 - val_loss: 0.0199\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0180 - val_loss: 0.0205\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0175 - val_loss: 0.0202\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 9s 586ms/step - loss: 0.0175 - val_loss: 0.0205\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 9s 578ms/step - loss: 0.0180 - val_loss: 0.0209\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 10s 596ms/step - loss: 0.0181 - val_loss: 0.0206\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0173 - val_loss: 0.0198\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0173 - val_loss: 0.0209\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0188 - val_loss: 0.0217\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 9s 547ms/step - loss: 0.0183 - val_loss: 0.0205\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0175 - val_loss: 0.0201\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0173 - val_loss: 0.0211\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0179 - val_loss: 0.0207\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0182 - val_loss: 0.0202\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 9s 544ms/step - loss: 0.0174 - val_loss: 0.0200\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0170 - val_loss: 0.0207\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0176 - val_loss: 0.0205\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0180 - val_loss: 0.0201\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0174 - val_loss: 0.0208\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0175 - val_loss: 0.0204\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0173 - val_loss: 0.0202\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0171 - val_loss: 0.0199\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 10s 650ms/step - loss: 0.0175 - val_loss: 0.0221\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 10s 629ms/step - loss: 0.0172 - val_loss: 0.0209\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0171 - val_loss: 0.0202\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0178 - val_loss: 0.0201\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0175 - val_loss: 0.0217\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0177 - val_loss: 0.0206\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0180 - val_loss: 0.0213\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 9s 532ms/step - loss: 0.0178 - val_loss: 0.0207\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 10s 594ms/step - loss: 0.0173 - val_loss: 0.0206\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0173 - val_loss: 0.0219\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 9s 590ms/step - loss: 0.0186 - val_loss: 0.0232\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0183 - val_loss: 0.0208\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0174 - val_loss: 0.0212\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0181 - val_loss: 0.0202\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 10s 607ms/step - loss: 0.0176 - val_loss: 0.0209\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0177 - val_loss: 0.0203\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 10s 596ms/step - loss: 0.0171 - val_loss: 0.0220\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0187 - val_loss: 0.0224\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 10s 601ms/step - loss: 0.0177 - val_loss: 0.0231\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 10s 610ms/step - loss: 0.0174 - val_loss: 0.0204\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 10s 601ms/step - loss: 0.0179 - val_loss: 0.0214\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 10s 638ms/step - loss: 0.0185 - val_loss: 0.0205\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0178 - val_loss: 0.0206\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0171 - val_loss: 0.0202\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 10s 615ms/step - loss: 0.0178 - val_loss: 0.0223\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0181 - val_loss: 0.0213\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 9s 586ms/step - loss: 0.0176 - val_loss: 0.0219\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 9s 575ms/step - loss: 0.0177 - val_loss: 0.0205\n",
      "Neurons: 148, Quantile: 0.6, Accuracy: 0.020537782460451126\n",
      "--- 7419.469901800156 seconds ---\n",
      "--- 7419.469901800156 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "quantiles = [0.3, 0.6]\n",
    "acts = list()\n",
    "preds = list()\n",
    "df_accuracies = DataFrame()\n",
    "model_hist = list()\n",
    "neurons_grid = [24, 47, 72, 148]\n",
    "\n",
    "for neuron in neurons_grid:\n",
    "    for quantile in quantiles:\n",
    "        model = build_model(neurons = neuron)\n",
    "        model, history = fit_model(quantile, train_X, train_y, val_X, val_y, batch_size = 700, verbose = 1)\n",
    "        score = history.history.get('val_loss')[-1]\n",
    "        model_hist.append(history)\n",
    "        print('Neurons: {}, Quantile: {}, Accuracy: {}'.format(neuron, quantile, score))\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04861b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24 neurons wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trining process\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ea08406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 0.1168 - val_loss: 0.1039\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0790 - val_loss: 0.0588\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0644 - val_loss: 0.0603\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0482 - val_loss: 0.0383\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0458 - val_loss: 0.0377\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0328 - val_loss: 0.0324\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0288 - val_loss: 0.0318\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0275 - val_loss: 0.0310\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0269 - val_loss: 0.0300\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0265 - val_loss: 0.0297\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0261 - val_loss: 0.0295\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0259 - val_loss: 0.0294\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0256 - val_loss: 0.0294\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0254 - val_loss: 0.0292\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0253 - val_loss: 0.0291\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.0251 - val_loss: 0.0289\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0249 - val_loss: 0.0286\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0248 - val_loss: 0.0284\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0246 - val_loss: 0.0281\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 0.0245 - val_loss: 0.0277\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.0244 - val_loss: 0.0274\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0242 - val_loss: 0.0270\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0241 - val_loss: 0.0267\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0240 - val_loss: 0.0264TA: 0s - loss: \n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0239 - val_loss: 0.0261\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0238 - val_loss: 0.0259\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0237 - val_loss: 0.0258\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0237 - val_loss: 0.0257\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0236 - val_loss: 0.0259\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0237 - val_loss: 0.0264\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0239 - val_loss: 0.0278\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0250 - val_loss: 0.0300\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0285 - val_loss: 0.0286\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0308 - val_loss: 0.0280\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0266 - val_loss: 0.0375\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0256 - val_loss: 0.0332\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0244 - val_loss: 0.0295ETA: 0s - loss: 0.0\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0237 - val_loss: 0.0277\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0233 - val_loss: 0.0265\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0231 - val_loss: 0.0260\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0228 - val_loss: 0.02560s - loss: \n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0227 - val_loss: 0.0252\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0225 - val_loss: 0.0247\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0224 - val_loss: 0.0243\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0223 - val_loss: 0.0241\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0223 - val_loss: 0.0240: 0s - loss: 0.0\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0223 - val_loss: 0.0242\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0225 - val_loss: 0.0247\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0231 - val_loss: 0.0252\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0239 - val_loss: 0.0253\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0247 - val_loss: 0.0247\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0243 - val_loss: 0.0246\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0231 - val_loss: 0.0251\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0225 - val_loss: 0.0245\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0221 - val_loss: 0.0238\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0219 - val_loss: 0.0235\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0219 - val_loss: 0.0234\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0219 - val_loss: 0.0234\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0220 - val_loss: 0.0236\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0222 - val_loss: 0.0239\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0226 - val_loss: 0.0240\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0231 - val_loss: 0.0237\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0231 - val_loss: 0.0236\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0226 - val_loss: 0.0236\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0221 - val_loss: 0.0235\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0218 - val_loss: 0.0233TA: 0s - loss: 0.0\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0216 - val_loss: 0.0230\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0215 - val_loss: 0.0229\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0216 - val_loss: 0.0229\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0216 - val_loss: 0.0230\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0217 - val_loss: 0.0230\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0219 - val_loss: 0.0230\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0220 - val_loss: 0.0230\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0221 - val_loss: 0.0230\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0221 - val_loss: 0.0229\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0219 - val_loss: 0.0229 ETA: 0s - loss: 0.02\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0217 - val_loss: 0.0228TA: 0s - loss: 0 - ETA: 0s - loss:\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0216 - val_loss: 0.0227\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0215 - val_loss: 0.0226\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0215 - val_loss: 0.0225\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0215 - val_loss: 0.0225ETA: 0s - loss: 0.02\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0215 - val_loss: 0.0225\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0215 - val_loss: 0.0225\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0215 - val_loss: 0.0225 ETA: 0s - loss: - ETA: 0s - loss: 0.02\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0214 - val_loss: 0.0224\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0214 - val_loss: 0.0224\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0213 - val_loss: 0.0224\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.0213 - val_loss: 0.0223\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0212 - val_loss: 0.0223\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0212 - val_loss: 0.0223TA: 0s - loss: 0.02\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0212 - val_loss: 0.0223\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0212 - val_loss: 0.0223TA: 0s - los\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0212 - val_loss: 0.0222\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0211 - val_loss: 0.0222\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0211 - val_loss: 0.0222\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0211 - val_loss: 0.0222\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0211 - val_loss: 0.0222\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0211 - val_loss: 0.0222ETA: 0s - loss: 0.020 - ETA: 0s - lo\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0211 - val_loss: 0.0222TA: 0s - loss: 0.0 - ETA: 0s - loss: 0. - ETA: 0s - loss: 0.0\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0211 - val_loss: 0.0221\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0211 - val_loss: 0.0221: 0s - loss: 0.02\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0210 - val_loss: 0.0221\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0210 - val_loss: 0.0221\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0210 - val_loss: 0.0221\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0210 - val_loss: 0.0221\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0210 - val_loss: 0.0220\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0209 - val_loss: 0.0220\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0209 - val_loss: 0.0220 ETA: 0s - loss: 0 - ETA: 0s - loss: 0\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0209 - val_loss: 0.0220A: 0s - loss: 0.0 - ETA: 0s - loss: 0 - ETA: 0s - loss: 0.0\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0209 - val_loss: 0.0220\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0208 - val_loss: 0.0220\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0208 - val_loss: 0.0219ETA: 0s - loss\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0208 - val_loss: 0.0219 - loss: 0.020 - ETA: 0s - loss: 0.0\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0208 - val_loss: 0.0218TA: 0s - loss: 0\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0208 - val_loss: 0.0218\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0207 - val_loss: 0.0218A: 0s - loss: 0.\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0207 - val_loss: 0.0218\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0207 - val_loss: 0.0218\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0207 - val_loss: 0.0218\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0206 - val_loss: 0.0218\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0206 - val_loss: 0.0218\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0206 - val_loss: 0.0218s - loss: 0.0\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0206 - val_loss: 0.0217 0s - los\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0205 - val_loss: 0.02170s - loss: 0.020\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0206 - val_loss: 0.0217A: 0s - loss: 0.02 - ETA: 0s - loss: 0.02\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0206 - val_loss: 0.0217 0s - loss:\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0206 - val_loss: 0.0217A: 0s - loss: 0.02\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0203 - val_loss: 0.0216TA: 0s - loss: 0.0 - ETA: 0s - loss: 0.\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0203 - val_loss: 0.02160s - loss: 0. - ETA: 0s - loss: 0.02\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0203 - val_loss: 0.0216A: 0s - loss: 0\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0203 - val_loss: 0.0216ETA: 0s - loss: 0.\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0202 - val_loss: 0.0216\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0203 - val_loss: 0.0216 ETA: 0s - loss: 0\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0202 - val_loss: 0.0216\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0202 - val_loss: 0.0216\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0201 - val_loss: 0.0216\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0201 - val_loss: 0.0216\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0201 - val_loss: 0.0217\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0200 - val_loss: 0.0217ETA: 0s - loss: 0.0 - ETA: 0s - loss: 0.0\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0200 - val_loss: 0.0217 ETA: 0s - loss: 0 - ETA: 0s - loss: 0.\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0200 - val_loss: 0.0217 ETA: 0s - loss:  - ETA: 0s - loss: \n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0200 - val_loss: 0.0217\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0199 - val_loss: 0.0217\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0199 - val_loss: 0.0217\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0199 - val_loss: 0.0217\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0199 - val_loss: 0.0216\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0199 - val_loss: 0.0216ETA: 0s - loss:  - ETA: 0s - loss: 0.\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0199 - val_loss: 0.02160s - loss: 0.\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0199 - val_loss: 0.0216A: 0s - loss: 0.0\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0199 - val_loss: 0.0216ETA: 0s - loss: 0.018 - ETA: 0s - \n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0199 - val_loss: 0.0216\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0199 - val_loss: 0.0216\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0200 - val_loss: 0.0217\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0201 - val_loss: 0.0218\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0203 - val_loss: 0.0220oss: 0.02\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0205 - val_loss: 0.0221TA: 0s - loss: 0.\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0207 - val_loss: 0.02200s - loss: 0.02 - ETA: 0s - loss: 0.02\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0208 - val_loss: 0.0219A: 0s - loss: 0 - ETA: 0s - loss: 0.02\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0206 - val_loss: 0.0218\n",
      "Neurons: 24, Quantile: 0.3, Accuracy: 0.0217608455568552\n",
      "--- 376.2106475830078 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 0.2092 - val_loss: 0.1555\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0916 - val_loss: 0.0663\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.0714 - val_loss: 0.0469\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0486 - val_loss: 0.0403\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0405 - val_loss: 0.0455\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0360 - val_loss: 0.0381\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0324 - val_loss: 0.0357\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0303 - val_loss: 0.0334\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0302 - val_loss: 0.0325\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0300 - val_loss: 0.0329\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0294 - val_loss: 0.0323\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0289 - val_loss: 0.0318\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0287 - val_loss: 0.0314\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0286 - val_loss: 0.0311- loss: 0.0 - ETA: 0s - loss: 0.02\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0286 - val_loss: 0.0309\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0288 - val_loss: 0.0308\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0289 - val_loss: 0.0307\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0289 - val_loss: 0.0308\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.0287 - val_loss: 0.0307\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.0284 - val_loss: 0.0306\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0281 - val_loss: 0.0303ETA: 0s - loss - ETA: 0s - loss: 0.02\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0279 - val_loss: 0.0301A: 0s - loss\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0278 - val_loss: 0.0298s - loss: 0\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0278 - val_loss: 0.0297\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0277 - val_loss: 0.0295\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0277 - val_loss: 0.0294\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0277 - val_loss: 0.0293\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0275 - val_loss: 0.0292\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0274 - val_loss: 0.0291\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0272 - val_loss: 0.0289\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0271 - val_loss: 0.0287\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0270 - val_loss: 0.0285\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0269 - val_loss: 0.0283\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0269 - val_loss: 0.0282\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0268 - val_loss: 0.0281\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0268 - val_loss: 0.0280\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0267 - val_loss: 0.0278 0s - loss: 0.02 - ETA: 0s - loss: 0.026\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0266 - val_loss: 0.0277\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0264 - val_loss: 0.0276- loss: 0.026 - ETA: 0s - loss: 0.02\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0263 - val_loss: 0.0274A: 0s - loss: 0\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0262 - val_loss: 0.0273\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0261 - val_loss: 0.0271\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0261 - val_loss: 0.0270\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0260 - val_loss: 0.0270\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.0259 - val_loss: 0.0268\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0258 - val_loss: 0.0267ETA: 0s - loss: \n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0258 - val_loss: 0.0266\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0257 - val_loss: 0.0266TA: 0s - loss: 0. - ETA: 0s - loss: 0.0\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0256 - val_loss: 0.0265\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0255 - val_loss: 0.0265\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0253 - val_loss: 0.0264\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0253 - val_loss: 0.0262\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0253 - val_loss: 0.0262\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0253 - val_loss: 0.0262\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0252 - val_loss: 0.0261\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0251 - val_loss: 0.0263- loss: 0.024 - ETA: 0s - loss: 0.02 - ETA: 0s - loss: 0.02\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0249 - val_loss: 0.0262 ETA: 0s - loss: 0\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0248 - val_loss: 0.0258\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0249 - val_loss: 0.0260\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0250 - val_loss: 0.0259\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0250 - val_loss: 0.0259\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0248 - val_loss: 0.0263\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0246 - val_loss: 0.0258\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0246 - val_loss: 0.0257\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0247 - val_loss: 0.0259\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0247 - val_loss: 0.0256\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0247 - val_loss: 0.0261\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0245 - val_loss: 0.0257\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0244 - val_loss: 0.0255\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0245 - val_loss: 0.0257\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0245 - val_loss: 0.0254\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0245 - val_loss: 0.02590s - loss: 0.02 - ETA: 0s - loss: 0.\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0243 - val_loss: 0.0254\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0243 - val_loss: 0.0256s - loss: 0.\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0244 - val_loss: 0.0254\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0243 - val_loss: 0.0254\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0242 - val_loss: 0.0256\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0241 - val_loss: 0.0251\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.0242 - val_loss: 0.0254\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0242 - val_loss: 0.0251\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0241 - val_loss: 0.0254A: 0s - loss: 0.0 - ETA: 0s - loss: 0.\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0241 - val_loss: 0.0251A: 0s - loss: 0.02 - ETA: 0s - loss: \n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0240 - val_loss: 0.0251\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0240 - val_loss: 0.0251\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0240 - val_loss: 0.0250\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0239 - val_loss: 0.0251\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0239 - val_loss: 0.0249\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0239 - val_loss: 0.0250\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0239 - val_loss: 0.0248\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0239 - val_loss: 0.0249A: 0s - loss: 0.0 - ETA: 0s - loss: 0 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.02\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0238 - val_loss: 0.0248 ETA: 0s - loss: 0.02 - ETA: 0s - loss: 0.0\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0237 - val_loss: 0.0248\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0238 - val_loss: 0.0248TA: 0s - los\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0237 - val_loss: 0.0248\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0237 - val_loss: 0.0248\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0236 - val_loss: 0.0247\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0236 - val_loss: 0.0247loss: 0.0\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0236 - val_loss: 0.0246\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0236 - val_loss: 0.0247\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0235 - val_loss: 0.0246A: 0s - loss: 0.02 - ETA: 0s - loss:\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0235 - val_loss: 0.0246\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0235 - val_loss: 0.0246\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0235 - val_loss: 0.0246\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0235 - val_loss: 0.0246\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0234 - val_loss: 0.0245\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0234 - val_loss: 0.0246 0s - los\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0234 - val_loss: 0.0245\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0234 - val_loss: 0.0245A: 0s - loss:\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0233 - val_loss: 0.0245\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0233 - val_loss: 0.0245\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0233 - val_loss: 0.0244\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0233 - val_loss: 0.0244\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0233 - val_loss: 0.0244\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0232 - val_loss: 0.0244s - loss: 0.\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0232 - val_loss: 0.0244 0s - loss: 0.0\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0232 - val_loss: 0.0244ETA: 0s - loss: 0.023 - ETA: 0s - loss: \n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0232 - val_loss: 0.0244\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0232 - val_loss: 0.0244\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0232 - val_loss: 0.0244\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0230 - val_loss: 0.0242\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0229 - val_loss: 0.02430s - loss: 0.0 - ETA: 0s - loss: 0.0\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0229 - val_loss: 0.0243\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0229 - val_loss: 0.0243\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0229 - val_loss: 0.0243\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0229 - val_loss: 0.0243\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0228 - val_loss: 0.0243\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.0228 - val_loss: 0.0243\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0230 - val_loss: 0.0243\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0228 - val_loss: 0.0243\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0229 - val_loss: 0.0242\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0227 - val_loss: 0.0242\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0229 - val_loss: 0.0242\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0228 - val_loss: 0.0242\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0228 - val_loss: 0.0242\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0227 - val_loss: 0.0242\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.0228 - val_loss: 0.0242\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0227 - val_loss: 0.0242\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0228 - val_loss: 0.0242\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0227 - val_loss: 0.0242\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0227 - val_loss: 0.0242\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0226 - val_loss: 0.0242\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0227 - val_loss: 0.0241\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0226 - val_loss: 0.0242\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0227 - val_loss: 0.0242\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0226 - val_loss: 0.0242\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0226 - val_loss: 0.0241A: 0s - loss: 0.0 - ETA: 0s - loss: \n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0226 - val_loss: 0.0242 ETA: 0s - loss:  - ETA: 0s - loss: 0.0 - ETA: 0s - loss: 0.0\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0226 - val_loss: 0.0241- loss: 0.02\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0226 - val_loss: 0.0242\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0226 - val_loss: 0.0241\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0225 - val_loss: 0.02420s - loss: 0.\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0225 - val_loss: 0.0241A: 0s - loss: 0.\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 180/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0224 - val_loss: 0.0241\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0223 - val_loss: 0.0241\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0223 - val_loss: 0.0241\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 0.0223 - val_loss: 0.0241\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0223 - val_loss: 0.0241\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0223 - val_loss: 0.0241: 0s - loss:\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0223 - val_loss: 0.0241 ETA: 0s - \n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0223 - val_loss: 0.0241\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0222 - val_loss: 0.0241\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0222 - val_loss: 0.0241 ETA: 0s - loss: 0.022\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0222 - val_loss: 0.0241\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0222 - val_loss: 0.0241\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0222 - val_loss: 0.0242\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0222 - val_loss: 0.0241ETA: 0s -\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0222 - val_loss: 0.0242\n",
      "Neurons: 24, Quantile: 0.6, Accuracy: 0.02418028563261032\n",
      "--- 757.896817445755 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 3s 209ms/step - loss: 0.1038 - val_loss: 0.0701\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 3s 188ms/step - loss: 0.0695 - val_loss: 0.0633\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 3s 188ms/step - loss: 0.0546 - val_loss: 0.0418\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 3s 186ms/step - loss: 0.0544 - val_loss: 0.0511\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0401 - val_loss: 0.0328\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0406 - val_loss: 0.0344\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0320 - val_loss: 0.0496\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0300 - val_loss: 0.0364\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0279 - val_loss: 0.0348\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0270 - val_loss: 0.0330\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0264 - val_loss: 0.0322\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0259 - val_loss: 0.0317\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0255 - val_loss: 0.0311\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0252 - val_loss: 0.0305\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0249 - val_loss: 0.0298\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0247 - val_loss: 0.0292\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0244 - val_loss: 0.0285\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0242 - val_loss: 0.0277\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0240 - val_loss: 0.0271\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0238 - val_loss: 0.0264\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0236 - val_loss: 0.0257\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0234 - val_loss: 0.0252\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0232 - val_loss: 0.0249\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0230 - val_loss: 0.0253\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0230 - val_loss: 0.0270\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0237 - val_loss: 0.0305\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0274 - val_loss: 0.0303\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0314 - val_loss: 0.0266\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0280 - val_loss: 0.0316\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0250 - val_loss: 0.0322\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0237 - val_loss: 0.0275\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0229 - val_loss: 0.0258\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0223 - val_loss: 0.0244\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0220 - val_loss: 0.0234\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0219 - val_loss: 0.0232\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0218 - val_loss: 0.0235\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0221 - val_loss: 0.0242\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0228 - val_loss: 0.0248\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0239 - val_loss: 0.0242\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0243 - val_loss: 0.0234\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0233 - val_loss: 0.0237\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0224 - val_loss: 0.0237\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 3s 186ms/step - loss: 0.0219 - val_loss: 0.0234\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 3s 189ms/step - loss: 0.0216 - val_loss: 0.0231\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 3s 190ms/step - loss: 0.0215 - val_loss: 0.0228\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0214 - val_loss: 0.0226\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0216 - val_loss: 0.0227\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0218 - val_loss: 0.0228\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0220 - val_loss: 0.0228\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0221 - val_loss: 0.0227\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0220 - val_loss: 0.0226\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0216 - val_loss: 0.0225\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 3s 186ms/step - loss: 0.0214 - val_loss: 0.0224\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 0.0213 - val_loss: 0.0224\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0212 - val_loss: 0.0223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0212 - val_loss: 0.0222\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 3s 188ms/step - loss: 0.0212 - val_loss: 0.0222\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 0.0212 - val_loss: 0.0222\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0212 - val_loss: 0.0223\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0213 - val_loss: 0.0223\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0213 - val_loss: 0.0223\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0214 - val_loss: 0.0222\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0213 - val_loss: 0.0222\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0213 - val_loss: 0.0222\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0212 - val_loss: 0.0221\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0212 - val_loss: 0.0221\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0211 - val_loss: 0.0220\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0210 - val_loss: 0.0220\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0210 - val_loss: 0.0220\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 3s 192ms/step - loss: 0.0210 - val_loss: 0.0220\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0209 - val_loss: 0.0220\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0210 - val_loss: 0.0219\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0209 - val_loss: 0.0218\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0208 - val_loss: 0.0218\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0208 - val_loss: 0.0218\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0208 - val_loss: 0.0218\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0208 - val_loss: 0.0218\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0207 - val_loss: 0.0218\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0207 - val_loss: 0.0218\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0205 - val_loss: 0.0216\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0205 - val_loss: 0.0216\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0205 - val_loss: 0.0216\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0205 - val_loss: 0.0216\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0202 - val_loss: 0.0216\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0202 - val_loss: 0.0216\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0202 - val_loss: 0.0216\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0201 - val_loss: 0.0217\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0202 - val_loss: 0.0216\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0201 - val_loss: 0.0216\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0202 - val_loss: 0.0217\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.0200 - val_loss: 0.0215\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0201 - val_loss: 0.0217\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0202 - val_loss: 0.0217\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0200 - val_loss: 0.0215\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0200 - val_loss: 0.0217\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0202 - val_loss: 0.0218\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0199 - val_loss: 0.0214\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0200 - val_loss: 0.0217\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0202 - val_loss: 0.0219\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0199 - val_loss: 0.0215\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0200 - val_loss: 0.0218\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0202 - val_loss: 0.0219\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0199 - val_loss: 0.0216\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0201 - val_loss: 0.0218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0202 - val_loss: 0.0218\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0199 - val_loss: 0.0215\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0200 - val_loss: 0.0218\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0201 - val_loss: 0.0218\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0198 - val_loss: 0.0216\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0200 - val_loss: 0.0218\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0200 - val_loss: 0.0217\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0197 - val_loss: 0.0215\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0200 - val_loss: 0.0218\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0198 - val_loss: 0.0216\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0196 - val_loss: 0.0215\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0198 - val_loss: 0.0218\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0197 - val_loss: 0.0214\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0195 - val_loss: 0.0215\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0198 - val_loss: 0.0217\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0196 - val_loss: 0.0214\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0195 - val_loss: 0.0215\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0197 - val_loss: 0.0216\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 3s 186ms/step - loss: 0.0195 - val_loss: 0.0215\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0196 - val_loss: 0.0215\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0196 - val_loss: 0.0217\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0194 - val_loss: 0.0216\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0197 - val_loss: 0.0218\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0195 - val_loss: 0.0217\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0194 - val_loss: 0.0214\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0196 - val_loss: 0.0219\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0194 - val_loss: 0.0215\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0194 - val_loss: 0.0214\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0194 - val_loss: 0.0218\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.0193 - val_loss: 0.0215\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0194 - val_loss: 0.0215\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0193 - val_loss: 0.0216\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0193 - val_loss: 0.0215\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0192 - val_loss: 0.0216\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0192 - val_loss: 0.0215\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0193 - val_loss: 0.0217\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0193 - val_loss: 0.0215\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0194 - val_loss: 0.0217\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.0193 - val_loss: 0.0218\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 3s 190ms/step - loss: 0.0192 - val_loss: 0.0215\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 3s 190ms/step - loss: 0.0194 - val_loss: 0.0221\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 3s 184ms/step - loss: 0.0192 - val_loss: 0.0214\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0192 - val_loss: 0.0219\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0195 - val_loss: 0.0225\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0193 - val_loss: 0.0217\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0197 - val_loss: 0.0228\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0194 - val_loss: 0.0213\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0190 - val_loss: 0.0215\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0193 - val_loss: 0.0227\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0192 - val_loss: 0.0214\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0191 - val_loss: 0.0219\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0191 - val_loss: 0.0218\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0190 - val_loss: 0.0214\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0192 - val_loss: 0.0221\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0191 - val_loss: 0.0215\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0190 - val_loss: 0.0216\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0191 - val_loss: 0.0218\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0190 - val_loss: 0.0215\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0191 - val_loss: 0.0219\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0191 - val_loss: 0.0219\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0189 - val_loss: 0.0219\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0189 - val_loss: 0.0215\n",
      "Neurons: 47, Quantile: 0.3, Accuracy: 0.02152092196047306\n",
      "--- 1363.1405560970306 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 3s 205ms/step - loss: 0.2182 - val_loss: 0.1450\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.1016 - val_loss: 0.0704\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.0860 - val_loss: 0.0513\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.0593 - val_loss: 0.0464\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0577 - val_loss: 0.0514\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0463 - val_loss: 0.0596\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0396 - val_loss: 0.0464\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0344 - val_loss: 0.0396\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0316 - val_loss: 0.0361\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0302 - val_loss: 0.0341\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0296 - val_loss: 0.0329\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0294 - val_loss: 0.0323\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0295 - val_loss: 0.0321\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0300 - val_loss: 0.0318\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0307 - val_loss: 0.0314\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0317 - val_loss: 0.0330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0312 - val_loss: 0.0338\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0301 - val_loss: 0.0333\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0291 - val_loss: 0.0321\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0284 - val_loss: 0.0310\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0279 - val_loss: 0.0302\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0277 - val_loss: 0.0297\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0276 - val_loss: 0.0294\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0277 - val_loss: 0.0291\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0278 - val_loss: 0.0290\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0279 - val_loss: 0.0287\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0279 - val_loss: 0.0287\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0278 - val_loss: 0.0286\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0275 - val_loss: 0.0286\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0271 - val_loss: 0.0281\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0269 - val_loss: 0.0275\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0267 - val_loss: 0.0277\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0265 - val_loss: 0.0271\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0263 - val_loss: 0.0278\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0263 - val_loss: 0.0267\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0263 - val_loss: 0.0272\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0262 - val_loss: 0.0265\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0260 - val_loss: 0.0274\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0260 - val_loss: 0.0263\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0257 - val_loss: 0.0264\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0255 - val_loss: 0.0261\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 3s 194ms/step - loss: 0.0253 - val_loss: 0.0261\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 0.0253 - val_loss: 0.0259\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0252 - val_loss: 0.0257\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0251 - val_loss: 0.0258\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0250 - val_loss: 0.0257\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 3s 185ms/step - loss: 0.0250 - val_loss: 0.0257\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 3s 186ms/step - loss: 0.0249 - val_loss: 0.0255\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0248 - val_loss: 0.0256\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0247 - val_loss: 0.0256\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0248 - val_loss: 0.0256\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 3s 189ms/step - loss: 0.0246 - val_loss: 0.0255\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0247 - val_loss: 0.0255\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0245 - val_loss: 0.0257\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0248 - val_loss: 0.0255\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0244 - val_loss: 0.0255\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0247 - val_loss: 0.0254\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0243 - val_loss: 0.0254\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0246 - val_loss: 0.0253\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0243 - val_loss: 0.0253\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0245 - val_loss: 0.0252\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0242 - val_loss: 0.0252\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0244 - val_loss: 0.0253\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0242 - val_loss: 0.0252\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0244 - val_loss: 0.0252\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0241 - val_loss: 0.0251\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0243 - val_loss: 0.0252\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0240 - val_loss: 0.0251\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0243 - val_loss: 0.0252\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0240 - val_loss: 0.0251\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0242 - val_loss: 0.0251\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0239 - val_loss: 0.0250\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0242 - val_loss: 0.0251\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0239 - val_loss: 0.0250\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0241 - val_loss: 0.0250\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0238 - val_loss: 0.0250\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0241 - val_loss: 0.0250\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0238 - val_loss: 0.0249\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0240 - val_loss: 0.0250\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0238 - val_loss: 0.0249\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 0.0240 - val_loss: 0.0250\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0237 - val_loss: 0.0249\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0239 - val_loss: 0.0249\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0237 - val_loss: 0.0249\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0239 - val_loss: 0.0249\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0236 - val_loss: 0.0248\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0238 - val_loss: 0.0249\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0236 - val_loss: 0.0248\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0238 - val_loss: 0.0248\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0236 - val_loss: 0.0248\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0237 - val_loss: 0.0248\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0236 - val_loss: 0.0248\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0237 - val_loss: 0.0248\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0235 - val_loss: 0.0248\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0236 - val_loss: 0.0248\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0235 - val_loss: 0.0248\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0236 - val_loss: 0.0247\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0235 - val_loss: 0.0248\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0236 - val_loss: 0.0247\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0234 - val_loss: 0.0247\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0235 - val_loss: 0.0247\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0234 - val_loss: 0.0247\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0234 - val_loss: 0.0247\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0234 - val_loss: 0.0247\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0234 - val_loss: 0.0246\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0233 - val_loss: 0.0247\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0234 - val_loss: 0.0246\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0233 - val_loss: 0.0247\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0233 - val_loss: 0.0246\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0233 - val_loss: 0.0247\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0233 - val_loss: 0.0246\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0233 - val_loss: 0.0247\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0233 - val_loss: 0.0246\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0232 - val_loss: 0.0247\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0232 - val_loss: 0.0246\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0232 - val_loss: 0.0247\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0232 - val_loss: 0.0246\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0232 - val_loss: 0.0248\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0231 - val_loss: 0.0246\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0231 - val_loss: 0.0247\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0231 - val_loss: 0.0246\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0231 - val_loss: 0.0248\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0230 - val_loss: 0.0246\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0230 - val_loss: 0.0248\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0230 - val_loss: 0.0246\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0230 - val_loss: 0.0248\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0230 - val_loss: 0.0246\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0229 - val_loss: 0.0248\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0229 - val_loss: 0.0246\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0228 - val_loss: 0.0247\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0229 - val_loss: 0.0247\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0228 - val_loss: 0.0248\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0228 - val_loss: 0.0246\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0227 - val_loss: 0.0247\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0227 - val_loss: 0.0246\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0227 - val_loss: 0.0249\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0227 - val_loss: 0.0246\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0227 - val_loss: 0.0247\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0226 - val_loss: 0.0248\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0225 - val_loss: 0.0244\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0224 - val_loss: 0.0245\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0226 - val_loss: 0.0246\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0225 - val_loss: 0.0243\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0223 - val_loss: 0.0244\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0224 - val_loss: 0.0242\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0222 - val_loss: 0.0243\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0226 - val_loss: 0.0245\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0225 - val_loss: 0.0242\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0223 - val_loss: 0.0244\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0228 - val_loss: 0.0245\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0227 - val_loss: 0.0250\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0227 - val_loss: 0.0243\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0225 - val_loss: 0.0244\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0228 - val_loss: 0.0246\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.0228 - val_loss: 0.0251\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 3s 188ms/step - loss: 0.0229 - val_loss: 0.0257\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 3s 182ms/step - loss: 0.0227 - val_loss: 0.0255\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0223 - val_loss: 0.0244\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0221 - val_loss: 0.0246\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0228 - val_loss: 0.0257\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0222 - val_loss: 0.0243\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0222 - val_loss: 0.0243\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0224 - val_loss: 0.0251\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0218 - val_loss: 0.0243\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0224 - val_loss: 0.0248\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0223 - val_loss: 0.0249\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0223 - val_loss: 0.0243\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0219 - val_loss: 0.0243\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0226 - val_loss: 0.0246\n",
      "Epoch 177/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0224 - val_loss: 0.0249\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0230 - val_loss: 0.0252\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0220 - val_loss: 0.0244\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0225 - val_loss: 0.0247\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0222 - val_loss: 0.0248\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0227 - val_loss: 0.0254\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0222 - val_loss: 0.0249\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0219 - val_loss: 0.0242\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0220 - val_loss: 0.0243\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0219 - val_loss: 0.0244\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 0.0237 - val_loss: 0.0250\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 3s 186ms/step - loss: 0.0226 - val_loss: 0.0244\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 3s 177ms/step - loss: 0.0228 - val_loss: 0.0245\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 0.0224 - val_loss: 0.0243\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0229 - val_loss: 0.0245\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0224 - val_loss: 0.0245\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0227 - val_loss: 0.0250\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0226 - val_loss: 0.0246\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0222 - val_loss: 0.0251\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0234 - val_loss: 0.0265\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0224 - val_loss: 0.0244\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.0221 - val_loss: 0.0244\n",
      "Neurons: 47, Quantile: 0.6, Accuracy: 0.024449491873383522\n",
      "--- 1953.102668762207 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 5s 295ms/step - loss: 0.1026 - val_loss: 0.0681\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 4s 276ms/step - loss: 0.0735 - val_loss: 0.0695\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 4s 275ms/step - loss: 0.0572 - val_loss: 0.0422\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0581 - val_loss: 0.0572\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0434 - val_loss: 0.0361 0s - loss: 0.\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0462 - val_loss: 0.0356\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0348 - val_loss: 0.0408\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0316 - val_loss: 0.0434\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0311 - val_loss: 0.0412\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0298 - val_loss: 0.0416\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0289 - val_loss: 0.0390\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0278 - val_loss: 0.0374\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0270 - val_loss: 0.0354\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0263 - val_loss: 0.0338\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0258 - val_loss: 0.0322\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0254 - val_loss: 0.0309\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0251 - val_loss: 0.0299\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0249 - val_loss: 0.0289\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0246 - val_loss: 0.0281\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0244 - val_loss: 0.0274\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0242 - val_loss: 0.0266\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0240 - val_loss: 0.0260\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0238 - val_loss: 0.0256\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0236 - val_loss: 0.0255\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0235 - val_loss: 0.0259\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0236 - val_loss: 0.0273\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0244 - val_loss: 0.0304\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0280 - val_loss: 0.0309\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0323 - val_loss: 0.0272\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0274 - val_loss: 0.0338\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0251 - val_loss: 0.0307\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0238 - val_loss: 0.0283\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0230 - val_loss: 0.0250\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0224 - val_loss: 0.0237\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0220 - val_loss: 0.0231\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0220 - val_loss: 0.0232\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0220 - val_loss: 0.0235\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0224 - val_loss: 0.0239\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0231 - val_loss: 0.0241\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0236 - val_loss: 0.0241\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0239 - val_loss: 0.0233\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0231 - val_loss: 0.0238\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0226 - val_loss: 0.0233\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0222 - val_loss: 0.0233\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0218 - val_loss: 0.0231\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0217 - val_loss: 0.0226\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0215 - val_loss: 0.0225\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0215 - val_loss: 0.0223\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0215 - val_loss: 0.0224\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0216 - val_loss: 0.0225\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0219 - val_loss: 0.0225\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0220 - val_loss: 0.0225\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0220 - val_loss: 0.0225\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0220 - val_loss: 0.0222\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0217 - val_loss: 0.0222\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0215 - val_loss: 0.0221\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0214 - val_loss: 0.0222\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0214 - val_loss: 0.0220\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0212 - val_loss: 0.0220\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0212 - val_loss: 0.0219\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0213 - val_loss: 0.0220\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0213 - val_loss: 0.0220\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0213 - val_loss: 0.0220\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0213 - val_loss: 0.0220\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0214 - val_loss: 0.0219\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0214 - val_loss: 0.0219\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0213 - val_loss: 0.0219\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0212 - val_loss: 0.0218\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0212 - val_loss: 0.0217\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0211 - val_loss: 0.0219\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0211 - val_loss: 0.0218\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0209 - val_loss: 0.0217\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0210 - val_loss: 0.0216\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0210 - val_loss: 0.0218\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0209 - val_loss: 0.0217\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0209 - val_loss: 0.0217\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0210 - val_loss: 0.0216\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0210 - val_loss: 0.0217\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0209 - val_loss: 0.0217\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0208 - val_loss: 0.0217\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0209 - val_loss: 0.0215\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0209 - val_loss: 0.0217\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0208 - val_loss: 0.0217\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0207 - val_loss: 0.0216\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0208 - val_loss: 0.0215\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0208 - val_loss: 0.02161s - loss:\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0206 - val_loss: 0.0215\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0206 - val_loss: 0.0215\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0207 - val_loss: 0.0215\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0206 - val_loss: 0.0216\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0205 - val_loss: 0.0215\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0206 - val_loss: 0.0214\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0206 - val_loss: 0.0215\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0205 - val_loss: 0.0215\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0206 - val_loss: 0.0214\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0206 - val_loss: 0.0215\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0204 - val_loss: 0.0215\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0205 - val_loss: 0.0213\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0204 - val_loss: 0.0214\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0203 - val_loss: 0.0215\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0203 - val_loss: 0.0216\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0204 - val_loss: 0.0213\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0203 - val_loss: 0.0214\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0203 - val_loss: 0.0215\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0202 - val_loss: 0.02150s - loss: 0.\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0203 - val_loss: 0.0213\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0202 - val_loss: 0.0213\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0202 - val_loss: 0.0214\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0202 - val_loss: 0.0215\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0202 - val_loss: 0.0214\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0203 - val_loss: 0.0213\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0202 - val_loss: 0.0214\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0201 - val_loss: 0.0215\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0201 - val_loss: 0.0214\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0202 - val_loss: 0.0213\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0201 - val_loss: 0.0214\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0200 - val_loss: 0.0216\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0201 - val_loss: 0.0214\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0201 - val_loss: 0.0212\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0201 - val_loss: 0.0213\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0200 - val_loss: 0.0216\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0201 - val_loss: 0.0214\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0200 - val_loss: 0.0213\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0200 - val_loss: 0.0213\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0200 - val_loss: 0.0216\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0200 - val_loss: 0.0214\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0199 - val_loss: 0.0212\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0199 - val_loss: 0.0214\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0200 - val_loss: 0.0214\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0199 - val_loss: 0.0213\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0198 - val_loss: 0.0214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0200 - val_loss: 0.0214\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0198 - val_loss: 0.0213\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0196 - val_loss: 0.0213\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0198 - val_loss: 0.0214\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0198 - val_loss: 0.0214\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0197 - val_loss: 0.0213\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0197 - val_loss: 0.0215\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0197 - val_loss: 0.0217\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0198 - val_loss: 0.0214\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0196 - val_loss: 0.0214\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0198 - val_loss: 0.0214\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0198 - val_loss: 0.0218\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0197 - val_loss: 0.0215\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0196 - val_loss: 0.0217\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0199 - val_loss: 0.0216\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0197 - val_loss: 0.0215\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0194 - val_loss: 0.0218\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0202 - val_loss: 0.0220\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0199 - val_loss: 0.0217\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0196 - val_loss: 0.0214\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0195 - val_loss: 0.0220\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0201 - val_loss: 0.0219\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0197 - val_loss: 0.0214\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0194 - val_loss: 0.0217\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0195 - val_loss: 0.0217\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0196 - val_loss: 0.0215\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0194 - val_loss: 0.0217\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0197 - val_loss: 0.0217\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0196 - val_loss: 0.0220\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0196 - val_loss: 0.0222\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0201 - val_loss: 0.0215\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0196 - val_loss: 0.0220\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0198 - val_loss: 0.0215\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0195 - val_loss: 0.0217\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0194 - val_loss: 0.0218\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0196 - val_loss: 0.0218\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0192 - val_loss: 0.0218\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0195 - val_loss: 0.0219\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0194 - val_loss: 0.0219\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0192 - val_loss: 0.0223\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0192 - val_loss: 0.0216\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0194 - val_loss: 0.0227\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0193 - val_loss: 0.0228\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0195 - val_loss: 0.0238\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0192 - val_loss: 0.0223\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0192 - val_loss: 0.0233\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0192 - val_loss: 0.0223\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0189 - val_loss: 0.0227\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0189 - val_loss: 0.0221\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 0.0194 - val_loss: 0.0237\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0200 - val_loss: 0.0224\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0194 - val_loss: 0.0222\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0191 - val_loss: 0.0222\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0193 - val_loss: 0.0226\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0191 - val_loss: 0.0221\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0189 - val_loss: 0.0228\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0192 - val_loss: 0.0225\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0188 - val_loss: 0.0218\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0193 - val_loss: 0.0226\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0196 - val_loss: 0.0225\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0193 - val_loss: 0.0222\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0189 - val_loss: 0.0220\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0200 - val_loss: 0.0222\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0192 - val_loss: 0.0227\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0196 - val_loss: 0.0219\n",
      "Neurons: 72, Quantile: 0.3, Accuracy: 0.021949278190732002\n",
      "--- 2820.532065629959 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 5s 290ms/step - loss: 0.2015 - val_loss: 0.1266\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 4s 271ms/step - loss: 0.1114 - val_loss: 0.0675\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 0.0780 - val_loss: 0.0518\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0729 - val_loss: 0.0486\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0652 - val_loss: 0.0460\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0596 - val_loss: 0.0504\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0499 - val_loss: 0.0554\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0403 - val_loss: 0.0439\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0348 - val_loss: 0.0390\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0320 - val_loss: 0.0347\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0306 - val_loss: 0.0328\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0300 - val_loss: 0.0321\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0301 - val_loss: 0.0318\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0305 - val_loss: 0.0318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0309 - val_loss: 0.0322A: 0s - loss: 0.03\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0310 - val_loss: 0.0328\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0306 - val_loss: 0.0330\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0300 - val_loss: 0.0325\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0293 - val_loss: 0.0317\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0287 - val_loss: 0.0310\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0284 - val_loss: 0.0304\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0282 - val_loss: 0.0299\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0282 - val_loss: 0.0297\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0283 - val_loss: 0.0295\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0283 - val_loss: 0.0295\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0283 - val_loss: 0.0295\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0281 - val_loss: 0.0293\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0278 - val_loss: 0.0290\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0276 - val_loss: 0.0288\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0274 - val_loss: 0.0289\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0271 - val_loss: 0.0283\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0270 - val_loss: 0.0281\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0269 - val_loss: 0.0297\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0272 - val_loss: 0.0278\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0271 - val_loss: 0.0298\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0270 - val_loss: 0.0271\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0268 - val_loss: 0.0282\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0272 - val_loss: 0.0275\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0267 - val_loss: 0.0268\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0264 - val_loss: 0.0269\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0261 - val_loss: 0.0265\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0258 - val_loss: 0.0262\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 262ms/step - loss: 0.0255 - val_loss: 0.0262\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0255 - val_loss: 0.0260\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0253 - val_loss: 0.0262\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0254 - val_loss: 0.0259\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0251 - val_loss: 0.0260\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0252 - val_loss: 0.0259\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0249 - val_loss: 0.0257\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0250 - val_loss: 0.0256\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0247 - val_loss: 0.0256\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0249 - val_loss: 0.0256\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0247 - val_loss: 0.0256\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0248 - val_loss: 0.0254\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0245 - val_loss: 0.0256\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0247 - val_loss: 0.0254\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0244 - val_loss: 0.0255ETA: 0s - loss: 0.0\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0246 - val_loss: 0.0252\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0243 - val_loss: 0.0255\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0246 - val_loss: 0.0253\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0243 - val_loss: 0.0254\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0244 - val_loss: 0.0251\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0242 - val_loss: 0.0254\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0244 - val_loss: 0.0252\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0241 - val_loss: 0.0252\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0242 - val_loss: 0.0251\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0241 - val_loss: 0.0252\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0242 - val_loss: 0.0252s - loss: 0.\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0240 - val_loss: 0.0252\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0240 - val_loss: 0.0250\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0240 - val_loss: 0.0252\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0240 - val_loss: 0.0251\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0239 - val_loss: 0.0251\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0239 - val_loss: 0.0249\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0239 - val_loss: 0.0251\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0239 - val_loss: 0.0250\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0238 - val_loss: 0.0251\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 0.0238 - val_loss: 0.0249\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0237 - val_loss: 0.0251\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0238 - val_loss: 0.0249\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0237 - val_loss: 0.0250\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0237 - val_loss: 0.0249\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0236 - val_loss: 0.0251\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0236 - val_loss: 0.0249\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0235 - val_loss: 0.0249\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0236 - val_loss: 0.02490s - loss: 0.0\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0234 - val_loss: 0.0249\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0235 - val_loss: 0.0249\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0234 - val_loss: 0.0248\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0234 - val_loss: 0.0249\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0233 - val_loss: 0.0249 0s - loss: \n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0233 - val_loss: 0.0250\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0231 - val_loss: 0.0246\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0232 - val_loss: 0.0247\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0232 - val_loss: 0.0248\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0233 - val_loss: 0.0250\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0230 - val_loss: 0.0246\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0231 - val_loss: 0.0246\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0232 - val_loss: 0.0248\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0233 - val_loss: 0.0251\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0230 - val_loss: 0.0245\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0229 - val_loss: 0.0247A: 1s - loss: \n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0231 - val_loss: 0.0246\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0237 - val_loss: 0.0253\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0230 - val_loss: 0.0246\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0231 - val_loss: 0.0248\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0229 - val_loss: 0.0246\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0230 - val_loss: 0.0246\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0229 - val_loss: 0.0247\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0229 - val_loss: 0.0246\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0227 - val_loss: 0.0247\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0230 - val_loss: 0.0248\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0230 - val_loss: 0.0251\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0228 - val_loss: 0.0246\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0227 - val_loss: 0.02480s - loss: 0.0\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0232 - val_loss: 0.0248\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0224 - val_loss: 0.0248\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0226 - val_loss: 0.0245\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0228 - val_loss: 0.0250\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0230 - val_loss: 0.0251\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0226 - val_loss: 0.0253\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0227 - val_loss: 0.0248\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0230 - val_loss: 0.0255\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0228 - val_loss: 0.0253\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0226 - val_loss: 0.0254\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0229 - val_loss: 0.0246\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0227 - val_loss: 0.0251\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0228 - val_loss: 0.0245\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0228 - val_loss: 0.0251\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0228 - val_loss: 0.0243A: 0s - loss: 0.0\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0230 - val_loss: 0.0250\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0231 - val_loss: 0.0252\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0231 - val_loss: 0.0252\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0226 - val_loss: 0.0247\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0232 - val_loss: 0.0254 ETA: 1s - loss\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0229 - val_loss: 0.0248\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0229 - val_loss: 0.0252\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.0226 - val_loss: 0.0245 ETA: 1s - loss - ETA: 0s - loss: 0\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0233 - val_loss: 0.0249\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0227 - val_loss: 0.0248\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0227 - val_loss: 0.0250\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0224 - val_loss: 0.0249\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0231 - val_loss: 0.0256\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0226 - val_loss: 0.0255\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0222 - val_loss: 0.0249\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0220 - val_loss: 0.0247\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0228 - val_loss: 0.0261\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0221 - val_loss: 0.0251\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0218 - val_loss: 0.0252\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0224 - val_loss: 0.0255\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0219 - val_loss: 0.0253\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0221 - val_loss: 0.0248\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0220 - val_loss: 0.0250\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0220 - val_loss: 0.0251\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0226 - val_loss: 0.0253\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0231 - val_loss: 0.0260\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0224 - val_loss: 0.0245\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0218 - val_loss: 0.0253\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0221 - val_loss: 0.0253\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0216 - val_loss: 0.0252\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0221 - val_loss: 0.0245\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0219 - val_loss: 0.0249\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0220 - val_loss: 0.0252\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0222 - val_loss: 0.0246\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0227 - val_loss: 0.0249\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0218 - val_loss: 0.0251\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0228 - val_loss: 0.0258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0225 - val_loss: 0.0246\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0221 - val_loss: 0.0254\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 4s 255ms/step - loss: 0.0220 - val_loss: 0.0249\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0225 - val_loss: 0.0254\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0219 - val_loss: 0.0246\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0214 - val_loss: 0.0248\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0218 - val_loss: 0.0255\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0220 - val_loss: 0.0253\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0213 - val_loss: 0.0251\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 0.0216 - val_loss: 0.0245\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 4s 252ms/step - loss: 0.0216 - val_loss: 0.0250 ETA: 0s - loss: 0.0\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0218 - val_loss: 0.0254\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0219 - val_loss: 0.0252\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0230 - val_loss: 0.0260\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0225 - val_loss: 0.0248\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0224 - val_loss: 0.0248\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 0.0221 - val_loss: 0.0246\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 4s 261ms/step - loss: 0.0216 - val_loss: 0.0248\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 0.0219 - val_loss: 0.0264\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0220 - val_loss: 0.0255\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0212 - val_loss: 0.0251\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0220 - val_loss: 0.0261\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0229 - val_loss: 0.0248\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0216 - val_loss: 0.0246\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 4s 260ms/step - loss: 0.0213 - val_loss: 0.0243\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0213 - val_loss: 0.0252\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0217 - val_loss: 0.0256\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 0.0212 - val_loss: 0.0248\n",
      "Neurons: 72, Quantile: 0.6, Accuracy: 0.024831047281622887\n",
      "--- 3681.1318151950836 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0728 - val_loss: 0.0847\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 9s 533ms/step - loss: 0.0761 - val_loss: 0.0523\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 8s 518ms/step - loss: 0.0670 - val_loss: 0.0626\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 8s 525ms/step - loss: 0.0566 - val_loss: 0.0410\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 8s 515ms/step - loss: 0.0546 - val_loss: 0.0471\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 8s 531ms/step - loss: 0.0426 - val_loss: 0.0346\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 8s 530ms/step - loss: 0.0360 - val_loss: 0.0336\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 8s 525ms/step - loss: 0.0328 - val_loss: 0.0316\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0320 - val_loss: 0.0315\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 8s 523ms/step - loss: 0.0298 - val_loss: 0.0322\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0283 - val_loss: 0.0316\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 8s 511ms/step - loss: 0.0273 - val_loss: 0.0306\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 8s 524ms/step - loss: 0.0266 - val_loss: 0.0294\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 8s 507ms/step - loss: 0.0260 - val_loss: 0.0285\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 9s 532ms/step - loss: 0.0256 - val_loss: 0.0278\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 9s 587ms/step - loss: 0.0252 - val_loss: 0.0273\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0249 - val_loss: 0.0270\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0247 - val_loss: 0.0270\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 11s 705ms/step - loss: 0.0248 - val_loss: 0.0271\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 10s 607ms/step - loss: 0.0249 - val_loss: 0.0284\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0263 - val_loss: 0.0312\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 8s 525ms/step - loss: 0.0307 - val_loss: 0.0285\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0304 - val_loss: 0.0282\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 9s 533ms/step - loss: 0.0282 - val_loss: 0.0282\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0254 - val_loss: 0.0264\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 10s 617ms/step - loss: 0.0239 - val_loss: 0.0251\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 9s 541ms/step - loss: 0.0234 - val_loss: 0.0237\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 9s 579ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0230 - val_loss: 0.0235\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0231 - val_loss: 0.0246\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0239 - val_loss: 0.0232\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0235 - val_loss: 0.0241\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 0.0234 - val_loss: 0.0231\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 8s 528ms/step - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0227 - val_loss: 0.0229\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0222 - val_loss: 0.0235\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 9s 536ms/step - loss: 0.0223 - val_loss: 0.0228\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0221 - val_loss: 0.0233\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0222 - val_loss: 0.0224\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0220 - val_loss: 0.0231\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 9s 541ms/step - loss: 0.0223 - val_loss: 0.0226\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0222 - val_loss: 0.0230\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0223 - val_loss: 0.0224\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0222 - val_loss: 0.0230\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 9s 541ms/step - loss: 0.0225 - val_loss: 0.0224\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 8s 528ms/step - loss: 0.0221 - val_loss: 0.0227\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 9s 579ms/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 8s 525ms/step - loss: 0.0219 - val_loss: 0.0227\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0216 - val_loss: 0.0224\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 8s 527ms/step - loss: 0.0215 - val_loss: 0.0224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0217 - val_loss: 0.0224\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 8s 511ms/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0215 - val_loss: 0.0224\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 8s 518ms/step - loss: 0.0216 - val_loss: 0.0222\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0216 - val_loss: 0.0222\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 8s 527ms/step - loss: 0.0215 - val_loss: 0.0220\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 8s 521ms/step - loss: 0.0214 - val_loss: 0.0221\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0214 - val_loss: 0.0219\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 9s 541ms/step - loss: 0.0214 - val_loss: 0.0218\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0211 - val_loss: 0.0220\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0212 - val_loss: 0.0218\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0211 - val_loss: 0.0218\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0212 - val_loss: 0.0217\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0211 - val_loss: 0.0218\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0211 - val_loss: 0.0217\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 8s 512ms/step - loss: 0.0209 - val_loss: 0.0218\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0210 - val_loss: 0.0218\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 8s 514ms/step - loss: 0.0210 - val_loss: 0.0217\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 8s 520ms/step - loss: 0.0211 - val_loss: 0.0217\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 8s 519ms/step - loss: 0.0209 - val_loss: 0.0218\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0208 - val_loss: 0.0217\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 9s 587ms/step - loss: 0.0207 - val_loss: 0.0222\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 8s 530ms/step - loss: 0.0210 - val_loss: 0.0217\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 8s 515ms/step - loss: 0.0210 - val_loss: 0.0221\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 9s 534ms/step - loss: 0.0209 - val_loss: 0.0217\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 8s 521ms/step - loss: 0.0206 - val_loss: 0.0220\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0207 - val_loss: 0.0222\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0213 - val_loss: 0.0220\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 9s 540ms/step - loss: 0.0209 - val_loss: 0.0217\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 8s 529ms/step - loss: 0.0207 - val_loss: 0.0219\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0206 - val_loss: 0.0223\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 9s 542ms/step - loss: 0.0211 - val_loss: 0.0218\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 8s 527ms/step - loss: 0.0209 - val_loss: 0.0218\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0209 - val_loss: 0.0217\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 8s 529ms/step - loss: 0.0203 - val_loss: 0.0221\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0209 - val_loss: 0.0219\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 9s 533ms/step - loss: 0.0208 - val_loss: 0.0218\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0207 - val_loss: 0.0216\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 9s 533ms/step - loss: 0.0202 - val_loss: 0.0217\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0205 - val_loss: 0.0219\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 9s 586ms/step - loss: 0.0206 - val_loss: 0.0218\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0206 - val_loss: 0.0219\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0204 - val_loss: 0.0216\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0201 - val_loss: 0.0218\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0204 - val_loss: 0.0219\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0206 - val_loss: 0.0220\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 9s 572ms/step - loss: 0.0200 - val_loss: 0.0221\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0203 - val_loss: 0.0220\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0206 - val_loss: 0.0216\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 9s 540ms/step - loss: 0.0200 - val_loss: 0.0227\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 8s 510ms/step - loss: 0.0210 - val_loss: 0.0220\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 9s 576ms/step - loss: 0.0208 - val_loss: 0.0221\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 8s 519ms/step - loss: 0.0201 - val_loss: 0.0228\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 8s 523ms/step - loss: 0.0212 - val_loss: 0.0221\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 8s 521ms/step - loss: 0.0209 - val_loss: 0.0220\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 10s 634ms/step - loss: 0.0207 - val_loss: 0.0215\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0201 - val_loss: 0.0223\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 9s 534ms/step - loss: 0.0205 - val_loss: 0.0220\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 8s 527ms/step - loss: 0.0206 - val_loss: 0.0222\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0203 - val_loss: 0.0218\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 9s 575ms/step - loss: 0.0200 - val_loss: 0.0217\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 9s 566ms/step - loss: 0.0198 - val_loss: 0.0222\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0204 - val_loss: 0.0226\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 8s 524ms/step - loss: 0.0202 - val_loss: 0.0218\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0200 - val_loss: 0.0222\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0197 - val_loss: 0.0221\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 9s 536ms/step - loss: 0.0200 - val_loss: 0.0224\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 9s 542ms/step - loss: 0.0199 - val_loss: 0.0217\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0198 - val_loss: 0.0225\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0195 - val_loss: 0.0218\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0196 - val_loss: 0.0221\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 8s 521ms/step - loss: 0.0198 - val_loss: 0.0217\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0202 - val_loss: 0.0224\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0198 - val_loss: 0.0221\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 9s 582ms/step - loss: 0.0199 - val_loss: 0.0224\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 8s 523ms/step - loss: 0.0202 - val_loss: 0.0218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0199 - val_loss: 0.0221\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 8s 526ms/step - loss: 0.0199 - val_loss: 0.0235\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0210 - val_loss: 0.0227\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 9s 536ms/step - loss: 0.0206 - val_loss: 0.0229\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 8s 525ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0203 - val_loss: 0.0224\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 9s 532ms/step - loss: 0.0205 - val_loss: 0.0221\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 9s 536ms/step - loss: 0.0201 - val_loss: 0.0235\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 9s 532ms/step - loss: 0.0206 - val_loss: 0.0224\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 0.0201 - val_loss: 0.0228\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 8s 528ms/step - loss: 0.0199 - val_loss: 0.0231\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 8s 522ms/step - loss: 0.0196 - val_loss: 0.0233\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0196 - val_loss: 0.0235\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0196 - val_loss: 0.0246\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0198 - val_loss: 0.0225\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 8s 518ms/step - loss: 0.0197 - val_loss: 0.0226\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0195 - val_loss: 0.0223\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0191 - val_loss: 0.0221\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 9s 540ms/step - loss: 0.0189 - val_loss: 0.0224\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0192 - val_loss: 0.0227\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 9s 533ms/step - loss: 0.0200 - val_loss: 0.0226\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0195 - val_loss: 0.0222\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 8s 526ms/step - loss: 0.0197 - val_loss: 0.0222\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0193 - val_loss: 0.0221\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 8s 528ms/step - loss: 0.0195 - val_loss: 0.0220\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 8s 530ms/step - loss: 0.0195 - val_loss: 0.0220\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0196 - val_loss: 0.0226\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 8s 523ms/step - loss: 0.0200 - val_loss: 0.0239\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0212 - val_loss: 0.0231\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 8s 505ms/step - loss: 0.0206 - val_loss: 0.0236\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 8s 522ms/step - loss: 0.0213 - val_loss: 0.0230\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 8s 504ms/step - loss: 0.0213 - val_loss: 0.0224\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 8s 521ms/step - loss: 0.0205 - val_loss: 0.0219\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 9s 534ms/step - loss: 0.0199 - val_loss: 0.0216\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0193 - val_loss: 0.0222\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 9s 580ms/step - loss: 0.0193 - val_loss: 0.0230\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 9s 532ms/step - loss: 0.0193 - val_loss: 0.0240\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0191 - val_loss: 0.0248\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 8s 506ms/step - loss: 0.0194 - val_loss: 0.0222\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 9s 533ms/step - loss: 0.0190 - val_loss: 0.0230\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0192 - val_loss: 0.0256\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0195 - val_loss: 0.0220\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 8s 528ms/step - loss: 0.0187 - val_loss: 0.0224\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 8s 525ms/step - loss: 0.0194 - val_loss: 0.0246\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0197 - val_loss: 0.0224\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 9s 544ms/step - loss: 0.0191 - val_loss: 0.0228\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0190 - val_loss: 0.0229\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0193 - val_loss: 0.0245\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 9s 542ms/step - loss: 0.0191 - val_loss: 0.0222\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 9s 532ms/step - loss: 0.0186 - val_loss: 0.0222\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 8s 514ms/step - loss: 0.0188 - val_loss: 0.0236\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0193 - val_loss: 0.0214\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0185 - val_loss: 0.0223\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 10s 598ms/step - loss: 0.0187 - val_loss: 0.0228\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0186 - val_loss: 0.0231\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0187 - val_loss: 0.0229\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 9s 541ms/step - loss: 0.0185 - val_loss: 0.0227\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 9s 532ms/step - loss: 0.0184 - val_loss: 0.0218\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0184 - val_loss: 0.0226\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 10s 622ms/step - loss: 0.0192 - val_loss: 0.0235\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0187 - val_loss: 0.0232\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 9s 571ms/step - loss: 0.0188 - val_loss: 0.0218\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0184 - val_loss: 0.0243\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 8s 530ms/step - loss: 0.0189 - val_loss: 0.0216\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 9s 532ms/step - loss: 0.0184 - val_loss: 0.0228\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 9s 534ms/step - loss: 0.0190 - val_loss: 0.0244\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 9s 588ms/step - loss: 0.0186 - val_loss: 0.0222\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0182 - val_loss: 0.0222\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0192 - val_loss: 0.0242\n",
      "Neurons: 148, Quantile: 0.3, Accuracy: 0.024230362847447395\n",
      "--- 5528.637852668762 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 9s 587ms/step - loss: 0.1404 - val_loss: 0.1445\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0979 - val_loss: 0.0802\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 9s 547ms/step - loss: 0.1015 - val_loss: 0.0713\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 9s 532ms/step - loss: 0.0685 - val_loss: 0.0538\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 0.0771 - val_loss: 0.0544\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 8s 522ms/step - loss: 0.0678 - val_loss: 0.0496\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 8s 500ms/step - loss: 0.0702 - val_loss: 0.0483\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 9s 534ms/step - loss: 0.0664 - val_loss: 0.0460\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0648 - val_loss: 0.0464\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0603 - val_loss: 0.0467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200\n",
      "16/16 [==============================] - 8s 500ms/step - loss: 0.0484 - val_loss: 0.0460\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 8s 528ms/step - loss: 0.0456 - val_loss: 0.0483\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0407 - val_loss: 0.0501\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 8s 525ms/step - loss: 0.0357 - val_loss: 0.0398\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 8s 517ms/step - loss: 0.0324 - val_loss: 0.0383\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 8s 499ms/step - loss: 0.0309 - val_loss: 0.0347\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 8s 519ms/step - loss: 0.0300 - val_loss: 0.0335\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 8s 508ms/step - loss: 0.0295 - val_loss: 0.0325\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0293 - val_loss: 0.0320\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 8s 508ms/step - loss: 0.0292 - val_loss: 0.0317\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 9s 547ms/step - loss: 0.0291 - val_loss: 0.0315\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0290 - val_loss: 0.0312\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 9s 534ms/step - loss: 0.0288 - val_loss: 0.0307\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 0.0284 - val_loss: 0.0304\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 8s 506ms/step - loss: 0.0280 - val_loss: 0.0298\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 9s 534ms/step - loss: 0.0278 - val_loss: 0.0292\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 8s 490ms/step - loss: 0.0279 - val_loss: 0.0300\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0278 - val_loss: 0.0283\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 9s 544ms/step - loss: 0.0274 - val_loss: 0.0287\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 10s 639ms/step - loss: 0.0276 - val_loss: 0.0290\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 9s 590ms/step - loss: 0.0279 - val_loss: 0.0278\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0275 - val_loss: 0.0278\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0270 - val_loss: 0.0276\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0268 - val_loss: 0.0286\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0272 - val_loss: 0.0283\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 8s 528ms/step - loss: 0.0259 - val_loss: 0.0267\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 8s 530ms/step - loss: 0.0258 - val_loss: 0.0265\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 8s 524ms/step - loss: 0.0256 - val_loss: 0.0266\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0259 - val_loss: 0.0266\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0256 - val_loss: 0.0268\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0261 - val_loss: 0.0264\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 9s 578ms/step - loss: 0.0255 - val_loss: 0.0264\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0256 - val_loss: 0.0264\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 9s 573ms/step - loss: 0.0252 - val_loss: 0.0265\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 9s 579ms/step - loss: 0.0255 - val_loss: 0.0264\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0252 - val_loss: 0.0265\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 9s 585ms/step - loss: 0.0255 - val_loss: 0.0265\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 10s 611ms/step - loss: 0.0249 - val_loss: 0.0259\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 9s 580ms/step - loss: 0.0252 - val_loss: 0.0263\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 10s 601ms/step - loss: 0.0249 - val_loss: 0.0260\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0251 - val_loss: 0.0262\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0248 - val_loss: 0.0259\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0250 - val_loss: 0.0262\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 9s 578ms/step - loss: 0.0248 - val_loss: 0.0257\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 10s 604ms/step - loss: 0.0249 - val_loss: 0.0260\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0246 - val_loss: 0.0258\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0249 - val_loss: 0.0260\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0245 - val_loss: 0.0254\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 9s 571ms/step - loss: 0.0246 - val_loss: 0.0260\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0244 - val_loss: 0.0253\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0246 - val_loss: 0.0261\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0243 - val_loss: 0.0251\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 9s 575ms/step - loss: 0.0244 - val_loss: 0.0260\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 9s 559ms/step - loss: 0.0242 - val_loss: 0.0250\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 9s 589ms/step - loss: 0.0243 - val_loss: 0.0260\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0242 - val_loss: 0.0250\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0241 - val_loss: 0.0255\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0241 - val_loss: 0.0251\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0241 - val_loss: 0.0256\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 9s 570ms/step - loss: 0.0241 - val_loss: 0.0254\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0238 - val_loss: 0.0250\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0240 - val_loss: 0.0260\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0236 - val_loss: 0.0248\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0236 - val_loss: 0.0252\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0235 - val_loss: 0.0249\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 9s 534ms/step - loss: 0.0237 - val_loss: 0.0250\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 9s 589ms/step - loss: 0.0237 - val_loss: 0.0251\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0238 - val_loss: 0.0255\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 10s 623ms/step - loss: 0.0238 - val_loss: 0.0251\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 12s 732ms/step - loss: 0.0238 - val_loss: 0.0257\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 10s 630ms/step - loss: 0.0235 - val_loss: 0.0249\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0236 - val_loss: 0.0252\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 9s 587ms/step - loss: 0.0234 - val_loss: 0.0252\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0235 - val_loss: 0.0253\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 9s 577ms/step - loss: 0.0233 - val_loss: 0.0248\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 9s 583ms/step - loss: 0.0239 - val_loss: 0.0251\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0244 - val_loss: 0.0256\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 9s 586ms/step - loss: 0.0240 - val_loss: 0.0252\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 9s 585ms/step - loss: 0.0239 - val_loss: 0.0252\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0237 - val_loss: 0.0250\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0237 - val_loss: 0.0247\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0238 - val_loss: 0.0252\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 9s 555ms/step - loss: 0.0235 - val_loss: 0.0246\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0236 - val_loss: 0.0249\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 9s 556ms/step - loss: 0.0236 - val_loss: 0.0246\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 10s 600ms/step - loss: 0.0238 - val_loss: 0.0251\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0235 - val_loss: 0.0247\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0235 - val_loss: 0.0249\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0236 - val_loss: 0.0249\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0237 - val_loss: 0.0250\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 9s 577ms/step - loss: 0.0235 - val_loss: 0.0249\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0235 - val_loss: 0.0251\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 9s 561ms/step - loss: 0.0234 - val_loss: 0.0247\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 9s 533ms/step - loss: 0.0236 - val_loss: 0.0250\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 9s 571ms/step - loss: 0.0233 - val_loss: 0.0247\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 9s 581ms/step - loss: 0.0235 - val_loss: 0.0250\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 9s 594ms/step - loss: 0.0232 - val_loss: 0.0244\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 10s 611ms/step - loss: 0.0234 - val_loss: 0.0249\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 9s 582ms/step - loss: 0.0234 - val_loss: 0.0244\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0232 - val_loss: 0.0246\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 9s 588ms/step - loss: 0.0232 - val_loss: 0.0246\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0231 - val_loss: 0.0243\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 10s 597ms/step - loss: 0.0230 - val_loss: 0.0247\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 9s 577ms/step - loss: 0.0233 - val_loss: 0.0246\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0228 - val_loss: 0.0244\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 9s 593ms/step - loss: 0.0234 - val_loss: 0.0251\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 8s 526ms/step - loss: 0.0232 - val_loss: 0.0244\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0232 - val_loss: 0.0254\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0234 - val_loss: 0.0249\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 9s 553ms/step - loss: 0.0232 - val_loss: 0.0251\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0228 - val_loss: 0.0247\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 0.0228 - val_loss: 0.0248\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 9s 581ms/step - loss: 0.0226 - val_loss: 0.0244\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 9s 549ms/step - loss: 0.0229 - val_loss: 0.0250\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0226 - val_loss: 0.0243\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 9s 580ms/step - loss: 0.0226 - val_loss: 0.0244\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 9s 585ms/step - loss: 0.0225 - val_loss: 0.0244\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 9s 572ms/step - loss: 0.0227 - val_loss: 0.0248\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 9s 585ms/step - loss: 0.0225 - val_loss: 0.0242\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 9s 583ms/step - loss: 0.0227 - val_loss: 0.0248\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 10s 617ms/step - loss: 0.0225 - val_loss: 0.0243\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 10s 597ms/step - loss: 0.0226 - val_loss: 0.0249\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 10s 618ms/step - loss: 0.0224 - val_loss: 0.0245\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 10s 609ms/step - loss: 0.0225 - val_loss: 0.0245\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 9s 590ms/step - loss: 0.0225 - val_loss: 0.0242\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 10s 614ms/step - loss: 0.0229 - val_loss: 0.0257\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0227 - val_loss: 0.0246\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0224 - val_loss: 0.0247\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 10s 610ms/step - loss: 0.0222 - val_loss: 0.0245\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0226 - val_loss: 0.0248\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0227 - val_loss: 0.0252\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 9s 568ms/step - loss: 0.0225 - val_loss: 0.0243\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 9s 551ms/step - loss: 0.0223 - val_loss: 0.0244\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 10s 617ms/step - loss: 0.0226 - val_loss: 0.0243\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 10s 602ms/step - loss: 0.0225 - val_loss: 0.0242\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 9s 590ms/step - loss: 0.0227 - val_loss: 0.0250\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 10s 595ms/step - loss: 0.0225 - val_loss: 0.0245\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0228 - val_loss: 0.0253\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 9s 575ms/step - loss: 0.0227 - val_loss: 0.0248\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0223 - val_loss: 0.0243\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0222 - val_loss: 0.0244\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 9s 563ms/step - loss: 0.0225 - val_loss: 0.0250\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 9s 539ms/step - loss: 0.0220 - val_loss: 0.0250\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 9s 577ms/step - loss: 0.0223 - val_loss: 0.0252\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 10s 621ms/step - loss: 0.0223 - val_loss: 0.0248\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 9s 574ms/step - loss: 0.0227 - val_loss: 0.0250\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 9s 580ms/step - loss: 0.0222 - val_loss: 0.0242\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 9s 587ms/step - loss: 0.0222 - val_loss: 0.0248\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 10s 622ms/step - loss: 0.0222 - val_loss: 0.0243\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 10s 619ms/step - loss: 0.0224 - val_loss: 0.0252\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 10s 619ms/step - loss: 0.0224 - val_loss: 0.0255\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 9s 587ms/step - loss: 0.0222 - val_loss: 0.0250\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0221 - val_loss: 0.0249\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0223 - val_loss: 0.0246\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 9s 558ms/step - loss: 0.0218 - val_loss: 0.0250\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 9s 533ms/step - loss: 0.0220 - val_loss: 0.0248\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 9s 565ms/step - loss: 0.0227 - val_loss: 0.0253\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0220 - val_loss: 0.0252\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 9s 548ms/step - loss: 0.0219 - val_loss: 0.0247\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 9s 547ms/step - loss: 0.0221 - val_loss: 0.0260\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 9s 535ms/step - loss: 0.0223 - val_loss: 0.0249\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 8s 528ms/step - loss: 0.0220 - val_loss: 0.0251\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 8s 525ms/step - loss: 0.0217 - val_loss: 0.0245\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 9s 537ms/step - loss: 0.0218 - val_loss: 0.0250\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 8s 529ms/step - loss: 0.0218 - val_loss: 0.0247\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 9s 546ms/step - loss: 0.0220 - val_loss: 0.0266\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 9s 587ms/step - loss: 0.0218 - val_loss: 0.0250\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0221 - val_loss: 0.0255\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 9s 572ms/step - loss: 0.0214 - val_loss: 0.0247\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 9s 569ms/step - loss: 0.0215 - val_loss: 0.0257\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 8s 526ms/step - loss: 0.0219 - val_loss: 0.0256\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 9s 550ms/step - loss: 0.0222 - val_loss: 0.0258\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 9s 540ms/step - loss: 0.0213 - val_loss: 0.0249\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 9s 554ms/step - loss: 0.0211 - val_loss: 0.0257\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 8s 523ms/step - loss: 0.0216 - val_loss: 0.0251\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 9s 585ms/step - loss: 0.0217 - val_loss: 0.0256\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 9s 562ms/step - loss: 0.0212 - val_loss: 0.0248\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 9s 538ms/step - loss: 0.0220 - val_loss: 0.0255\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 10s 611ms/step - loss: 0.0213 - val_loss: 0.0243\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 9s 542ms/step - loss: 0.0213 - val_loss: 0.0246\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 9s 567ms/step - loss: 0.0215 - val_loss: 0.0256\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 9s 543ms/step - loss: 0.0221 - val_loss: 0.0249\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 9s 557ms/step - loss: 0.0218 - val_loss: 0.0258\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 9s 541ms/step - loss: 0.0223 - val_loss: 0.0249\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 9s 544ms/step - loss: 0.0218 - val_loss: 0.0248\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 9s 552ms/step - loss: 0.0214 - val_loss: 0.0258\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 8s 527ms/step - loss: 0.0217 - val_loss: 0.0260\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 9s 560ms/step - loss: 0.0219 - val_loss: 0.0254\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 10s 594ms/step - loss: 0.0221 - val_loss: 0.0255\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 9s 564ms/step - loss: 0.0209 - val_loss: 0.0248\n",
      "Neurons: 148, Quantile: 0.6, Accuracy: 0.02480473928153515\n",
      "--- 7435.654329299927 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 13s 824ms/step - loss: 0.0787 - val_loss: 0.0967\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 12s 758ms/step - loss: 0.0766 - val_loss: 0.0624\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 12s 780ms/step - loss: 0.0714 - val_loss: 0.0610\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 13s 787ms/step - loss: 0.0581 - val_loss: 0.0407\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 13s 839ms/step - loss: 0.0529 - val_loss: 0.0427\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 13s 802ms/step - loss: 0.0405 - val_loss: 0.0344\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 14s 859ms/step - loss: 0.0324 - val_loss: 0.0348\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 14s 897ms/step - loss: 0.0365 - val_loss: 0.0313\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 13s 793ms/step - loss: 0.0346 - val_loss: 0.0368\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 13s 796ms/step - loss: 0.0314 - val_loss: 0.0395\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 13s 790ms/step - loss: 0.0299 - val_loss: 0.0379\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 13s 800ms/step - loss: 0.0285 - val_loss: 0.0358\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 14s 851ms/step - loss: 0.0274 - val_loss: 0.0335\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 13s 796ms/step - loss: 0.0266 - val_loss: 0.0316\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 14s 883ms/step - loss: 0.0260 - val_loss: 0.0302\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 13s 819ms/step - loss: 0.0256 - val_loss: 0.0287\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 13s 809ms/step - loss: 0.0252 - val_loss: 0.0276\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 12s 773ms/step - loss: 0.0248 - val_loss: 0.0268\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 13s 836ms/step - loss: 0.0245 - val_loss: 0.0261\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 13s 843ms/step - loss: 0.0242 - val_loss: 0.0260\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 13s 827ms/step - loss: 0.0241 - val_loss: 0.0272\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 13s 795ms/step - loss: 0.0248 - val_loss: 0.0305\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 13s 818ms/step - loss: 0.0288 - val_loss: 0.0306\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 13s 809ms/step - loss: 0.0318 - val_loss: 0.0281\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 13s 808ms/step - loss: 0.0288 - val_loss: 0.0271\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 13s 796ms/step - loss: 0.0260 - val_loss: 0.0252\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 12s 754ms/step - loss: 0.0244 - val_loss: 0.0247\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 13s 784ms/step - loss: 0.0236 - val_loss: 0.0239\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 14s 870ms/step - loss: 0.0232 - val_loss: 0.0237\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 14s 876ms/step - loss: 0.0228 - val_loss: 0.0233\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 13s 807ms/step - loss: 0.0226 - val_loss: 0.0234\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 13s 794ms/step - loss: 0.0227 - val_loss: 0.0232\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 12s 778ms/step - loss: 0.0227 - val_loss: 0.0233\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 13s 810ms/step - loss: 0.0228 - val_loss: 0.0231\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 13s 786ms/step - loss: 0.0227 - val_loss: 0.0232\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 13s 788ms/step - loss: 0.0228 - val_loss: 0.0230\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 13s 782ms/step - loss: 0.0225 - val_loss: 0.0228\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 14s 878ms/step - loss: 0.0223 - val_loss: 0.0227\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 13s 824ms/step - loss: 0.0221 - val_loss: 0.0226\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 13s 822ms/step - loss: 0.0220 - val_loss: 0.0226\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 14s 856ms/step - loss: 0.0220 - val_loss: 0.0225\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 13s 812ms/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 14s 869ms/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 13s 806ms/step - loss: 0.0219 - val_loss: 0.0225\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 14s 847ms/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 13s 830ms/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 13s 837ms/step - loss: 0.0217 - val_loss: 0.0223\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 13s 799ms/step - loss: 0.0217 - val_loss: 0.0223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "16/16 [==============================] - 13s 787ms/step - loss: 0.0217 - val_loss: 0.0222\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 13s 827ms/step - loss: 0.0216 - val_loss: 0.0220\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 13s 818ms/step - loss: 0.0214 - val_loss: 0.0221\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 13s 818ms/step - loss: 0.0214 - val_loss: 0.0221\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 13s 840ms/step - loss: 0.0214 - val_loss: 0.0221\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 12s 771ms/step - loss: 0.0214 - val_loss: 0.0222\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 13s 811ms/step - loss: 0.0214 - val_loss: 0.0219\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 13s 797ms/step - loss: 0.0214 - val_loss: 0.0219\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 13s 788ms/step - loss: 0.0213 - val_loss: 0.0222\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 14s 852ms/step - loss: 0.0215 - val_loss: 0.0223\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 12s 772ms/step - loss: 0.0211 - val_loss: 0.0228\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 13s 812ms/step - loss: 0.0217 - val_loss: 0.0222\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 13s 805ms/step - loss: 0.0216 - val_loss: 0.0225\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 13s 802ms/step - loss: 0.0212 - val_loss: 0.0229\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 13s 801ms/step - loss: 0.0215 - val_loss: 0.0226\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 13s 796ms/step - loss: 0.0216 - val_loss: 0.0218\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 13s 789ms/step - loss: 0.0213 - val_loss: 0.0218\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 13s 782ms/step - loss: 0.0208 - val_loss: 0.0223\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 13s 804ms/step - loss: 0.0216 - val_loss: 0.0223\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 12s 781ms/step - loss: 0.0214 - val_loss: 0.0219\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 13s 800ms/step - loss: 0.0212 - val_loss: 0.0223\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 13s 819ms/step - loss: 0.0210 - val_loss: 0.0223\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 13s 816ms/step - loss: 0.0215 - val_loss: 0.0219\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 13s 818ms/step - loss: 0.0213 - val_loss: 0.0220\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 13s 811ms/step - loss: 0.0212 - val_loss: 0.0227\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 13s 795ms/step - loss: 0.0211 - val_loss: 0.0219\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 13s 791ms/step - loss: 0.0209 - val_loss: 0.0222\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 14s 873ms/step - loss: 0.0211 - val_loss: 0.0218\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 13s 829ms/step - loss: 0.0208 - val_loss: 0.0220\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 13s 814ms/step - loss: 0.0209 - val_loss: 0.0224\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 14s 846ms/step - loss: 0.0207 - val_loss: 0.0222\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 12s 769ms/step - loss: 0.0211 - val_loss: 0.0220\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 13s 809ms/step - loss: 0.0208 - val_loss: 0.0217\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 14s 867ms/step - loss: 0.0207 - val_loss: 0.0224\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 12s 763ms/step - loss: 0.0210 - val_loss: 0.0222\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 13s 795ms/step - loss: 0.0210 - val_loss: 0.0217\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 13s 833ms/step - loss: 0.0204 - val_loss: 0.0221\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 13s 827ms/step - loss: 0.0209 - val_loss: 0.0223\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 13s 787ms/step - loss: 0.0210 - val_loss: 0.0219\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 13s 806ms/step - loss: 0.0209 - val_loss: 0.0220\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 14s 849ms/step - loss: 0.0206 - val_loss: 0.0219\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 13s 794ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 13s 793ms/step - loss: 0.0206 - val_loss: 0.0220\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 13s 814ms/step - loss: 0.0206 - val_loss: 0.0219\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 13s 839ms/step - loss: 0.0205 - val_loss: 0.0222\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 14s 862ms/step - loss: 0.0208 - val_loss: 0.0219\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 13s 788ms/step - loss: 0.0204 - val_loss: 0.0218\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 13s 826ms/step - loss: 0.0203 - val_loss: 0.0218\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 12s 770ms/step - loss: 0.0202 - val_loss: 0.0218\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 11s 702ms/step - loss: 0.0203 - val_loss: 0.0218\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 13s 782ms/step - loss: 0.0206 - val_loss: 0.0221\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 13s 787ms/step - loss: 0.0203 - val_loss: 0.0220\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 12s 761ms/step - loss: 0.0205 - val_loss: 0.0219\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 11s 700ms/step - loss: 0.0205 - val_loss: 0.0224\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 11s 692ms/step - loss: 0.0201 - val_loss: 0.0226\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 11s 691ms/step - loss: 0.0207 - val_loss: 0.0222\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 11s 694ms/step - loss: 0.0209 - val_loss: 0.0228\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 12s 725ms/step - loss: 0.0205 - val_loss: 0.0224\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 12s 750ms/step - loss: 0.0211 - val_loss: 0.0227\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 12s 727ms/step - loss: 0.0205 - val_loss: 0.0228\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 12s 736ms/step - loss: 0.0209 - val_loss: 0.0221\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 12s 731ms/step - loss: 0.0208 - val_loss: 0.0227\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 11s 716ms/step - loss: 0.0207 - val_loss: 0.0224\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 11s 698ms/step - loss: 0.0204 - val_loss: 0.0225\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 12s 746ms/step - loss: 0.0206 - val_loss: 0.0221\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 12s 743ms/step - loss: 0.0202 - val_loss: 0.0223\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 13s 782ms/step - loss: 0.0200 - val_loss: 0.0218\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 11s 710ms/step - loss: 0.0199 - val_loss: 0.0219\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 11s 710ms/step - loss: 0.0204 - val_loss: 0.0231\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 11s 708ms/step - loss: 0.0201 - val_loss: 0.0228\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 12s 736ms/step - loss: 0.0200 - val_loss: 0.0234\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 12s 776ms/step - loss: 0.0198 - val_loss: 0.0222\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 13s 783ms/step - loss: 0.0195 - val_loss: 0.0222\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 13s 786ms/step - loss: 0.0198 - val_loss: 0.0235\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 11s 704ms/step - loss: 0.0197 - val_loss: 0.0226\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 12s 723ms/step - loss: 0.0197 - val_loss: 0.0229\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 12s 722ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 12s 724ms/step - loss: 0.0201 - val_loss: 0.0229\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 12s 721ms/step - loss: 0.0208 - val_loss: 0.0239\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 11s 682ms/step - loss: 0.0208 - val_loss: 0.0229\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 11s 709ms/step - loss: 0.0207 - val_loss: 0.0239\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 11s 702ms/step - loss: 0.0210 - val_loss: 0.0233\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 11s 703ms/step - loss: 0.0212 - val_loss: 0.0226\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 11s 717ms/step - loss: 0.0208 - val_loss: 0.0245\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 11s 711ms/step - loss: 0.0216 - val_loss: 0.0220\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 11s 696ms/step - loss: 0.0209 - val_loss: 0.0226\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 11s 683ms/step - loss: 0.0206 - val_loss: 0.0224\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 11s 679ms/step - loss: 0.0205 - val_loss: 0.0217\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 11s 709ms/step - loss: 0.0197 - val_loss: 0.0227\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 11s 718ms/step - loss: 0.0197 - val_loss: 0.0220\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 13s 802ms/step - loss: 0.0196 - val_loss: 0.0229\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 13s 813ms/step - loss: 0.0197 - val_loss: 0.0243\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 12s 755ms/step - loss: 0.0197 - val_loss: 0.0217\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 12s 761ms/step - loss: 0.0197 - val_loss: 0.0236\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 12s 738ms/step - loss: 0.0196 - val_loss: 0.0228\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 12s 721ms/step - loss: 0.0192 - val_loss: 0.0221\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 13s 811ms/step - loss: 0.0194 - val_loss: 0.0235\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 12s 721ms/step - loss: 0.0193 - val_loss: 0.0221\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 12s 739ms/step - loss: 0.0193 - val_loss: 0.0238\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 12s 764ms/step - loss: 0.0192 - val_loss: 0.0230\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 12s 735ms/step - loss: 0.0192 - val_loss: 0.0214\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 13s 819ms/step - loss: 0.0195 - val_loss: 0.0235\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 12s 760ms/step - loss: 0.0196 - val_loss: 0.0221\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 12s 775ms/step - loss: 0.0189 - val_loss: 0.0217\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 12s 725ms/step - loss: 0.0194 - val_loss: 0.0222\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 12s 757ms/step - loss: 0.0190 - val_loss: 0.0216\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 13s 814ms/step - loss: 0.0188 - val_loss: 0.0220\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 11s 699ms/step - loss: 0.0189 - val_loss: 0.0224\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 12s 772ms/step - loss: 0.0189 - val_loss: 0.0216\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 12s 733ms/step - loss: 0.0194 - val_loss: 0.0233\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 13s 808ms/step - loss: 0.0203 - val_loss: 0.0219\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 13s 801ms/step - loss: 0.0192 - val_loss: 0.0230\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 11s 705ms/step - loss: 0.0191 - val_loss: 0.0216\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 11s 714ms/step - loss: 0.0192 - val_loss: 0.0227\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 12s 720ms/step - loss: 0.0197 - val_loss: 0.0237\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 11s 703ms/step - loss: 0.0191 - val_loss: 0.0217\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 12s 734ms/step - loss: 0.0188 - val_loss: 0.0217\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 12s 779ms/step - loss: 0.0189 - val_loss: 0.0227\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 12s 739ms/step - loss: 0.0198 - val_loss: 0.0228\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 11s 716ms/step - loss: 0.0196 - val_loss: 0.0226\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 12s 770ms/step - loss: 0.0198 - val_loss: 0.0219\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 11s 701ms/step - loss: 0.0194 - val_loss: 0.0220\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 13s 784ms/step - loss: 0.0194 - val_loss: 0.0227\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 12s 739ms/step - loss: 0.0196 - val_loss: 0.0232\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 11s 706ms/step - loss: 0.0200 - val_loss: 0.0239\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 12s 743ms/step - loss: 0.0201 - val_loss: 0.0261\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 12s 781ms/step - loss: 0.0214 - val_loss: 0.0249\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 12s 723ms/step - loss: 0.0224 - val_loss: 0.0247\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 11s 704ms/step - loss: 0.0224 - val_loss: 0.0280\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 12s 740ms/step - loss: 0.0233 - val_loss: 0.0268\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 12s 735ms/step - loss: 0.0254 - val_loss: 0.0269\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 12s 750ms/step - loss: 0.0236 - val_loss: 0.0228\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 13s 793ms/step - loss: 0.0220 - val_loss: 0.0237\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 12s 761ms/step - loss: 0.0221 - val_loss: 0.0230\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 12s 768ms/step - loss: 0.0217 - val_loss: 0.0242\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 12s 732ms/step - loss: 0.0201 - val_loss: 0.0245\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 12s 729ms/step - loss: 0.0197 - val_loss: 0.0256\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 11s 711ms/step - loss: 0.0198 - val_loss: 0.0247\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 11s 706ms/step - loss: 0.0194 - val_loss: 0.0233\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 11s 712ms/step - loss: 0.0191 - val_loss: 0.0247\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 11s 715ms/step - loss: 0.0190 - val_loss: 0.0217\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 11s 709ms/step - loss: 0.0188 - val_loss: 0.0225\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 12s 761ms/step - loss: 0.0189 - val_loss: 0.0234\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 11s 704ms/step - loss: 0.0187 - val_loss: 0.0216\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 11s 711ms/step - loss: 0.0183 - val_loss: 0.0218\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 11s 703ms/step - loss: 0.0186 - val_loss: 0.0227\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 12s 723ms/step - loss: 0.0187 - val_loss: 0.0219\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 12s 739ms/step - loss: 0.0184 - val_loss: 0.0224\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 11s 716ms/step - loss: 0.0185 - val_loss: 0.0230\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 11s 706ms/step - loss: 0.0185 - val_loss: 0.0222\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 12s 723ms/step - loss: 0.0186 - val_loss: 0.0232\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 13s 808ms/step - loss: 0.0189 - val_loss: 0.0238\n",
      "Neurons: 200, Quantile: 0.3, Accuracy: 0.023806219920516014\n",
      "--- 10049.06998181343 seconds ---\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 12s 729ms/step - loss: 0.1277 - val_loss: 0.1452\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 11s 709ms/step - loss: 0.0964 - val_loss: 0.0791\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 12s 736ms/step - loss: 0.0979 - val_loss: 0.0715\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 11s 709ms/step - loss: 0.0752 - val_loss: 0.0536\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 11s 701ms/step - loss: 0.0743 - val_loss: 0.0540\n",
      "Epoch 6/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 12s 722ms/step - loss: 0.0709 - val_loss: 0.0516\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 12s 729ms/step - loss: 0.0704 - val_loss: 0.0504\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 12s 738ms/step - loss: 0.0693 - val_loss: 0.0488\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 12s 749ms/step - loss: 0.0684 - val_loss: 0.0474\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 12s 744ms/step - loss: 0.0669 - val_loss: 0.0464\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 12s 757ms/step - loss: 0.0641 - val_loss: 0.0481\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 12s 732ms/step - loss: 0.0596 - val_loss: 0.0504\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 11s 698ms/step - loss: 0.0501 - val_loss: 0.0480\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 12s 737ms/step - loss: 0.0455 - val_loss: 0.0494\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 12s 739ms/step - loss: 0.0411 - val_loss: 0.0470\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 12s 754ms/step - loss: 0.0373 - val_loss: 0.0481\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 12s 744ms/step - loss: 0.0344 - val_loss: 0.0403\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 11s 699ms/step - loss: 0.0319 - val_loss: 0.0379\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 11s 693ms/step - loss: 0.0306 - val_loss: 0.0343\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 12s 735ms/step - loss: 0.0297 - val_loss: 0.0324\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 11s 704ms/step - loss: 0.0292 - val_loss: 0.0316\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 11s 679ms/step - loss: 0.0291 - val_loss: 0.0313\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 11s 682ms/step - loss: 0.0292 - val_loss: 0.0314\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 11s 704ms/step - loss: 0.0292 - val_loss: 0.0316\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 11s 715ms/step - loss: 0.0288 - val_loss: 0.0307\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 11s 705ms/step - loss: 0.0287 - val_loss: 0.0306\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 11s 688ms/step - loss: 0.0282 - val_loss: 0.0295\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 11s 694ms/step - loss: 0.0280 - val_loss: 0.0292\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 11s 682ms/step - loss: 0.0285 - val_loss: 0.0300\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 11s 700ms/step - loss: 0.0281 - val_loss: 0.0292\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 11s 715ms/step - loss: 0.0278 - val_loss: 0.0290\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.0279 - val_loss: 0.0299\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.0278 - val_loss: 0.0295\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0276 - val_loss: 0.0281\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0270 - val_loss: 0.0292\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0270 - val_loss: 0.0277\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0263 - val_loss: 0.0270\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0263 - val_loss: 0.0269\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0259 - val_loss: 0.0268\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0261 - val_loss: 0.0269\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0260 - val_loss: 0.0271\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0257 - val_loss: 0.0267\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0254 - val_loss: 0.0277\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 16s 978ms/step - loss: 0.0263 - val_loss: 0.0268\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0257 - val_loss: 0.0263\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0253 - val_loss: 0.0267\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0256 - val_loss: 0.0272\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0259 - val_loss: 0.0261\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0251 - val_loss: 0.0268\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0256 - val_loss: 0.0261\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0253 - val_loss: 0.0260\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0253 - val_loss: 0.0262\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0249 - val_loss: 0.0261\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0252 - val_loss: 0.0265\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0248 - val_loss: 0.0255\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0247 - val_loss: 0.0256\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0247 - val_loss: 0.0258\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 21s 1s/step - loss: 0.0248 - val_loss: 0.0258\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.0246 - val_loss: 0.0254\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0246 - val_loss: 0.0257\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0245 - val_loss: 0.0256\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0245 - val_loss: 0.0258\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 21s 1s/step - loss: 0.0244 - val_loss: 0.0254\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0244 - val_loss: 0.0257\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0244 - val_loss: 0.0253\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0245 - val_loss: 0.0256\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0241 - val_loss: 0.0253\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0240 - val_loss: 0.0250\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0239 - val_loss: 0.0253\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0243 - val_loss: 0.0251\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.0242 - val_loss: 0.0257\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0240 - val_loss: 0.0253\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0239 - val_loss: 0.0252\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0242 - val_loss: 0.0256\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 16s 1s/step - loss: 0.0237 - val_loss: 0.0249\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0238 - val_loss: 0.0254\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0236 - val_loss: 0.0247\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0239 - val_loss: 0.0252\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0238 - val_loss: 0.0250\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0240 - val_loss: 0.0251\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0236 - val_loss: 0.0252\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.0237 - val_loss: 0.0248\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 14s 906ms/step - loss: 0.0233 - val_loss: 0.0247\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 15s 920ms/step - loss: 0.0237 - val_loss: 0.0249\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.0237 - val_loss: 0.0246\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0237 - val_loss: 0.0251\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0233 - val_loss: 0.0245\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.0233 - val_loss: 0.0248\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0235 - val_loss: 0.0257\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 16s 994ms/step - loss: 0.0237 - val_loss: 0.0248\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.0235 - val_loss: 0.0251\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 16s 985ms/step - loss: 0.0235 - val_loss: 0.0244\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 16s 982ms/step - loss: 0.0232 - val_loss: 0.0249\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 14s 885ms/step - loss: 0.0238 - val_loss: 0.0246\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.0241 - val_loss: 0.0252\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 22s 1s/step - loss: 0.0240 - val_loss: 0.0253\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 23s 1s/step - loss: 0.0235 - val_loss: 0.0251\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 21s 1s/step - loss: 0.0235 - val_loss: 0.0249\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.0233 - val_loss: 0.0249\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 21s 1s/step - loss: 0.0232 - val_loss: 0.0247\n",
      "Epoch 101/200\n",
      " 9/16 [===============>..............] - ETA: 6s - loss: 0.0229"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-7df8f08d5840>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mquantile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model_one_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneurons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneuron\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquantile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m700\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mmodel_hist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-b727cb032b7c>\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(quantile, trainx, trainy, valx, valy, epoch, batch_size, verbose)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtilted_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquantile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvaly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# make a prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM only one layer\n",
    "def build_model_one_layer(neurons = 200):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(72, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    #model.add(RepeatVector(24))\n",
    "    #model.add(LSTM(neurons, activation='relu', return_sequences=True))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(48))\n",
    "\n",
    "    return model\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "quantiles = [0.3, 0.6]\n",
    "acts = list()\n",
    "preds = list()\n",
    "df_accuracies = DataFrame()\n",
    "model_hist = list()\n",
    "neurons_grid = [24, 47, 72, 148, 200]\n",
    "#select 72 neurons\n",
    "for neuron in neurons_grid:\n",
    "    for quantile in quantiles:\n",
    "        model = build_model_one_layer(neurons = neuron)\n",
    "        model, history = fit_model(quantile, train_X, train_y, val_X, val_y, batch_size = 700, verbose = 1)\n",
    "        score = history.history.get('val_loss')[-1]\n",
    "        model_hist.append(history)\n",
    "        print('Neurons: {}, Quantile: {}, Accuracy: {}'.format(neuron, quantile, score))\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
